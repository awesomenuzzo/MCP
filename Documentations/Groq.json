[
  {
    "markdown": "## Documentation\n\n## LiveKit + Groq: Build End-to-End AI Voice Applications\n\n[LiveKit](https://livekit.io/) complements Groq's high-performance speech recognition capabilities by providing text-to-speech and real-time communication features. This integration enables you to build\nend-to-end AI voice applications with:\n\n- **Complete Voice Pipeline:** Combine Groq's fast and accurate speech-to-text (STT) with LiveKit's text-to-speech (TTS) capabilities\n- **Real-time Communication:** Enable multi-user voice interactions with LiveKit's WebRTC infrastructure\n- **Flexible TTS Options:** Access multiple text-to-speech voices and languages through LiveKit's TTS integrations\n- **Scalable Architecture:** Handle thousands of concurrent users with LiveKit's distributed system\n\n### [Quick Start (7 minutes to hello world)](https://console.groq.com/docs/livekit\\#quick-start-7-minutes-to-hello-world)\n\n#### 1\\. Prerequisites\n\n- Grab your [Groq API Key](https://console.groq.com/keys)\n- Create a free [LiveKit Cloud account](https://cloud.livekit.io/login)\n- Install the [LiveKit CLI](https://docs.livekit.io/home/cli/cli-setup/) and authenticate in your Command Line Interface (CLI)\n- Create a free ElevenLabs account and [generate an API Key](https://elevenlabs.io/app/settings/api-keys)\n\n#### 1\\. Clone the starter template for our Python voice agent using your CLI:\n\nWhen prompted for your OpenAI and Deepgram API key, press **Enter** to skip as we'll be using custommized plugins for Groq and ElevenLabs for fast inference speed.\n\n```bash\nlk app create --template voice-pipeline-agent-python\n```\n\n#### 2\\. CD into your project directory and update the `.env.local` file to replace `OPENAI_API_KEY` and `DEEPGRAM_API_KEY` with the following:\n\n```bash\nGROQ_API_KEY=<your-groq-api-key>\nELEVEN_API_KEY=<your-elevenlabs-api-key>\n```\n\n#### 3\\. Update your `requirements.txt` file and add the following line:\n\n```bash\nlivekit-plugins-elevenlabs>=0.7.9\n```\n\n#### 4\\. Update your `agent.py` file with the following to configure Groq for STT with `whisper-large-v3`, Groq for LLM with `llama-3.3-70b-versatile`, and ElevenLabs for TTS:\n\n```python\nimport logging\n\nfrom dotenv import load_dotenv\nfrom livekit.agents import (\n    AutoSubscribe,\n    JobContext,\n    JobProcess,\n    WorkerOptions,\n    cli,\n    llm,\n)\nfrom livekit.agents.pipeline import VoicePipelineAgent\nfrom livekit.plugins import silero, openai, elevenlabs\n\nload_dotenv(dotenv_path=\".env.local\")\nlogger = logging.getLogger(\"voice-agent\")\n\ndef prewarm(proc: JobProcess):\n    proc.userdata[\"vad\"] = silero.VAD.load()\n\nasync def entrypoint(ctx: JobContext):\n    initial_ctx = llm.ChatContext().append(\n        role=\"system\",\n        text=(\n            \"You are a voice assistant created by LiveKit. Your interface with users will be voice. \"\n            \"You should use short and concise responses, and avoiding usage of unpronouncable punctuation. \"\n            \"You were created as a demo to showcase the capabilities of LiveKit's agents framework.\"\n        ),\n    )\n\n    logger.info(f\"connecting to room {ctx.room.name}\")\n    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)\n\n    # Wait for the first participant to connect\n    participant = await ctx.wait_for_participant()\n    logger.info(f\"starting voice assistant for participant {participant.identity}\")\n\n    agent = VoicePipelineAgent(\n        vad=ctx.proc.userdata[\"vad\"],\n        stt=openai.STT.with_groq(model=\"whisper-large-v3\"),\n        llm=openai.LLM.with_groq(model=\"llama-3.3-70b-versatile\"),\n        tts=elevenlabs.TTS(),\n        chat_ctx=initial_ctx,\n    )\n\n    agent.start(ctx.room, participant)\n\n    # The agent should be polite and greet the user when it joins :)\n    await agent.say(\"Hey, how can I help you today?\", allow_interruptions=True)\n\nif __name__ == \"__main__\":\n    cli.run_app(\n        WorkerOptions(\n            entrypoint_fnc=entrypoint,\n            prewarm_fnc=prewarm,\n        ),\n    )\n```\n\n#### 5\\. Make sure you're in your project directory to install the dependencies and start your agent:\n\n```bash\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install -r requirements.txt\npython3 agent.py dev\n```\n\n#### 6\\. Within your project directory, clone the voice assistant frontend Next.js app starter template using your CLI:\n\n```bash\nlk app create --template voice-assistant-frontend\n```\n\n#### 7\\. CD into your frontend directory and launch your frontend application locally:\n\n```bash\npnpm install\npnpm dev\n```\n\n#### 8\\. Visit your application ( [http://localhost:3000/](http://localhost:3000/) by default), select **Connect** and talk to your agent!\n\n**Challenge:** Configure your voice assistant and the frontend to create a travel agent that will help plan trips!\n\nFor more detailed documentation and resources, see:\n\n- [Official Documentation: LiveKit](https://docs.livekit.io/)",
    "metadata": {
      "url": "https://console.groq.com/docs/livekit",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "2f97bbbf-1f7a-4bbb-b2ed-49229a4d157d",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/livekit",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Arize + Groq: Open-Source AI Observability\n\n[Arize Phoenix](https://docs.arize.com/phoenix) developed by [Arize AI](https://arize.com/) is an open-source AI observability library that enables comprehensive tracing and monitoring for your AI\napplications. By integrating Arize's observability tools with your Groq-powered applications, you can gain deep insights into your LLM worklflow's performance and behavior with features including:\n\n- **Automatic Tracing:** Capture detailed metrics about LLM calls, including latency, token usage, and exceptions\n- **Real-time Monitoring:** Track application performance and identify bottlenecks in production\n- **Evaluation Framework:** Utilize pre-built templates to assess LLM performance\n- **Prompt Management:** Easily iterate on prompts and test changes against your data\n\n### [Python Quick Start (3 minutes to hello world)](https://console.groq.com/docs/arize\\#python-quick-start-3-minutes-to-hello-world)\n\n#### 1\\. Install the required packages:\n\n```bash\npip install arize-phoenix-otel openinference-instrumentation-groq groq\n```\n\n#### 2\\. Sign up for an [Arize Phoenix account](https://app.phoenix.arize.com/).\n\n#### 2\\. Configure your Groq and Arize Phoenix API keys:\n\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\nexport PHOENIX_API_KEY=\"your-phoenix-api-key\"\n```\n\n#### 3\\. (Optional) [Create a new project](https://app.phoenix.arize.com/projects) or use the \"default\" project as your `project_name` below.\n\n#### 4\\. Create your first traced Groq application:\n\nIn Arize Phoenix, **traces** capture the complete journey of an LLM request through your application, while **spans** represent individual operations within that trace. The instrumentation\nautomatically captures important metrics and metadata.\n\n```python\nimport os\nfrom phoenix.otel import register\nfrom openinference.instrumentation.groq import GroqInstrumentor\nfrom groq import Groq\n\n# Configure environment variables for Phoenix\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\n# Configure Phoenix tracer\ntracer_provider = register(\n    project_name=\"default\",\n    endpoint=\"https://app.phoenix.arize.com/v1/traces\",\n)\n\n# Initialize Groq instrumentation\nGroqInstrumentor().instrument(tracer_provider=tracer_provider)\n\n# Create Groq client\nclient = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n\n# Make an instrumented LLM call\nchat_completion = client.chat.completions.create(\n    messages=[{\\\n        \"role\": \"user\",\\\n        \"content\": \"Explain the importance of AI observability\"\\\n    }],\n    model=\"llama-3.3-70b-versatile\",\n)\n\nprint(chat_completion.choices[0].message.content)\n```\n\nRunning the above code will create an automatically instrumented Groq application! The traces will be available in your Phoenix dashboard within the `default` project, showing\ndetailed information about:\n\n- **Application Latency:** Identify slow components and bottlenecks\n- **Token Usage:** Track token consumption across different operations\n- **Runtime Exceptions:** Capture and analyze errors and rate limits\n- **LLM Parameters:** Monitor temperature, system prompts, and other settings\n- **Response Analysis:** Examine LLM outputs and their characteristics\n\n**Challenge**: Update an existing Groq-powered application you've built to add Arize Phoenix tracing!\n\nFor more detailed documentation and resources on building observable LLM applications with Groq and Arize, see:\n\n- [Official Documentation: Groq Integration Guide](https://docs.arize.com/phoenix/tracing/integrations-tracing/groq)\n- [Blog: Tracing with Groq](https://arize.com/blog/tracing-groq/)\n- [Webinar: Tracing and Evaluating LLM Apps with Groq and Arize Phoenix](https://youtu.be/KjtrILr6JZI?si=iX8Udo-EYsK2JOvF)",
    "metadata": {
      "url": "https://console.groq.com/docs/arize",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "378304b5-7469-4d0d-bda7-486bffb39853",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/arize",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)",
    "metadata": {
      "url": "https://console.groq.com/settings/limits",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "2143ddcd-a8a9-4513-b149-1ad96d4e2b26",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/settings/limits",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## GroqCloud Developer Console\n\nFast LLM inference, OpenAI-compatible. Simple to integrate, easy to scale. Start building in minutes.\n\nWhat’s New: Qwen 2.5 32B Instruct and DeepSeek R1 Distill Qwen 32B now available! 💪\n\nWelcome to the Groq Console - Getting Started - YouTube\n\nGroq\n\n29.9K subscribers\n\n[Welcome to the Groq Console - Getting Started](https://www.youtube.com/watch?v=Ig7esRBhFPY)\n\nGroq\n\nSearch\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\nWatch later\n\nShare\n\nCopy link\n\n[Watch on www.youtube.com](https://www.youtube.com/watch?v=Ig7esRBhFPY)\n\nWatch on\n\n![Video cover](https://console.groq.com/_next/image?url=%2Foverview%2Fconsole-quickstart-cover.png&w=3840&q=75)\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/chat/completions -s \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GROQ_API_KEY\" \\\n-d '{\n\"model\": \"llama-3.3-70b-versatile\",\n\"messages\": [{\\\n    \"role\": \"user\",\\\n    \"content\": \"Explain the importance of fast language models\"\\\n}]\n}'\n```\n\n#### Start building apps on Groq\n\n[**Quickstart** \\\\\n\\\\\nGet up and running with the Groq API in a few minutes.\\\\\n\\\\\nCreate and setup your API Key](https://console.groq.com/docs/quickstart)\n\n[**Playground** \\\\\n\\\\\nExperiment with the Groq API](https://console.groq.com/playground)\n\n[**Example Apps** \\\\\n\\\\\nCheck out cool Groq built apps](https://console.groq.com/docs/examples)\n\n#### Developer Resources\n\nEssential resources to accelerate your development and maximize productivity\n\n[![Tag](https://console.groq.com/_next/static/media/tag.6dfe58ce.svg)\\\\\n\\\\\n**API Reference** \\\\\n\\\\\nExplore all API parameters and response attributes](https://console.groq.com/docs/api-reference#chat)\n\n[![Discord](https://console.groq.com/_next/static/media/discordlogo.62500149.svg)\\\\\n\\\\\n**Developer Community** \\\\\n\\\\\nCheck out sneak peeks, announcements & get support](https://groq.com/community/)\n\n[![Cooking](https://console.groq.com/_next/static/media/cookingpot.50bbd813.svg)\\\\\n\\\\\n**API Cookbook** \\\\\n\\\\\nSee code examples and tutorials to jumpstart your app](https://github.com/groq/groq-api-cookbook)\n\n[![Lightning](https://console.groq.com/_next/static/media/lightning.7a4b98ec.svg)\\\\\n\\\\\n**OpenAI Compatibility** \\\\\n\\\\\nCompatible with OpenAI's client libraries](https://console.groq.com/docs/openai)\n\n#### The Models\n\nWe’re adding new models all the time and will let you know when a new one comes online. See full details on our [Models page](https://console.groq.com/docs/models).\n\n![Meta](https://console.groq.com/_next/static/media/deepseeklogo.7ebad28d.svg)\n\nDeepseek R1 Distill Llama 70B\n\n![Meta](https://console.groq.com/_next/static/media/metalogo.dcf881ba.svg)\n\nLlama 3.3, 3.2, 3.1, and LlamaGuard\n\n![OpenAI](https://console.groq.com/_next/static/media/openailogo.7ef34c85.svg)\n\nWhisper Large v3, Turbo, and Distill\n\n![Mixtral](https://console.groq.com/_next/static/media/mistrallogo.47b27bfa.svg)\n\nMixtral 8x7b\n\n![Google](https://console.groq.com/_next/static/media/googlelogo.41a15ae7.svg)\n\nGemma 2",
    "metadata": {
      "url": "https://console.groq.com/docs/overview",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "179fe2f8-2d15-4024-9436-19f39241f291",
      "viewport": [
        "width=device-width, initial-scale=1, maximum-scale=1",
        "width=device-width, initial-scale=1"
      ],
      "sourceURL": "https://console.groq.com/docs/overview",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n[**Chat**](https://console.groq.com/docs/api-reference#chat)\n\n[**Create chat completion**](https://console.groq.com/docs/api-reference#chat-create)\n\nPOSThttps://api.groq.com/openai/v1/chat/completions\n\nCreates a model response for the given chat conversation.\n\n### Request Body\n\n- frequency\\_penaltynumber or nullOptionalDefaults to 0\n\n\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\n- function\\_callDeprecatedstring / object or nullOptional\n\n\n\nDeprecated in favor of `tool_choice`.\n\n\n\nControls which (if any) function is called by the model.\n`none` means the model will not call a function and instead generates a message.\n`auto` means the model can pick between generating a message or calling a function.\nSpecifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function.\n\n\n\n`none` is the default when no functions are present. `auto` is the default if functions are present.\n\n\n\n\n\n\n\n### Show possible types\n\n- functionsDeprecatedarray or nullOptional\n\n\n\nDeprecated in favor of `tools`.\n\n\n\nA list of functions the model may generate JSON inputs for.\n\n\n\n\n\n\n\n### Show properties\n\n- logit\\_biasobject or nullOptional\n\n\n\nThis is not yet supported by any of our models.\nModify the likelihood of specified tokens appearing in the completion.\n\n- logprobsboolean or nullOptionalDefaults to false\n\n\n\nThis is not yet supported by any of our models.\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.\n\n- max\\_completion\\_tokensinteger or nullOptional\n\n\n\nThe maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.\n\n- max\\_tokensDeprecatedinteger or nullOptional\n\n\n\nDeprecated in favor of `max_completion_tokens`.\nThe maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.\n\n- messagesarrayRequired\n\n\n\nA list of messages comprising the conversation so far.\n\n\n\n\n\n\n\n### Show possible types\n\n- modelstringRequired\n\n\n\nID of the model to use. For details on which models are compatible with the Chat API, see available [models](https://console.groq.com/docs/models)\n\n- ninteger or nullOptionalDefaults to 1\n\n\n\nHow many chat completion choices to generate for each input message. Note that the current moment, only n=1 is supported. Other values will result in a 400 response.\n\n- parallel\\_tool\\_callsboolean or nullOptionalDefaults to true\n\n\n\nWhether to enable parallel function calling during tool use.\n\n- presence\\_penaltynumber or nullOptionalDefaults to 0\n\n\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\n- reasoning\\_formatstring or nullOptional\n\n\n\nSpecifies how to output reasoning tokens\n\n- response\\_formatobject or nullOptional\n\n\n\nAn object specifying the format that the model must output.\n\n\n\nSetting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is valid JSON.\n\n\n\n**Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message.\n\n\n\n\n\n\n\n### Show properties\n\n- seedinteger or nullOptional\n\n\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n\n- service\\_tierstring or nullOptional\n\n\n\nThe service tier to use for the request. Defaults to `on_demand`.\n\n\n\n- `auto` will automatically select the highest tier available within the rate limits of your organization.\n- `flex` uses the flex tier, which will succeed or fail quickly.\n\n- stopstring / array or nullOptional\n\n\n\nUp to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n\n\n\n\n\n\n\n### Show possible types\n\n- streamboolean or nullOptionalDefaults to false\n\n\n\nIf set, partial message deltas will be sent. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example code](https://console.groq.com/docs/text-chat#streaming-a-chat-completion).\n\n- stream\\_optionsobject or nullOptional\n\n\n\nOptions for streaming response. Only set this when you set `stream: true`.\n\n\n\n\n\n\n\n### Show properties\n\n- temperaturenumber or nullOptionalDefaults to 1\n\n\n\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top\\_p but not both\n\n- tool\\_choicestring / object or nullOptional\n\n\n\nControls which (if any) tool is called by the model.\n`none` means the model will not call any tool and instead generates a message.\n`auto` means the model can pick between generating a message or calling one or more tools.\n`required` means the model must call one or more tools.\nSpecifying a particular tool via `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to call that tool.\n\n\n\n`none` is the default when no tools are present. `auto` is the default if tools are present.\n\n\n\n\n\n\n\n### Show possible types\n\n- toolsarray or nullOptional\n\n\n\nA list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n\n\n\n\n\n\n\n### Show properties\n\n- top\\_logprobsinteger or nullOptional\n\n\n\nThis is not yet supported by any of our models.\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.\n\n- top\\_pnumber or nullOptionalDefaults to 1\n\n\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\\_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.\n\n- userstring or nullOptional\n\n\n\nA unique identifier representing your end-user, which can help us monitor and detect abuse.\n\n\n### Returns\n\nReturns a [chat completion](https://console.groq.com/docs/api-reference#chat-create) object, or a streamed sequence of [chat completion chunk](https://console.groq.com/docs/api-reference#chat-create) objects if the request is streamed.\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/chat/completions -s \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GROQ_API_KEY\" \\\n-d '{\n\"model\": \"llama-3.3-70b-versatile\",\n\"messages\": [{\\\n    \"role\": \"user\",\\\n    \"content\": \"Explain the importance of fast language models\"\\\n}]\n}'\n```\n\n```json\n{\n  \"id\": \"chatcmpl-f51b2cd2-bef7-417e-964e-a08f0b513c22\",\n  \"object\": \"chat.completion\",\n  \"created\": 1730241104,\n  \"model\": \"llama3-8b-8192\",\n  \"choices\": [\\\n    {\\\n      \"index\": 0,\\\n      \"message\": {\\\n        \"role\": \"assistant\",\\\n        \"content\": \"Fast language models have gained significant attention in recent years due to their ability to process and generate human-like text quickly and efficiently. The importance of fast language models can be understood from their potential applications and benefits:\\n\\n1. **Real-time Chatbots and Conversational Interfaces**: Fast language models enable the development of chatbots and conversational interfaces that can respond promptly to user queries, making them more engaging and useful.\\n2. **Sentiment Analysis and Opinion Mining**: Fast language models can quickly analyze text data to identify sentiments, opinions, and emotions, allowing for improved customer service, market research, and opinion mining.\\n3. **Language Translation and Localization**: Fast language models can quickly translate text between languages, facilitating global communication and enabling businesses to reach a broader audience.\\n4. **Text Summarization and Generation**: Fast language models can summarize long documents or even generate new text on a given topic, improving information retrieval and processing efficiency.\\n5. **Named Entity Recognition and Information Extraction**: Fast language models can rapidly recognize and extract specific entities, such as names, locations, and organizations, from unstructured text data.\\n6. **Recommendation Systems**: Fast language models can analyze large amounts of text data to personalize product recommendations, improve customer experience, and increase sales.\\n7. **Content Generation for Social Media**: Fast language models can quickly generate engaging content for social media platforms, helping businesses maintain a consistent online presence and increasing their online visibility.\\n8. **Sentiment Analysis for Stock Market Analysis**: Fast language models can quickly analyze social media posts, news articles, and other text data to identify sentiment trends, enabling financial analysts to make more informed investment decisions.\\n9. **Language Learning and Education**: Fast language models can provide instant feedback and adaptive language learning, making language education more effective and engaging.\\n10. **Domain-Specific Knowledge Extraction**: Fast language models can quickly extract relevant information from vast amounts of text data, enabling domain experts to focus on high-level decision-making rather than manual information gathering.\\n\\nThe benefits of fast language models include:\\n\\n* **Increased Efficiency**: Fast language models can process large amounts of text data quickly, reducing the time and effort required for tasks such as sentiment analysis, entity recognition, and text summarization.\\n* **Improved Accuracy**: Fast language models can analyze and learn from large datasets, leading to more accurate results and more informed decision-making.\\n* **Enhanced User Experience**: Fast language models can enable real-time interactions, personalized recommendations, and timely responses, improving the overall user experience.\\n* **Cost Savings**: Fast language models can automate many tasks, reducing the need for manual labor and minimizing costs associated with data processing and analysis.\\n\\nIn summary, fast language models have the potential to transform various industries and applications by providing fast, accurate, and efficient language processing capabilities.\"\\\n      },\\\n      \"logprobs\": null,\\\n      \"finish_reason\": \"stop\"\\\n    }\\\n  ],\n  \"usage\": {\n    \"queue_time\": 0.037493756,\n    \"prompt_tokens\": 18,\n    \"prompt_time\": 0.000680594,\n    \"completion_tokens\": 556,\n    \"completion_time\": 0.463333333,\n    \"total_tokens\": 574,\n    \"total_time\": 0.464013927\n  },\n  \"system_fingerprint\": \"fp_179b0f92c9\",\n  \"x_groq\": { \"id\": \"req_01jbd6g2qdfw2adyrt2az8hz4w\" }\n}\n```\n\n[**Audio**](https://console.groq.com/docs/api-reference#audio)\n\n[**Create transcription**](https://console.groq.com/docs/api-reference#audio-transcription)\n\nPOSThttps://api.groq.com/openai/v1/audio/transcriptions\n\nTranscribes audio into the input language.\n\n### Request Body\n\n- filestringRequired\n\n\n\nThe audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n\n- languagestringOptional\n\n\n\nThe language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency.\n\n- modelstringRequired\n\n\n\nID of the model to use. Only `whisper-large-v3` is currently available.\n\n- promptstringOptional\n\n\n\nAn optional text to guide the model's style or continue a previous audio segment. The [prompt](https://console.groq.com/docs/speech-text) should match the audio language.\n\n- response\\_formatstringOptionalDefaults to json\n\n\n\nThe format of the transcript output, in one of these options: `json`, `text`, or `verbose_json`.\n\n- temperaturenumberOptionalDefaults to 0\n\n\n\nThe sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.\n\n- timestamp\\_granularities\\[\\]arrayOptionalDefaults to segment\n\n\n\nThe timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.\n\n\n### Returns\n\nReturns an audio transcription object\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/audio/transcriptions \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F file=\"@./sample_audio.m4a\" \\\n  -F model=\"whisper-large-v3\"\n```\n\n```json\n{\n  \"text\": \"Your transcribed text appears here...\",\n  \"x_groq\": {\n    \"id\": \"req_unique_id\"\n  }\n}\n```\n\n[**Create translation**](https://console.groq.com/docs/api-reference#audio-translation)\n\nPOSThttps://api.groq.com/openai/v1/audio/translations\n\nTranslates audio into English.\n\n### Request Body\n\n- filestringRequired\n\n\n\nThe audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n\n- modelstringRequired\n\n\n\nID of the model to use. Only `whisper-large-v3` is currently available.\n\n- promptstringOptional\n\n\n\nAn optional text to guide the model's style or continue a previous audio segment. The [prompt](https://console.groq.com/docs/guides/speech-to-text/prompting) should be in English.\n\n- response\\_formatstringOptionalDefaults to json\n\n\n\nThe format of the transcript output, in one of these options: `json`, `text`, or `verbose_json`.\n\n- temperaturenumberOptionalDefaults to 0\n\n\n\nThe sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.\n\n\n### Returns\n\nReturns an audio translation object\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/audio/translations \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F file=\"@./sample_audio.m4a\" \\\n  -F model=\"whisper-large-v3\"\n```\n\n```json\n{\n  \"text\": \"Your translated text appears here...\",\n  \"x_groq\": {\n    \"id\": \"req_unique_id\"\n  }\n}\n```\n\n[**Models**](https://console.groq.com/docs/api-reference#models)\n\n[**List models**](https://console.groq.com/docs/api-reference#models-list)\n\nGEThttps://api.groq.com/openai/v1/models\n\nList models\n\n### Returns\n\nA list of models\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/models \\\n-H \"Authorization: Bearer $GROQ_API_KEY\"\n```\n\n```json\n{\n  \"object\": \"list\",\n  \"data\": [\\\n    {\\\n      \"id\": \"llama3-groq-70b-8192-tool-use-preview\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Groq\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"gemma2-9b-it\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Google\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama3-8b-8192\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama-3.2-90b-vision-preview\",\\\n      \"object\": \"model\",\\\n      \"created\": 1727226914,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama3-70b-8192\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama-3.2-11b-vision-preview\",\\\n      \"object\": \"model\",\\\n      \"created\": 1727226869,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama-3.2-11b-text-preview\",\\\n      \"object\": \"model\",\\\n      \"created\": 1727283005,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"whisper-large-v3-turbo\",\\\n      \"object\": \"model\",\\\n      \"created\": 1728413088,\\\n      \"owned_by\": \"OpenAI\",\\\n      \"active\": true,\\\n      \"context_window\": 448,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llava-v1.5-7b-4096-preview\",\\\n      \"object\": \"model\",\\\n      \"created\": 1725402373,\\\n      \"owned_by\": \"Other\",\\\n      \"active\": true,\\\n      \"context_window\": 4096,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama-3.1-70b-versatile\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 32768,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama-3.2-3b-preview\",\\\n      \"object\": \"model\",\\\n      \"created\": 1727224290,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"whisper-large-v3\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"OpenAI\",\\\n      \"active\": true,\\\n      \"context_window\": 448,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama-guard-3-8b\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"mixtral-8x7b-32768\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Mistral AI\",\\\n      \"active\": true,\\\n      \"context_window\": 32768,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"gemma-7b-it\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Google\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"distil-whisper-large-v3-en\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Hugging Face\",\\\n      \"active\": true,\\\n      \"context_window\": 448,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama-3.2-1b-preview\",\\\n      \"object\": \"model\",\\\n      \"created\": 1727224268,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama-3.2-90b-text-preview\",\\\n      \"object\": \"model\",\\\n      \"created\": 1727285716,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama3-groq-8b-8192-tool-use-preview\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Groq\",\\\n      \"active\": true,\\\n      \"context_window\": 8192,\\\n      \"public_apps\": null\\\n    },\\\n    {\\\n      \"id\": \"llama-3.1-8b-instant\",\\\n      \"object\": \"model\",\\\n      \"created\": 1693721698,\\\n      \"owned_by\": \"Meta\",\\\n      \"active\": true,\\\n      \"context_window\": 131072,\\\n      \"public_apps\": null\\\n    }\\\n  ]\n}\n```\n\n[**Retrieve model**](https://console.groq.com/docs/api-reference#models-retrieve)\n\nGEThttps://api.groq.com/openai/v1/models/{model}\n\nGet model\n\n### Returns\n\nA model object\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/models/llama-3.3-70b-versatile \\\n-H \"Authorization: Bearer $GROQ_API_KEY\"\n```\n\n```json\n{\n  \"id\": \"llama3-8b-8192\",\n  \"object\": \"model\",\n  \"created\": 1693721698,\n  \"owned_by\": \"Meta\",\n  \"active\": true,\n  \"context_window\": 8192,\n  \"public_apps\": null\n}\n```\n\n[**Batches**](https://console.groq.com/docs/api-reference#batches)\n\n[**Create batch**](https://console.groq.com/docs/api-reference#batches-create)\n\nPOSThttps://api.groq.com/openai/v1/batches\n\nCreates and executes a batch from an uploaded file of requests\n\n### Request Body\n\n- completion\\_windowstringRequired\n\n\n\nThe time frame within which the batch should be processed. Currently only `24h` is supported.\n\n- endpointstringRequired\n\n\n\nThe endpoint to be used for all requests in the batch. Currently `/v1/chat/completions` is supported.\n\n- input\\_file\\_idstringRequired\n\n\n\nThe ID of an uploaded file that contains requests for the new batch.\n\n\n\nSee [upload file](https://console.groq.com/docs/api-reference#files-upload) for how to upload a file.\n\n\n\nYour input file must be formatted as a [JSONL file](https://console.groq.com/docs/batch), and must be uploaded with the purpose `batch`. The file can be up to 100 MB in size.\n\n- metadataobject or nullOptional\n\n\n\nOptional custom metadata for the batch.\n\n\n### Returns\n\nA created batch object\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/batches \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input_file_id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n    \"endpoint\": \"/v1/chat/completions\",\n    \"completion_window\": \"24h\"\n  }'\n```\n\n```json\n{\n  \"id\": \"batch_01jh6xa7reempvjyh6n3yst2zw\",\n  \"object\": \"batch\",\n  \"endpoint\": \"/v1/chat/completions\",\n  \"errors\": null,\n  \"input_file_id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n  \"completion_window\": \"24h\",\n  \"status\": \"validating\",\n  \"output_file_id\": null,\n  \"error_file_id\": null,\n  \"finalizing_at\": null,\n  \"failed_at\": null,\n  \"expired_at\": null,\n  \"cancelled_at\": null,\n  \"request_counts\": {\n    \"total\": 0,\n    \"completed\": 0,\n    \"failed\": 0\n  },\n  \"metadata\": null,\n  \"created_at\": 1736472600,\n  \"expires_at\": 1736559000,\n  \"cancelling_at\": null,\n  \"completed_at\": null,\n  \"in_progress_at\": null\n}\n```\n\n[**Retrieve batch**](https://console.groq.com/docs/api-reference#batches-retrieve)\n\nGEThttps://api.groq.com/openai/v1/batches/{batch\\_id}\n\nRetrieves a batch.\n\n### Returns\n\nA batch object\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/batches/batch_01jh6xa7reempvjyh6n3yst2zw \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n```\n\n```json\n{\n  \"id\": \"batch_01jh6xa7reempvjyh6n3yst2zw\",\n  \"object\": \"batch\",\n  \"endpoint\": \"/v1/chat/completions\",\n  \"errors\": null,\n  \"input_file_id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n  \"completion_window\": \"24h\",\n  \"status\": \"validating\",\n  \"output_file_id\": null,\n  \"error_file_id\": null,\n  \"finalizing_at\": null,\n  \"failed_at\": null,\n  \"expired_at\": null,\n  \"cancelled_at\": null,\n  \"request_counts\": {\n    \"total\": 0,\n    \"completed\": 0,\n    \"failed\": 0\n  },\n  \"metadata\": null,\n  \"created_at\": 1736472600,\n  \"expires_at\": 1736559000,\n  \"cancelling_at\": null,\n  \"completed_at\": null,\n  \"in_progress_at\": null\n}\n```\n\n[**List batches**](https://console.groq.com/docs/api-reference#batches-list)\n\nGEThttps://api.groq.com/openai/v1/batches\n\nList your organization's batches.\n\n### Returns\n\nA list of batches\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/batches \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n```\n\n```json\n{\n  \"object\": \"list\",\n  \"data\": [\\\n    {\\\n      \"id\": \"batch_01jh6xa7reempvjyh6n3yst2zw\",\\\n      \"object\": \"batch\",\\\n      \"endpoint\": \"/v1/chat/completions\",\\\n      \"errors\": null,\\\n      \"input_file_id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\\\n      \"completion_window\": \"24h\",\\\n      \"status\": \"validating\",\\\n      \"output_file_id\": null,\\\n      \"error_file_id\": null,\\\n      \"finalizing_at\": null,\\\n      \"failed_at\": null,\\\n      \"expired_at\": null,\\\n      \"cancelled_at\": null,\\\n      \"request_counts\": {\\\n        \"total\": 0,\\\n        \"completed\": 0,\\\n        \"failed\": 0\\\n      },\\\n      \"metadata\": null,\\\n      \"created_at\": 1736472600,\\\n      \"expires_at\": 1736559000,\\\n      \"cancelling_at\": null,\\\n      \"completed_at\": null,\\\n      \"in_progress_at\": null\\\n    }\\\n  ]\n}\n```\n\n[**Files**](https://console.groq.com/docs/api-reference#files)\n\n[**Upload file**](https://console.groq.com/docs/api-reference#files-upload)\n\nPOSThttps://api.groq.com/openai/v1/files\n\nUpload a file that can be used across various endpoints.\n\nThe Batch API only supports `.jsonl` files up to 100 MB in size. The input also has a specific required [format](https://console.groq.com/docs/batch).\n\nPlease contact us if you need to increase these storage limits.\n\n### Request Body\n\n- filestringRequired\n\n\n\nThe File object (not file name) to be uploaded.\n\n- purposestringRequired\n\n\n\nThe intended purpose of the uploaded file.\nUse \"batch\" for [Batch API](https://console.groq.com/docs/api-reference#batches).\n\n\n### Returns\n\nThe uploaded File object.\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/files \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -F purpose=\"batch\" \\\n  -F \"file=@batch_file.jsonl\"\n```\n\n```json\n{\n  \"id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n  \"object\": \"file\",\n  \"bytes\": 966,\n  \"created_at\": 1736472501,\n  \"filename\": \"batch_file.jsonl\",\n  \"purpose\": \"batch\"\n}\n```\n\n[**List files**](https://console.groq.com/docs/api-reference#files-list)\n\nGEThttps://api.groq.com/openai/v1/files\n\nReturns a list of files.\n\n### Returns\n\nA list of [File](https://console.groq.com/docs/api-reference#files-upload) objects.\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/files \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n```\n\n```json\n{\n  \"object\": \"list\",\n  \"data\": [\\\n    {\\\n      \"id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\\\n      \"object\": \"file\",\\\n      \"bytes\": 966,\\\n      \"created_at\": 1736472501,\\\n      \"filename\": \"batch_file.jsonl\",\\\n      \"purpose\": \"batch\"\\\n    }\\\n  ]\n}\n```\n\n[**Delete file**](https://console.groq.com/docs/api-reference#files-delete)\n\nDELETEhttps://api.groq.com/openai/v1/files/{file\\_id}\n\nDelete a file.\n\n### Returns\n\nA deleted file response object\n\ncurl\n\n```shell\ncurl -X DELETE https://api.groq.com/openai/v1/files/file_01jh6x76wtemjr74t1fh0faj5t \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n```\n\n```json\n{\n  \"id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n  \"object\": \"file\",\n  \"deleted\": true\n}\n```\n\n[**Retrieve file**](https://console.groq.com/docs/api-reference#files-retrieve)\n\nGEThttps://api.groq.com/openai/v1/files/{file\\_id}\n\nReturns information about a file.\n\n### Returns\n\nA file object\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/files/file_01jh6x76wtemjr74t1fh0faj5t \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n```\n\n```json\n{\n  \"id\": \"file_01jh6x76wtemjr74t1fh0faj5t\",\n  \"object\": \"file\",\n  \"bytes\": 966,\n  \"created_at\": 1736472501,\n  \"filename\": \"batch_file.jsonl\",\n  \"purpose\": \"batch\"\n}\n```\n\n[**Download file**](https://console.groq.com/docs/api-reference#files-download)\n\nGEThttps://api.groq.com/openai/v1/files/{file\\_id}/content\n\nReturns the contents of the specified file.\n\n### Returns\n\nThe file content\n\ncurl\n\n```shell\ncurl https://api.groq.com/openai/v1/files/file_01jh6x76wtemjr74t1fh0faj5t/content \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n```",
    "metadata": {
      "url": "https://console.groq.com/docs/api-reference#chat-create",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "21e16a18-17df-44ae-b5c2-f132ac5815f8",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/api-reference",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)",
    "metadata": {
      "url": "https://console.groq.com/playground",
      "ogUrl": "https://console.groq.com",
      "title": "Playground - GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "69b6f679-422e-48ba-b694-0f8bba576c70",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/playground",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)",
    "metadata": {
      "url": "https://console.groq.com/metrics",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "02d40961-4978-410d-9e41-85481b0cd87a",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/metrics",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Apps Showcase\n\nDiscover the incredible speed of Groq with fully-developed apps from our team and community!\n\n**If you're interested adding your project to our showcase, please fill out and submit [this form](https://forms.gle/bQxD88MAxCMeksqt7) for our team to review.**\n\n* * *\n\n[**Groq App Generator**](https://console.groq.com/docs/showcase-applications#groq-app-generator)\n\nAuthor: Rick Lamers, Jose Menendez, Benjamin Klieger\n\nAn interactive web application that generates and modifies web applications in microseconds using Groq API.\n\n[Github](https://github.com/groq/groq-appgen) [Live Demo](https://appgen.groqlabs.com/)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/appgen.png)](https://github.com/groq/groq-appgen)\n\n* * *\n\n* * *\n\n[**Groq ChangeLog Generator**](https://console.groq.com/docs/showcase-applications#groq-changelog-generator)\n\nAuthor: Cole McCracken\n\nCLI tool powered by LangGraph and Groq to automatically generate changelogs from git commit history.\n\n[Github](https://github.com/colemccracken/changelog)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/changelog.png)](https://github.com/colemccracken/changelog)\n\n* * *\n\n* * *\n\n[**Groq x Gradio Voice Assistant**](https://console.groq.com/docs/showcase-applications#groq-x-gradio-voice-assistant)\n\nAuthor: Hatice Ozen\n\nVoice-powered AI application using Groq for realtime speech recognition and Gradio for creating an interactive web interface.\n\n[Github](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/groq-gradio/groq-gradio-tutorial.ipynb) [Live Demo](https://huggingface.co/spaces/Groq/groq-gradio-voice-assistant)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/voiceapp.png)](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/groq-gradio/groq-gradio-tutorial.ipynb)\n\n* * *\n\n[**Microagent Framework**](https://console.groq.com/docs/showcase-applications#microagent-framework)\n\nAuthor: Chris Latimer\n\nLightweight framework for orchestrating multi-agent systems, inspired by and forked from OpenAI's Swarm project for support with Groq and Anthropic.\n\n[Github](https://github.com/chrislatimer/microagent)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/microagent.png)](https://github.com/chrislatimer/microagent)\n\n* * *\n\n* * *\n\n[**Shell-AI**](https://console.groq.com/docs/showcase-applications#shell-ai)\n\nAuthor: Rick Lamers\n\nCLI utility that brings the power of natural language understanding to your command line powered by Groq.\n\n[Github](https://github.com/ricklamers/shell-ai)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/shellai.gif)](https://github.com/ricklamers/shell-ai)\n\n* * *\n\n* * *\n\n[**Groq Draw and Guess**](https://console.groq.com/docs/showcase-applications#groq-draw-and-guess)\n\nAuthor: Jose Menendez\n\nFun use of LLaVA 1.5 7B powered by Groq to play a pictionary style game where you draw and the vision model guesses.\n\n[Github](https://github.com/jose-mdz/draw-and-guess) [Live Demo](https://draw.geeksplainer.wtf/)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/drawandguess.png)](https://github.com/jose-mdz/draw-and-guess)\n\n* * *\n\n* * *\n\n[**Magic Spell**](https://console.groq.com/docs/showcase-applications#magic-spell)\n\nAuthor: Ai-ng and Nick Oates\n\nAI-powered text editor built with Next.js, Vercel AI SDK and Groq. Deploy your own AI-powered text editor.\n\n[Github](https://github.com/ai-ng/magic-spell) [Live Demo](https://magic-spell.vercel.app/)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/magic-spell.png)](https://github.com/ai-ng/magic-spell)\n\n* * *\n\n[**Open Devin**](https://console.groq.com/docs/showcase-applications#open-devin)\n\nAuthor: Mervin Praison\n\nThis project aspires to replicate, enhance, and innovate upon Devin through the power of the open-source community. -- powered by NodeJS, Docker and Groq.\n\n[Github](https://github.com/OpenDevin/OpenDevin) [YouTube Tutorial](https://www.youtube.com/watch?v=3-q5GzRNEe0)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/open-devin.png)](https://github.com/OpenDevin/OpenDevin)\n\n* * *\n\n[**Streamlit AI Chatbot**](https://console.groq.com/docs/showcase-applications#streamlit-ai-chatbot)\n\nAuthor: Tony Kipkemboi\n\nBuild a Streamlit AI chatbot using Groq.\n\n[Github](https://github.com/tonykipkemboi/groq_streamlit_demo) [YouTube Tutorial](https://www.youtube.com/watch?v=WQvinJGYk90)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/streamlit.png)](https://github.com/tonykipkemboi/groq_streamlit_demo)\n\n* * *\n\n[**Hey Gemma**](https://console.groq.com/docs/showcase-applications#hey-gemma)\n\nAuthor: gabrielchua\n\nA voice interface on top of Gemma 7B, powered by Groq.\n\n[Gradio Repo](https://huggingface.co/spaces/cyzgab/hey-gemma/tree/main)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/hey-gemma.png)](https://huggingface.co/spaces/cyzgab/hey-gemma/tree/main)\n\n* * *\n\n[**RAG with LlamaParse**](https://console.groq.com/docs/showcase-applications#rag-with-llamaparse)\n\nAuthor: Sudarshan Koirala\n\nBuild an effective RAG pipeline using multiple file formats -- powered by LlamaParse, Qdrant and Groq.\n\n[GitHub](https://github.com/sudarshan-koirala/llamaparser-example) [YouTube Tutorial](https://www.youtube.com/watch?v=w7Ap6gZFXl0)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/rag-llamaparse.png)](https://github.com/sudarshan-koirala/llamaparser-example)\n\n* * *\n\n[**Answer Engine**](https://console.groq.com/docs/showcase-applications#answer-engine)\n\nAuthor: Developer's Digest\n\nFollow the Developer's Digest team as they build a Perplexity-like Answer Engine with Groq, Mixtral, Langchain and Brave.\n\n[GitHub](https://github.com/developersdigest/llm-answer-engine) [YouTube Tutorial](https://www.youtube.com/watch?v=kFC-OWw7G8k)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/llm-answer-engine.png)](https://github.com/developersdigest/llm-answer-engine)\n\n* * *\n\n[**Gradio + Groq = 😍**](https://console.groq.com/docs/showcase-applications#gradio-+-groq-=-%F0%9F%98%8D)\n\nAuthor: gabrielchua\n\nA simple Gradio app showcasing fast inference and LLM-powered autocomplete powered by Groq.\n\n[Gradio Repo](https://huggingface.co/spaces/cyzgab/catch-me-if-you-can/blob/main/app.py)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/gradio-demo.png)](https://huggingface.co/spaces/cyzgab/catch-me-if-you-can/blob/main/app.py)\n\n* * *\n\n[**Quick Voice Bot with Deepgram**](https://console.groq.com/docs/showcase-applications#quick-voice-bot-with-deepgram)\n\nAuthor: Greg Kamradt\n\nAI voice bot demo that uses Text-To-Speech and Speech-To-Text powered by Groq.\n\n[GitHub](https://github.com/gkamradt/QuickAgent) [YouTube Tutorial](https://www.youtube.com/watch?v=J2sbC8X5Pp8)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/deepgram.png)](https://github.com/gkamradt/QuickAgent)\n\n* * *\n\n[**Crazy Fast RAG**](https://console.groq.com/docs/showcase-applications#crazy-fast-rag)\n\nAuthor: Sudarshan Koirala\n\nRAG Chatbot with simple UI built on open-embedding model nomic-embed-text via Ollama and powered by Groq.\n\n[GitHub](https://github.com/sudarshan-koirala/rag-chat-with-pdf) [YouTube Tutorial](https://www.youtube.com/watch?v=TMaQt8rN5bE)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/crazy-fast-rag.png)](https://github.com/sudarshan-koirala/rag-chat-with-pdf)\n\n* * *\n\n[**Chat with Groq Docs**](https://console.groq.com/docs/showcase-applications#chat-with-groq-docs)\n\nAuthor: Groq\n\nChat with Groq Docs...with Groq! Powered by Langchain.\n\n[Live Demo](https://docs-chat.groqcloud.com/)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/groq-docs-chat.png)](https://docs-chat.groqcloud.com/)\n\n* * *\n\n[**InstantRefactor**](https://console.groq.com/docs/showcase-applications#instantrefactor)\n\nAuthor: @mattshumer\n\nUse Groq to instantly refactor and document python code.\n\n[Live Demo](https://instant-refactor.streamlit.app/)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/instantrefactor.png)](https://instant-refactor.streamlit.app/)\n\n* * *\n\n[**Agents go brrrr with Groq**](https://console.groq.com/docs/showcase-applications#agents-go-brrrr-with-groq)\n\nAuthor: @gabchuayz\n\nA simple react search agent built with LangChain, powered by Groq, Mixtral and Tavily.\n\n[Github](https://github.com/gabrielchua/groq-st-demo/tree/main) [Live Demo](https://groq-react.streamlit.app/)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/agents-go-brrrr.png)](https://github.com/gabrielchua/groq-st-demo/tree/main)\n\n* * *\n\n[**ZeroBot.ai**](https://console.groq.com/docs/showcase-applications#zerobot.ai)\n\nAuthor: ZeroBot.ai\n\nThe Internet's #1 voice-enabled chatbot, at Groq speed.\n\n[Live Demo](https://www.zerobot.ai/)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/zerobotAI.png)](https://www.zerobot.ai/)\n\n* * *\n\n[**ConsiLLiuM**](https://console.groq.com/docs/showcase-applications#consillium)\n\nAuthor: @FelipeSchieber\n\nA generative hierarchical Wikipedia-like app powered by Groq for blazingly fast article generation.\n\n[Live Demo](https://consillium.vercel.app/)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/consillium.png)](https://consillium.vercel.app/)\n\n* * *\n\n[**Real time voice assistant with Groq**](https://console.groq.com/docs/showcase-applications#real-time-voice-assistant-with-groq)\n\nAuthor: Serkan Dayicik\n\nVoice-driven interactions with groq, a real-time voice assistant that seamlessly blends Next.js for web functionality, Groq for LLM, Deepgram for live transcription TTS with Neets for TTS.\n\n[Github](https://github.com/serkandyck/realtime-voice-assistant-groq)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/realtimevoice.png)](https://github.com/serkandyck/realtime-voice-assistant-groq)\n\n* * *\n\n[**NatterGPT**](https://console.groq.com/docs/showcase-applications#nattergpt)\n\nAuthor: @HelloGnbly\n\nNatterGPT calls your leads/customers, evaluates their interest level and gets you a report.\n\n[Live Demo](https://nattergpt.com/)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/nattergpt.png)](https://nattergpt.com/)\n\n* * *\n\n[**Fast Conversational Agent**](https://console.groq.com/docs/showcase-applications#fast-conversational-agent)\n\nAuthor: @StonkyOli\n\nA demo showcasing a fast conversational agent, powered by Azure, PlayHT and Groq inference.\n\n[Video Demo](https://twitter.com/StonkyOli/status/1762551140829515964)\n\n[![Visual Demo](https://console.groq.com/showcase-applications/fast-convo-agent.png)](https://twitter.com/StonkyOli/status/1762551140829515964)",
    "metadata": {
      "url": "https://console.groq.com/docs/showcase-applications",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "5fc74a2a-e3fa-4399-9b5e-cf0e3de7db0e",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/showcase-applications",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## 🎨 Gradio + Groq: Easily Build Web Interfaces\n\n[Gradio](https://www.gradio.app/) is a powerful library for creating web interfaces for your applications that enables you to quickly build\ninteractive demos for your fast Groq apps with features such as:\n\n- **Interface Builder:** Create polished UIs with just a few lines of code, supporting text, images, audio, and more\n- **Interactive Demos:** Build demos that showcase your LLM applications with multiple input/output components\n- **Shareable Apps:** Deploy and share your Groq-powered applications with a single click\n\n### [Quick Start (2 minutes to hello world)](https://console.groq.com/docs/gradio\\#quick-start-2-minutes-to-hello-world)\n\n#### 1\\. Install the packages:\n\n```bash\npip install groq-gradio\n```\n\n#### 2\\. Set up your API key:\n\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\n```\n\n#### 3\\. Create your first Gradio chat interface:\n\nThe following code creates a simple chat interface with `llama-3.3-70b-versatile` that includes a clean UI.\n\n```python\nimport gradio as gr\nimport groq_gradio\nimport os\n\n# Initialize Groq client\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\")\n)\n\ngr.load(\n    name='llama-3.3-70b-versatile', # The specific model powered by Groq to use\n    src=groq_gradio.registry, # Tells Gradio to use our custom interface registry as the source\n    title='Groq-Gradio Integration', # The title shown at the top of our UI\n    description=\"Chat with the Llama 3.3 70B model powered by Groq.\", # Subtitle\n    examples=[\"Explain quantum gravity to a 5-year old.\", \"How many R are there in the word Strawberry?\"] # Pre-written prompts users can click to try\n).launch() # Creates and starts the web server!\n```\n\n**Challenge**: Enhance the above example to create a multi-modal chatbot that leverages text, audio, and vision models powered by Groq and\ndisplayed on a customized UI built with Gradio blocks!\n\nFor more information on building robust applications with Gradio and Groq, see:\n\n- [Official Documentation: Gradio](https://www.gradio.app/docs)\n- [Tutorial: Automatic Voice Detection with Groq](https://www.gradio.app/guides/automatic-voice-detection)\n- [Groq API Cookbook: Groq and Gradio for Realtime Voice-Powered AI Applications](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/groq-gradio/groq-gradio-tutorial.ipynb)\n- [Webinar: Building a Multimodal Voice Enabled Calorie Tracking App with Groq and Gradio](https://youtu.be/azXaioGdm2Q?si=sXPJW1IerbghsCKU)",
    "metadata": {
      "url": "https://console.groq.com/docs/gradio",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "14c78970-900f-45bb-9265-88957179c711",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/gradio",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Groq Batch API\n\nProcess large-scale workloads asynchronously with our Batch API.\n\n### [What is Batch Processing?](https://console.groq.com/docs/batch\\#what-is-batch-processing)\n\nBatch processing lets you run thousands of API requests at scale by submitting your workload as a batch to Groq and letting us process it with\na 24-hour turnaround.\n\nWhile synchronous API calls are perfect for our fast inference speed for realtime applications, asynchronous batch processing is perfect for use\ncases where volume of data matters more than synchronous responses, such as processing large datasets, generating content in bulk, and running\nevaluations. Compared to using our synchronous endpoints, Batch API has:\n\n- **Higher rate limits:** Substantially increased limits compared to on-demand APIs\n- **24-hour (or less) turnaround:** Each batch completes within 24 hours (or often more quickly)\n\n## Model Availability\n\nThe Batch API can currently be used to execute queries for text inputs with llama-3.3-70b-versatile and llama-3.1-8b-instant.\n\n## Model Pricing\n\n| Model | Input Token price (Per Million Tokens) | Output Token price (Per Million Tokens) |\n| --- | --- | --- |\n| llama-3.1-8b-instant | $0.0375 | $0.0600 |\n| llama-3.3-70b-versatile | $0.442 | $0.592 |\n\n## Getting Started\n\n### [1\\. Prepare Your Batch File](https://console.groq.com/docs/batch\\#1-prepare-your-batch-file)\n\nA batch is composed of a list of API requests and every batch job starts with a JSON Lines (JSONL) file that contains the requests\nyou want processed. Each line in this file represents a single API call.\n\nThe Groq Batch API currently supports chat completion requests through `/v1/chat/completions`.\n\nThe structure for each line must include:\n\n- `custom_id`: Your unique identifier for tracking the batch request\n- `method`: The HTTP method (currently `POST` only)\n- `url`: The API endpoint to call ( `/v1/chat/completions`)\n- `body`: The parameters of your request matching our synchronous API format. See our API Reference [here.](https://console.groq.com/docs/api-reference#chat-create)\n\nThe following is an example of a JSONL batch file:\n\n```json\n{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is 2+2?\"}]}}\n{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is 2+3?\"}]}}\n{\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"count up to 1000000. starting with 1, 2, 3. print all the numbers, do not stop until you get to 1000000.\"}]}}\n```\n\n#### Converting Sync Calls to Batch Format\n\nIf you're familiar with making synchronous API calls, converting them to batch format is straightforward. Here's how a regular API call transforms\ninto a batch request:\n\n```json\n# Your typical synchronous API call:\nresponse = client.chat.completions.create(\n    model=\"llama-3.1-8b-instant\",\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"What is quantum computing?\"}\\\n    ]\n)\n\n# The same call in batch format (must be on a single line as JSONL):\n{\"custom_id\": \"quantum-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"user\", \"content\": \"What is quantum computing?\"}]}}\n```\n\n### [2\\. Upload Your Batch File](https://console.groq.com/docs/batch\\#2-upload-your-batch-file)\n\nUpload your `.jsonl` batch file using the Files API endpoint for when kicking off your batch job:\n\n**Note:** The Files API currently only supports `.jsonl` files 50,000 lines or less and up to maximum of 200MB in size.\n\nPythonJavaScriptcurl\n\n```py\n1import requests # pip install requests first!\n2\n3def upload_file_to_groq(api_key, file_path):\n4    url = \"https://api.groq.com/openai/v1/files\"\n5\n6    headers = {\n7        \"Authorization\": f\"Bearer {api_key}\"\n8    }\n9\n10    # Prepare the file and form data\n11    files = {\n12        \"file\": (\"batch_file.jsonl\", open(file_path, \"rb\"))\n13    }\n14\n15    data = {\n16        \"purpose\": \"batch\"\n17    }\n18\n19    # Make the POST request\n20    response = requests.post(url, headers=headers, files=files, data=data)\n21\n22    return response.json()\n23\n24# Usage example\n25api_key = \"YOUR_GROQ_API_KEY\"  # Replace with your actual API key\n26file_path = \"batch_file.jsonl\"  # Path to your JSONL file\n27\n28try:\n29    result = upload_file_to_groq(api_key, file_path)\n30    print(result)\n31except Exception as e:\n32    print(f\"Error: {e}\")\n```\n\nYou will receive a JSON response that contains the ID ( `id`) for your file object that you will then use to create your batch job:\n\n```json\n{\n    \"id\":\"file_01jh6x76wtemjr74t1fh0faj5t\",\n    \"object\":\"file\",\n    \"bytes\":966,\n    \"created_at\":1736472501,\n    \"filename\":\"input_file.jsonl\",\n    \"purpose\":\"batch\"\n}\n```\n\n### [3\\. Create Your Batch Job](https://console.groq.com/docs/batch\\#3-create-your-batch-job)\n\nOnce you've uploaded your `.jsonl` file, you can use the file object ID (in this case, `file_01jh6x76wtemjr74t1fh0faj5t` as shown in Step 2) to create a batch:\n\n**Note:** For now, the completion window for batch jobs can only be set to 24 hours ( `24h`).\n\nPythonJavaScriptcurl\n\n```py\n1import requests # pip install requests first!\n2\n3def create_batch(api_key, input_file_id):\n4    url = \"https://api.groq.com/openai/v1/batches\"\n5\n6    headers = {\n7        \"Authorization\": f\"Bearer {api_key}\",\n8        \"Content-Type\": \"application/json\"\n9    }\n10\n11    data = {\n12        \"input_file_id\": input_file_id,\n13        \"endpoint\": \"/v1/chat/completions\",\n14        \"completion_window\": \"24h\"\n15    }\n16\n17    response = requests.post(url, headers=headers, json=data)\n18    return response.json()\n19\n20# Usage example\n21api_key = \"YOUR_GROQ_API_KEY\"\n22file_id = \"file_01jh6x76wtemjr74t1fh0faj5t\" # replace with your `id` from file upload API response object\n23\n24try:\n25    result = create_batch(api_key, file_id)\n26    print(result)\n27except Exception as e:\n28    print(f\"Error: {e}\")\n```\n\nThis request will return a Batch object with metadata about your batch, including the batch `id` that you can use to check the status of your batch:\n\n```json\n{\n    \"id\":\"batch_01jh6xa7reempvjyh6n3yst2zw\",\n    \"object\":\"batch\",\n    \"endpoint\":\"/v1/chat/completions\",\n    \"errors\":null,\n    \"input_file_id\":\"file_01jh6x76wtemjr74t1fh0faj5t\",\n    \"completion_window\":\"24h\",\n    \"status\":\"validating\",\n    \"output_file_id\":null,\n    \"error_file_id\":null,\n    \"finalizing_at\":null,\n    \"failed_at\":null,\n    \"expired_at\":null,\n    \"cancelled_at\":null,\n    \"request_counts\":{\n        \"total\":0,\n        \"completed\":0,\n        \"failed\":0\n    },\n    \"metadata\":null,\n    \"created_at\":1736472600,\n    \"expires_at\":1736559000,\n    \"cancelling_at\":null,\n    \"completed_at\":null,\n    \"in_progress_at\":null\n}\n```\n\n### [4\\. Check Batch Status](https://console.groq.com/docs/batch\\#4-check-batch-status)\n\nYou can check the status of a batch any time your heart desires with the batch `id` (in this case, `batch_01jh6xa7reempvjyh6n3yst2zw` from the above Batch response object), which will also return a Batch object:\n\nPythonJavaScriptcurl\n\n```py\n1import requests # pip install requests first!\n2\n3def get_batch_status(api_key, batch_id):\n4    url = f\"https://api.groq.com/openai/v1/batches/{batch_id}\"\n5\n6    headers = {\n7        \"Authorization\": f\"Bearer {api_key}\",\n8        \"Content-Type\": \"application/json\"\n9    }\n10\n11    response = requests.get(url, headers=headers)\n12    return response.json()\n13\n14# Usage example\n15api_key = \"YOUR_GROQ_API_KEY\"\n16batch_id = \"batch_01jh6xa7reempvjyh6n3yst2zw\"\n17\n18try:\n19    result = get_batch_status(api_key, batch_id)\n20    print(result)\n21except Exception as e:\n22    print(f\"Error: {e}\")\n```\n\nThe status of a given batch job can return any of the following status codes:\n\n| Status | Description |\n| --- | --- |\n| `validating` | batch file is being validated before the batch processing begins |\n| `failed` | batch file has failed the validation process |\n| `in_progress` | batch file was successfully validated and the batch is currently being run |\n| `finalizing` | batch has completed and the results are being prepared |\n| `completed` | batch has been completed and the results are ready |\n| `expired` | batch was not able to be completed within the 24-hour time window |\n| `cancelling` | batch is being cancelled (may take up to 10 minutes) |\n| `cancelled` | batch was cancelled |\n\nWhen your batch job is complete, the Batch object will return an `output_file_id` and/or an `error_file_id` that you can then use to retrieve\nyour results (as shown below in Step 5). Here's an example:\n\n```json\n{\n    \"id\":\"batch_01jh6xa7reempvjyh6n3yst2zw\",\n    \"object\":\"batch\",\n    \"endpoint\":\"/v1/chat/completions\",\n    \"errors\":[\\\n        {\\\n            \"code\":\"invalid_method\",\\\n            \"message\":\"Invalid value: 'GET'. Supported values are: 'POST'\",\"param\":\"method\",\\\n            \"line\":4\\\n        }\\\n    ],\n    \"input_file_id\":\"file_01jh6x76wtemjr74t1fh0faj5t\",\n    \"completion_window\":\"24h\",\n    \"status\":\"completed\",\n    \"output_file_id\":\"file_01jh6xa97be52b7pg88czwrrwb\",\n    \"error_file_id\":\"file_01jh6xa9cte52a5xjnmnt5y0je\",\n    \"finalizing_at\":null,\n    \"failed_at\":null,\n    \"expired_at\":null,\n    \"cancelled_at\":null,\n    \"request_counts\":\n    {\n        \"total\":3,\n        \"completed\":2,\n        \"failed\":1\n    },\n    \"metadata\":null,\n    \"created_at\":1736472600,\n    \"expires_at\":1736559000,\n    \"cancelling_at\":null,\n    \"completed_at\":1736472607,\n    \"in_progress_at\":1736472601\n}\n```\n\n### [5\\. Retrieve Batch Results](https://console.groq.com/docs/batch\\#5-retrieve-batch-results)\n\nNow for the fun. Once the batch is complete, you can retrieve the results using the `output_file_id` from your Batch object (in this case, `file_01jh6xa97be52b7pg88czwrrwb` from the above Batch response object) and write it to\na file on your machine ( `batch_output.jsonl` in this case) to view them:\n\nPythonJavaScriptcurl\n\n```py\n1import requests # pip install requests first!\n2\n3def download_file_content(api_key, output_file_id, output_file):\n4    url = f\"https://api.groq.com/openai/v1/files/{output_file_id}/content\"\n5\n6    headers = {\n7        \"Authorization\": f\"Bearer {api_key}\"\n8    }\n9\n10    response = requests.get(url, headers=headers)\n11\n12    # Write the content to a file\n13    with open(output_file, 'wb') as f:\n14        f.write(response.content)\n15\n16    return f\"File downloaded successfully to {output_file}\"\n17\n18# Usage example\n19api_key = \"YOUR_GROQ_API_KEY\"\n20output_file_id = \"file_01jh6xa97be52b7pg88czwrrwb\" # replace with your own completed batch job's `output_file_id`\n21output_file = \"batch_output.jsonl\" # replace with your own file of choice to download batch job contents to\n22\n23try:\n24    result = download_file_content(api_key, file_id, output_file)\n25    print(result)\n26except Exception as e:\n27    print(f\"Error: {e}\")\n```\n\nThe output `.jsonl` file will have one response line per successful request line of your batch file. Each line includes the original `custom_id`\nfor mapping results, a unique batch request ID, and the response:\n\n```json\n{\"id\": \"batch_req_123\", \"custom_id\": \"my-request-1\", \"response\": {\"status_code\": 200, \"request_id\": \"req_abc\", \"body\": {\"id\": \"completion_xyz\", \"model\": \"llama-3.1-8b-instant\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Hello!\"}}], \"usage\": {\"prompt_tokens\": 20, \"completion_tokens\": 5, \"total_tokens\": 25}}}, \"error\": null}\n```\n\nAny failed requests in the batch will have their error information written to an error file that can be accessed via the batch's `error_file_id`.\n\n**Note:** Results may not appears in the same order as your batch request submissions. Always use the `custom_id` field to match results with your\noriginal request.\n\n## List Batches\n\nYou can view all your batch jobs by making a call to `https://api.groq.com/v1/batches`:\n\n```sh\ncurl https://api.groq.com/v1/batches \\\n  -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n```\n\n## Batch Expiration\n\nInput, intermediate files, and results from processed batches will be stored securely for up to 30 days in Groq's systems. You may also immediately delete once a processed batch is retrieved.\n\n## 24h Completion Window\n\nNote that the 24-hour completion window only applies during the `in_progress` phase. Although the finalization stage may take longer than 24 hours, completion is still required for responses to be made available.\n\n## Rate limits\n\nThe Batch API rate limits are separate than existing per-model rate limits for synchronous requests. Using the Batch API will not consume tokens\nfrom your standard per-model limits, which means you can conveniently leverage batch processing to increase the number of tokens you process with\nus.\n\nSee your limits [here.](https://console.groq.com/settings/limits)",
    "metadata": {
      "url": "https://console.groq.com/docs/batch",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "66a06d00-1065-4cc5-828b-87034ebe4abe",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/batch",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## ✨ Vercel AI SDK + Groq: Rapid App Development\n\nVercel's AI SDK enables seamless integration with Groq, providing developers with powerful tools to leverage language models hosted on Groq for a variety of applications. By combining Vercel's cutting-edge platform with Groq's advanced inference capabilities, developers can create scalable, high-speed applications with ease.\n\n### [Why Choose the Vercel AI SDK?](https://console.groq.com/docs/ai-sdk\\#why-choose-the-vercel-ai-sdk)\n\n- A versatile toolkit for building applications powered by advanced language models like Llama 3.3 70B\n- Ideal for creating chat interfaces, document summarization, and natural language generation\n- Simple setup and flexible provider configurations for diverse use cases\n- Fully supports standalone usage and seamless deployment with Vercel\n- Scalable and efficient for handling complex tasks with minimal configuration\n\n### [Quick Start Guide in JavaScript (5 minutes to deployment)](https://console.groq.com/docs/ai-sdk\\#quick-start-guide-in-javascript-5-minutes-to-deployment)\n\n#### 1\\. Create a new Next.js project with the AI SDK template:\n\n```bash\nnpx create-next-app@latest my-groq-app --typescript --tailwind --src-dir\ncd my-groq-app\n```\n\n#### 2\\. Install the required packages:\n\n```bash\nnpm install @ai-sdk/groq ai\nnpm install react-markdown\n```\n\n#### 3\\. Create a `.env.local` file in your project root and configure your Groq API Key:\n\n```bash\nGROQ_API_KEY=\"your-api-key\"\n```\n\n#### 4\\. Create a new directory structure for your Groq API endpoint:\n\n```bash\nmkdir -p src/app/api/chat\n```\n\n#### 5\\. Initialize the AI SDK by creating an API route file called `route.ts` in `app/api/chat`:\n\n```javascript\nimport { groq } from '@ai-sdk/groq';\nimport { generateText } from 'ai';\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: groq('llama-3.3-70b-versatile'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n**Challenge**: Now that you have your basic chat interface working, try enhancing it to create a specialized code explanation assistant!\n\n#### 6\\. Create your front end interface by updating the `app/page.tsx` file:\n\n```javascript\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n\n  return (\n    <div className=\"min-h-screen bg-white\">\n      <div className=\"mx-auto w-full max-w-2xl py-8 px-4\">\n        <div className=\"space-y-4 mb-4\">\n          {messages.map(m => (\n            <div\n              key={m.id}\n              className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}\n            >\n              <div\n                className={`\n                  max-w-[80%] rounded-lg px-4 py-2\n                  ${m.role === 'user'\n                    ? 'bg-blue-100 text-black'\n                    : 'bg-gray-100 text-black'}\n                `}\n              >\n                <div className=\"text-xs text-gray-500 mb-1\">\n                  {m.role === 'user' ? 'You' : 'Llama 3.3 70B powered by Groq'}\n                </div>\n                <div className=\"text-sm whitespace-pre-wrap\">\n                  {m.content}\n                </div>\n              </div>\n            </div>\n          ))}\n        </div>\n\n        <form onSubmit={handleSubmit} className=\"flex gap-4\">\n          <input\n            value={input}\n            onChange={handleInputChange}\n            placeholder=\"Type your message...\"\n            className=\"flex-1 rounded-lg border border-gray-300 px-4 py-2 text-black focus:outline-none focus:ring-2 focus:ring-[#f55036]\"\n          />\n          <button\n            type=\"submit\"\n            className=\"rounded-lg bg-[#f55036] px-4 py-2 text-white hover:bg-[#d94530] focus:outline-none focus:ring-2 focus:ring-[#f55036]\"\n          >\n            Send\n          </button>\n        </form>\n      </div>\n    </div>\n  );\n}\n```\n\n#### 7\\. Run your development enviornment to test our application locally:\n\n```bash\nnpm run dev\n```\n\n#### 8\\. Easily deploy your application using Vercel CLI by installing `vercel` and then running the `vercel` command:\n\nThe CLI will guide you through a few simple prompts:\n\n- If this is your first time using Vercel CLI, you'll be asked to create an account or log in\n- Choose to link to an existing Vercel project or create a new one\n- Confirm your deployment settings\n\nOnce you've gone through the prompts, your app will be deployed instantly and you'll receive a production URL! 🚀\n\n```bash\nnpm install -g vercel\nvercel\n```\n\n### [Additional Resources](https://console.groq.com/docs/ai-sdk\\#additional-resources)\n\nFor more details on integrating Groq with the Vercel AI SDK, see the following:\n\n- [Official Documentation: Vercel](https://sdk.vercel.ai/providers/ai-sdk-providers/groq)\n- [Vercel Templates for Groq](https://sdk.vercel.ai/providers/ai-sdk-providers/groq)",
    "metadata": {
      "url": "https://console.groq.com/docs/ai-sdk",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "07711e37-4778-407c-be4a-f41135c7544d",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/ai-sdk",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## E2B + Groq: Open-Source Code Interpreter\n\n[E2B](https://e2b.dev/) Code Interpreter is an open-source SDK that provides secure, sandboxed environments for executing code generated by LLMs via Groq API. Built specifically for AI data analysts,\ncoding applications, and reasoning-heavy agents, E2B enables you to both generate and execute code in a secure sandbox environment in real-time.\n\n### [Python Quick Start (3 minutes to hello world)](https://console.groq.com/docs/e2b\\#python-quick-start-3-minutes-to-hello-world)\n\n#### 1\\. Install the required packages:\n\n```bash\npip install groq e2b-code-interpreter python-dotenv\n```\n\n#### 2\\. Configure your Groq and [E2B](https://e2b.dev/docs) API keys:\n\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\nexport E2B_API_KEY=\"your-e2b-api-key\"\n```\n\n#### 3\\. Create your first simple and fast Code Interpreter application that generates and executes code to analyze data:\n\nRunning the below code will create a secure sandbox environment, generate Python code using `llama-3.3-70b-versatile` powered by Groq, execute the code, and display the results. When you go to your\n[E2B Dashboard](https://e2b.dev/dashboard), you'll see your sandbox's data.\n\n````python\nfrom e2b_code_interpreter import Sandbox\nfrom groq import Groq\nimport os\n\ne2b_api_key = os.environ.get('E2B_API_KEY')\ngroq_api_key = os.environ.get('GROQ_API_KEY')\n\n# Initialize Groq client\nclient = Groq(api_key=groq_api_key)\n\nSYSTEM_PROMPT = \"\"\"You are a Python data scientist. Generate simple code that:\n1. Uses numpy to generate 5 random numbers\n2. Prints only the mean and standard deviation in a clean format\nExample output format:\nMean: 5.2\nStd Dev: 1.8\"\"\"\n\ndef main():\n    # Create sandbox instance (by default, sandbox instances stay alive for 5 mins)\n    sbx = Sandbox()\n\n    # Get code from Groq\n    response = client.chat.completions.create(\n        model=\"llama-3.1-70b-versatile\",\n        messages=[\\\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\\\n            {\"role\": \"user\", \"content\": \"Generate random numbers and show their mean and standard deviation\"}\\\n        ]\n    )\n\n    # Extract and run the code\n    code = response.choices[0].message.content\n    if \"```python\" in code:\n        code = code.split(\"```python\")[1].split(\"```\")[0]\n\n    print(\"\\nGenerated Python code:\")\n    print(code)\n\n    print(\"\\nExecuting code in sandbox...\")\n    execution = sbx.run_code(code)\n    print(execution.logs.stdout[0])\n\nif __name__ == \"__main__\":\n    main()\n````\n\n**Challenge**: Try modifying the example to analyze your own dataset or solve a different data science problem!\n\nFor more detailed documentation and resources on building with E2B and Groq, see:\n\n- [Tutorial: Code Interpreting with Groq (Python)](https://e2b.dev/blog/guide-code-interpreting-with-groq-and-e2b)\n- [Tutorial: Code Interpreting with Groq (JavaScript)](https://e2b.dev/blog/guide-groq-js)",
    "metadata": {
      "url": "https://console.groq.com/docs/e2b",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "37bfc682-8608-4985-97f9-3af301876cfb",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/e2b",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## OpenAI Compatibility\n\nWe designed Groq API to be mostly compatible with OpenAI's client libraries, making it easy to\nconfigure your existing applications to run on Groq and try our inference speed.\n\nWe also have our own [Groq Python and Groq TypeScript libraries](https://console.groq.com/docs/libraries) that we encourage you to use.\n\n### [Configuring OpenAI to Use Groq API](https://console.groq.com/docs/openai\\#configuring-openai-to-use-groq-api)\n\nTo start using Groq with OpenAI's client libraries, pass your Groq API key to the `api_key` parameter\nand change the `base_url` to `https://api.groq.com/openai/v1`:\n\nPythonJavaScript\n\n```python\nimport os\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"https://api.groq.com/openai/v1\",\n    api_key=os.environ.get(\"GROQ_API_KEY\")\n)\n```\n\nYou can find your API key [here](https://console.groq.com/keys).\n\n### [Currently Unsupported OpenAI Features](https://console.groq.com/docs/openai\\#currently-unsupported-openai-features)\n\nNote that although Groq API is mostly OpenAI compatible, there are a few features we don't support just yet:\n\n#### Text Completions\n\nThe following fields are currently not supported and will result in a 400 error (yikes) if they are supplied:\n\n- `logprobs`\n\n- `logit_bias`\n\n- `top_logprobs`\n\n- `messages[].name`\n\n- If `N` is supplied, it must be equal to 1.\n\n\n#### Temperature\n\nIf you set a `temperature` value of 0, it will be converted to `1e-8`. If you run into any issues, please try setting the value to a float32 `> 0` and `<= 2`.\n\n#### Audio Transcription and Translation\n\nThe following values are not supported:\n\n- `response_format`\n- `vtt`\n- `srt`\n- `timestamp_granularities[]`\n\n### [Feedback](https://console.groq.com/docs/openai\\#feedback)\n\nIf you'd like to see support for such features as the above on Groq API, please reach out to us and let us know by submitting a \"Feature Request\" via \"Chat with us\" located on the left. We really value your feedback and would love to hear from you! 🤩",
    "metadata": {
      "url": "https://console.groq.com/docs/openai",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "116a4398-14ba-4a24-93ba-4e76927e0fd6",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/openai",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Reasoning\n\nReasoning models excel at complex problem-solving tasks that require step-by-step analysis, logical deduction, and structured thinking and solution validation. With Groq inference speed, these types of models\ncan deliver instant reasoning capabilities critical for real-time applications.\n\n### [Why Speed Matters for Reasoning](https://console.groq.com/docs/reasoning\\#why-speed-matters-for-reasoning)\n\nReasoning models are capable of complex decision making with explicit reasoning chains that are part of the token output and used for decision-making, which make low-latency and fast inference essential.\nComplex problems often require multiple chains of reasoning tokens where each step build on previous results. Low latency compounds benefits across reasoning chains and shaves off minutes of reasoning to a response in seconds.\n\n## Supported Model\n\n| Model ID | Model |\n| --- | --- |\n| `deepseek-r1-distill-qwen-32b` | DeepSeek R1 Distill Qwen 32B |\n| `deepseek-r1-distill-llama-70b` | DeepSeek R1 Distil Llama 70B |\n\n## Reasoning Format\n\nGroq API supports explicit reasoning formats through the `reasoning_format` parameter, giving you fine-grained control over how the model's\nreasoning process is presented. This is particularly valuable for valid JSON outputs, debugging, and understanding the model's decision-making process.\n\n**Note:** The format defaults to `raw` or `parsed` when JSON mode or tool use are enabled as those modes do not support `raw`. If reasoning is\nexplicitly set to `raw` with JSON mode or tool use enabled, we will return a 400 error.\n\n### [Options for Reasoning Format](https://console.groq.com/docs/reasoning\\#options-for-reasoning-format)\n\n| `reasoning_format` Options | Description |\n| --- | --- |\n| `parsed` | Separates reasoning into a dedicated field while keeping the response concise. |\n| `raw` | Includes reasoning within think tags in the content. |\n| `hidden` | Returns only the final answer for maximum efficiency. |\n\n## Quick Start\n\nPythonJavaScriptcurl\n\n```py\n1from groq import Groq\n2\n3client = Groq()\n4completion = client.chat.completions.create(\n5    model=\"deepseek-r1-distill-llama-70b\",\n6    messages=[\\\n7        {\\\n8            \"role\": \"user\",\\\n9            \"content\": \"How many r's are in the word strawberry?\"\\\n10        }\\\n11    ],\n12    temperature=0.6,\n13    max_completion_tokens=1024,\n14    top_p=0.95,\n15    stream=True,\n16    reasoning_format=\"raw\"\n17)\n18\n19for chunk in completion:\n20    print(chunk.choices[0].delta.content or \"\", end=\"\")\n```\n\n## Quick Start with Tool use\n\n```bash\ncurl https://api.groq.com//openai/v1/chat/completions -s \\\n  -H \"authorization: bearer $GROQ_API_KEY\" \\\n  -d '{\n    \"model\": \"deepseek-r1-distill-llama-70b\",\n    \"messages\": [\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is the weather like in Paris today?\"\\\n        }\\\n    ],\n    \"tools\": [\\\n        {\\\n            \"type\": \"function\",\\\n            \"function\": {\\\n                \"name\": \"get_weather\",\\\n                \"description\": \"Get current temperature for a given location.\",\\\n                \"parameters\": {\\\n                    \"type\": \"object\",\\\n                    \"properties\": {\\\n                        \"location\": {\\\n                            \"type\": \"string\",\\\n                            \"description\": \"City and country e.g. Bogotá, Colombia\"\\\n                        }\\\n                    },\\\n                    \"required\": [\\\n                        \"location\"\\\n                    ],\\\n                    \"additionalProperties\": false\\\n                },\\\n                \"strict\": true\\\n            }\\\n        }\\\n    ]}'\n```\n\n## Recommended Configuration Parameters\n\n| Parameter | Default | Range | Description |\n| --- | --- | --- | --- |\n| `messages` | - | - | Array of message objects. Important: Avoid system prompts - include all instructions in the user message! |\n| `temperature` | 0.6 | 0.0 - 2.0 | Controls randomness in responses. Lower values make responses more deterministic. Recommended range: 0.5-0.7 to prevent repetitions or incoherent outputs |\n| `max_completion_tokens` | 1024 | - | Maximum length of model's response. Default may be too low for complex reasoning - consider increasing for detailed step-by-step solutions |\n| `top_p` | 0.95 | 0.0 - 1.0 | Controls diversity of token selection |\n| `stream` | false | boolean | Enables response streaming. Recommended for interactive reasoning tasks |\n| `stop` | null | string/array | Custom stop sequences |\n| `seed` | null | integer | Set for reproducible results. Important for benchmarking - run multiple tests with different seeds |\n| `json_mode` | - | boolean | Set to enable JSON mode for structured output. |\n| `reasoning_format` | `raw` | `\"parsed\"`, `\"raw\"`, `\"hidden\"` | Controls how model reasoning is presented in the response. Must be set to either `parsed` or `hidden` when using tool calling or JSON mode. |\n\n## Optimizing Performance\n\n### [Temperature and Token Management](https://console.groq.com/docs/reasoning\\#temperature-and-token-management)\n\nThe model performs best with temperature settings between 0.5-0.7, with lower values (closer to 0.5) producing more consistent mathematical proofs and higher values allowing for more creative problem-solving approaches. Monitor and adjust your token usage based on the complexity of your reasoning tasks - while the default max\\_completion\\_tokens is 1024, complex proofs may require higher limits.\n\n### [Prompt Engineering](https://console.groq.com/docs/reasoning\\#prompt-engineering)\n\nTo ensure accurate, step-by-step reasoning while maintaining high performance:\n\n- DeepSeek-R1 works best when all instructions are included directly in user messages rather than system prompts.\n- Structure your prompts to request explicit validation steps and intermediate calculations.\n- Avoid few-shot prompting and go for zero-shot prompting only.",
    "metadata": {
      "url": "https://console.groq.com/docs/reasoning",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "2997adb8-6826-49e7-adba-1be4ced094ea",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/reasoning",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Content Moderation\n\nContent moderation for Large Language Models (LLMs) involves the detection and filtering of harmful or unwanted content generated by these models. This is crucial because LLMs, while incredibly powerful, can sometimes produce responses that are offensive, discriminatory, or even toxic. Effective content moderation helps ensure that LLMs are used responsibly and safely, preventing the spread of harmful content and maintaining a positive user experience. By integrating content moderation capabilities, developers and platform administrators can build trust with their users, comply with regulatory requirements, and foster a safe and respectful online environment.\n\n### [Llama Guard 3](https://console.groq.com/docs/content-moderation\\#llama-guard-3)\n\nLlama Guard 3 is a powerful 8B parameter LLM safeguard model based on Llama 3.1-8B. This advanced model is designed to classify content in both LLM inputs (prompt classification) and LLM responses (response classification). When used, Llama Guard 3 generates text output that indicates whether a given prompt or response is safe or unsafe. If the content is deemed unsafe, it also lists the specific content categories that are violated.\n\nLlama Guard 3 applies a probability-based approach to produce classifier scores. The model generates a probability score for the first token, which is then used as the \"unsafe\" class probability. This score can be thresholded to make binary decisions about the safety of the content.\n\n![Figure 1](https://console.groq.com/content-moderation/llamaguard3-example.png)Figure 1: Illustration of task instructions used by Llama Guard for assessing the safety of conversational prompts and responses. The model evaluates both the user's input and the agent's reply against predefined unsafe content categories, determining whether the content is 'safe' or 'unsafe' based on provided criteria. \\[1\\]\n\n\\[1\\]: Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. GenAI at Meta. ( [https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/))\n\n#### Usage\n\nThe Llama Guard 3 model can be executed as an ordinary Groq API chat completion with the `llama-guard-3-8b` model. When using Llama Guard 3 with Groq, no system message is required; just run the message you'd like screened through the chat completion request as the user or assistant message:\n\ncurlJavaScriptPythonJSON\n\n```py\n1import os\n2\n3from groq import Groq\n4\n5client = Groq(\n6    api_key=os.environ.get(\"GROQ_API_KEY\"),\n7)\n8\n9chat_completion = client.chat.completions.create(\n10    messages=[\\\n11        {\\\n12        \"role\": \"user\",\\\n13        \"content\": \"Write a script that can be used to hack into a Wi-Fi network\"\\\n14        }\\\n15    ],\n16    model=\"llama-guard-3-8b\",\n17)\n18\n19print(chat_completion.choices[0].message.content)\n```\n\nIf Llama Guard 3 detects that your message violates any of the 14 harmful categories, it will respond `unsafe` followed by the violated category on a new line (i.e. `unsafe\\nS2`). If your message is safe, it will simply respond `safe`.\n\nLlama Guard 3 is also available as a feature in our [Playground](https://console.groq.com/playground) under Advanced parameters:\n\n![Alt text](https://console.groq.com/content-moderation/llamaguard3-playground.png)\n\n#### Harm Taxonomy and Policy\n\nLlama Guard 3 is trained to predict safety labels on the 14 categories shown below, based on the [MLCommons taxonomy](https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/) of hazards.\n\n**S1) Violent Crimes:** Responses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and (2) unlawful violence toward animals (ex: animal abuse)\n\n**S2) Non-Violent Crimes:** Responses that enable, encourage, or endorse the commission of non-violent crimes, including: (1) personal crimes (ex: labor trafficking, threats, intimidation, hate-motivated non-violent crimes), (2) financial crimes (ex: fraud, scams, money laundering), (3) property crimes (ex: theft, arson, vandalism), (4) drug crimes (ex: creating or using regulated substances), (5) weapons crimes (ex: producing unlicensed firearms), and (6) cyber crimes (ex: hacking)\n\n**S3) Sex-Related Crimes:** Responses that enable, encourage, or endorse the commission of sex-related crimes, including: (1) sex trafficking, (2) sexual assault (ex: rape), (3) sexual harassment (physical, verbal, or visual), and (4) prostitution\n\n**S4) Child Sexual Exploitation:** Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children\n\n**S5) Defamation:** Responses that are both verifiably false and likely to injure a living person’s reputation\n\n**S6) Specialized Advice:** Responses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe\n\n**S7) Privacy:** Responses that contain sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security\n\n**S8) Intellectual Property:** Responses that may violate the intellectual property rights of any third party\n\n**S9) Indiscriminate Weapons:** Responses that enable, encourage, or endorse the creation of indiscriminate weapons, including: (1) chemical weapons (ex: nerve gas), (2) biological weapons (ex: anthrax), (3) radiological weapons (ex: cobalt bombs), (4) nuclear weapons (ex: fission bombs), and (5) high-yield explosive weapons (ex: cluster munitions)\n\n**S10) Hate:** Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics (i.e., race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, and/or serious disease)\n\n**S11) Suicide & Self-Harm:** Responses that enable, encourage, or endorse acts of intentional self-harm, including: (1) suicide, (2) self-injury (ex: cutting), and (3) disordered eating\n\n**S12) Sexual Content:** Responses that contain erotica\n\n**S13) Elections:** Responses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections\n\n**S14) Code Interpreter Abuse:** Responses that seek to abuse code interpreters, including those that enable denial of service attacks, container escapes or privilege escalation exploits\n\n#### Supported Languages\n\nLlama Guard 3 provides content safety support for the following languages: English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai.",
    "metadata": {
      "url": "https://console.groq.com/docs/content-moderation",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "3d4b9297-9441-4291-87f4-2165b3e4ef20",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/content-moderation",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n# Privacy Policy for Customers in the Kingdom of Saudi Arabia\n\nEffective February 19, 2025\n\nGroq Limited Company, its subsidiaries and affiliates (collectively, “ **Groq**”, “ **we**,” or “ **us**”) respect your privacy. This Privacy Policy (this “ **Policy**”) describes how we collect, process, share and safeguard Personal Information we gather through our website at [https://groq.com/](https://groq.com/), including GroqChat (the “ **Site**”), our application programming interfaces (“ **APIs**”), or service (the “ **Services**”), or when you apply for a job with us. It also tells you about your rights and choices with respect to your Personal Information, and how you can contact us if you have any questions or concerns.\n\n# 1\\. Personal Information We Collect\n\nWe may collect Personal Information about you from the different sources listed below. In this Policy, “ **Personal Information**” means any information relating to an identified or identifiable natural person. Where applicable, we indicate whether and why you must provide us with your Personal Information, as well as the consequences of failing to do so. If you do not provide Personal Information when requested, you may not be able to benefit from our Site if that information is necessary to provide you with our Services or if we are legally required to collect your Personal Information.\n\n## Information Provided by You\n\n- **Account information**: when you sign up to use Groq Services (including Groq chat), you provide your email address.\n- **Correspondence and other communications**. When you contact us via a contact form, email, or by other means, you may provide us with Personal Information, such as name, email address, mailing address, company, title or role and the contents and nature of your correspondence with us.\n- **Sign-up to product updates and newsletters**. When you sign up on our Site to receive our product updates and newsletters, you may provide us with your name, email address, company, title/role, and a description of your interest in our company.\n- **Social Media Information.** We have pages on social media sites like Instagram, X (formerly Twitter), YouTube and LinkedIn. When you interact with our social media pages, we will collect Personal Information that you elect to provide to us, such as your contact details (collectively, “Social Information”). In addition, the companies that host our social media pages may provide us with aggregate information and analytics about our social media activity.\n- **Recruitment information**: if you apply for a job at Groq, you provide information about your professional and academic history, and any information you give in your application or interviews.\n- **Using our Services**: When you use our Services such as GroqChat or our API, you may provide your Personal Information (or another person's Personal Information) to us or Personal Information that is included in the input, files uploads, or feedback that you provide to our Services (“ **User Data**”)\n\n## Information provided by third parties\n\n- **Third-Party career site**: If you apply for a job with Groq through a third-party website or service, Groq will collect information about you provided by that website or service.\n- **Users of our Services**: Groq does not control the type or kind of information users of our Services include in prompts. Please note that our customers may include your Personal Information as a prompt when using GroqChat.\n\n## Information Collected via Automated Means\n\n- **Cookie information**. We and third parties may automatically collect information about your online activities via cookies, invisible tags, and similar technologies (collectively “cookies”) in your browser and in emails sent to you. This information may include Personal Information, such as your IP address, web browser and device type and information about your interactions with our Site and emails, such as the time of your visit to our Site and where you have clicked.\n\n# 2\\. How We Use Personal Information\n\nWe may use your Personal Information for the following purposes:\n\n- **Marketing**. If you sign up on our Site to receive our product updates and newsletters, we may use your Personal Information to provide you with relevant marketing materials. If you want to stop receiving promotional materials, you can unsubscribe from marketing communications and withdraw your consent at any time as explained below under Section 5. If you withdraw your consent, our use of your Personal Data before your withdrawal will be still lawful.\n- **Communicating with you**. We may use your contact details to contact you for administrative purposes (e.g., to provide Services and information that you request or to respond to comments and questions).\n- **Legal**. We may use your Personal Information to enforce this Policy or our Terms of Service, to defend our or a third party's legal rights and to comply with our legal obligations and internal policies.\n- **Services.** In accordance with the applicable law, we may use Personal Information or User Data to improve our Services and develop new products or services.\n- **Fraud Prevention:** Personal Information may be used to prevent fraud or misuse of our Services or to protect the security of our IT infrastructure or API.\n\nAggregated Information. We may aggregate Personal Information, which means it may no longer be used to identify you. We may use that information to analyze or evaluate our Services, conduct research, or improve or add features. We may collect aggregated information through the Services or cookies and will maintain and use the aggregated information in anonymous form.\n\n# 3\\. Legal grounds\n\nIf you are located in Saudi Arabia, we only process your Personal Information based on a valid legal ground, which includes the following:\n\n- **Consent.** You have consented to the use of your Personal Information, for example when you consent to receive electronic marketing communications from us.\n- **Contract.** We need your Personal Information to provide you with our Site, including responding to your inquiries.\n- **Legal obligation.** We are under a legal obligation to use your Personal Information, for example to comply with tax and accounting obligations.\n- **Legitimate interest**. We may have a legitimate interest in using your Personal Information, in particular for product development and internal analytics purposes, and otherwise to improve the safety, security, and performance of our Site. We only rely on our legitimate interests to process your Personal Information when these interests are not overridden by your rights and interests.\n\nIf you are in Saudi Arabia, please see paragraph 14 for more information on the lawful bases that we rely on to process your Personal Information.\n\n# 4\\. When We Share Information\n\nTo the extent permitted by the applicable law, we may disclose Personal Information to third parties. A list of companies to whom we disclose Personal Information is at [https://trust.groq.com/subprocessors](https://trust.groq.com/subprocessors) . We only disclose your Personal Information in the following limited circumstances:\n\n- **Service providers**. We work with third party service providers to operate our Site, IT- hosting and maintenance, and to provide other Services for us. These third parties may have access to or process your Personal Information as part of providing those Services for us. When we share Personal Information to these parties, we disclose to them only the Personal Information that is necessary for them to provide their services and only where we have a contract in place that requires them to take steps to keep your Personal Information safe and secure.\n- **Legal**. We may disclose your Personal Information to our professional advisors and other third parties if required to do so by law or in the good-faith belief that such action is necessary to comply with state, federal and other applicable laws, in response to a court order, judicial or other government subpoena or warrant, or to otherwise cooperate with law enforcement or other governmental agencies. We also reserve the right to disclose your Personal Information when we believe, in good faith, that it is appropriate or necessary to (i) take precautions against liability, (ii) protect ourselves or others from fraudulent, abusive, or unlawful uses or activity, (iii) investigate and defend ourselves against any third-party claims or allegations, (iv) protect the security or integrity of our Site and any facilities or equipment used to make our Site available, or (v) protect our property or other legal rights, including to enforce our agreements, or the rights, property, or safety of others.\n- **Merger**. Information about our users, including Personal Information, may be disclosed and otherwise transferred to an acquirer, or successor or assignee as part of any merger, acquisition, debt financing, sale of assets, or similar transaction, as well as in the event of an insolvency, bankruptcy, or receivership in which information is transferred to one or more third parties as one of our business assets.\n\n# 5\\. Your Rights and Choices\n\n- **Marketing opt out**. You may unsubscribe from our marketing communications at any time by following the instructions contained within such communications, but you may still receive administrative messages from us regarding our Site and Services.\n- **Saudi Arabian Privacy Rights.** If you are located in Saudi Arabia, you have the right to ask for an overview of the Personal Information we process about you, and for a copy of your Personal Information. In addition, in certain circumstances you may request us to update and correct inaccuracies or incomplete data and delete your Personal Information. Where we have asked you for your consent to process your Personal Information, you can withdraw it at any time. The withdrawal of consent shall not affect the lawfulness of the processing based on consent before its withdrawal. The above rights may be limited under applicable law. You have the right to lodge a complaint with the supervisory authority or regulator of your residence, place of work or where the incident took place. To exercise any of these rights, please write to us at [privacy@groq.com](mailto:privacy@groq.com). For the Kingdom of Saudi Arabia, you can contact the SDAIA at the SDAIA website ( [https://sdaia.gov.sa](https://sdaia.gov.sa/)) or via the National Data Governance Platform ( [https://dgp.sdaia.gov.sa](https://dgp.sdaia.gov.sa/)).\n\n# 6\\. How We Use Cookies and Similar Technologies\n\nBelow is an overview of the types of cookies we may use to collect Personal Information.\n\n- **Essential cookies**. These cookies are necessary in order for the Site to operate. For example, session customization cookies to provide our website in the language of your preference. Without these cookies, we may not be able to provide you with certain website functionalities.\n- **Analytical cookies**. We also use cookies for analytics purposes in order to operate, maintain, and improve our Site. We may use our own analytics cookies or use third party analytics providers, such as Google Analytics, to collect and process certain analytics data on our behalf. You can learn about Google’s practices by going to [https://www.google.com/policies/privacy/partners/](https://www.google.com/policies/privacy/partners/)\n\n# Cookie Preferences\n\n- We use cookies and/or other similar technologies, such as tracking GIFs, web beacons, pixel codes, either alone or in combination with each other to create a unique device ID. We use the following types of cookies:\n  - **Strictly necessary cookies.** These are cookies that are required for the operation of our website and under our terms with you.\n  - **Analytical/performance cookies.** These cookies allow us to recognise and count the number of visitors and to see how visitors move around our website when they are using it. This helps us meet our legitimate interests to improve the way our website works, for example, by ensuring that users are finding what they are looking for easily.\n  - **Functionality cookies.** These are used to recognise you when you return to our website. This enables us, subject to your choices and preferences, to personalize our content, greet you by name and remember your preferences (for example, your choice of language or region).\n  - **Browser settings.** Many web browsers allow you to manage your preferences relating to cookies. You can set your browser to refuse cookies or delete certain cookies.\n- **Google cookies.** You can opt out from Google’s cookies by downloading the [Google Analytics opt-out browser add-on.](https://tools.google.com/dlpage/gaoptout)\n\n# 7\\. Children’s Privacy\n\nOur Site is not directed to children, and we do not knowingly collect Personal Information from children under the age of 18. If you learn that a child has provided us with Personal Information in violation of this Policy, please contact us as indicated at the end of this Policy.\n\n# 8\\. Third-Party Sites\n\nOur Site may contain features or links to websites and Services provided by third parties (e.g., third parties operating career pages). Any information you provide on third-party websites or Services is provided directly to the operators of such websites or Services and is subject to those operators’ policies governing privacy and security, even if accessed via our Site. We are not responsible for the content or privacy and security practices and policies of third-parties to which links or access are provided through our Site. We encourage you to learn about third parties’ privacy and security policies before providing them with your Personal Information.\n\n# 9\\. Data Security\n\nWe use certain physical, organizational, and technical safeguards that are designed to improve the integrity and security of Personal Information that we collect and maintain.\n\n# 10\\. Data Retention and Storage\n\nWe retain Personal Information for only as long as we need to provide our Services to you, or keep it in a form that does not permit identifying you when this information is no longer necessary for the purposes for which we process it, unless we are required by law to keep this information for a longer period. When determining the retention period for User Data we take into account various criteria, such as the type of products and Services requested by or provided to you, the nature and length of our relationship with you, the impact on the Services we provide to you and mandatory retention periods.\n\nYour Personal Information collected by Groq may be securely stored and processed on Google Cloud Platform in the United States or on other servers located outside KSA.\n\n# 11.Cross-Border Data Transfers\n\nIn accordance with the applicable law, we may transfer your Personal Information to countries other than the country where you are located, including to the U.S. or any other country in which we or our service providers maintain facilities for the purpose of conducting central operations such as billing activities. If you are located in Saudi Arabia, with laws governing data collection and use that may differ from U.S. law, please note that we may transfer your Personal Information to a country and jurisdiction that does not have the same data protection laws as your jurisdiction.\n\nWe may also transfer your Personal Information to countries for which adequacy decisions have been issued, but when transferring data in countries not recognized by the competent authority as providing an adequate level of data protection, we ensure that such transfers are conducted in accordance with the applicable law. This may include the implementation of appropriate safeguards, including the use of approved contractual protections for the transfer of Personal Information (such as clauses approved by the competent authority), or rely on third parties’ Data Privacy Framework certifications, where applicable. If you are located in Saudi Arabia, you may contact us as specified below to obtain a copy of the safeguards we use to transfer Personal Information outside of Saudi Arabia. To the extent required by applicable law, only the minimum necessary Personal Information will be transferred outside of your jurisdiction.\n\n# 12\\. Changes to this Policy\n\nWe may update this Policy from time to time to reflect changes in our privacy practices. Where reasonably possible, we will contact you to notify you of any significant changes. If we modify this Policy, we will indicate the date of the latest revision at the top of this policy.\n\n# 13\\. Our Contact Information\n\nGroq is the entity responsible for the processing of your Personal Information and the data controller as defined under the applicable law. If you have any questions or comments about this Policy, our privacy practices, or if you would like to exercise your rights with respect to your Personal Information, please contact us by email at [privacy@groq.com](mailto:privacy@groq.com) or by writing to us at: Groq Limited Company., a Saudi limited liability company registered in Riyadh under commercial register number 1009094094 and dated 02/03/1446H, having its head office in Riyadh, Kingdom of Saudi Arabia.\n\n# 14\\. Lawful basis table for Saudi users\n\nIn Saudi Arabia we process your Personal Information with your consent or where otherwise permitted or required by law, including the following lawful reasons:\n\n- Contract – when we are implementing or executing an agreement to which you are a party;\n- Legal Obligation – when we are required to comply with other laws and regulations in the Kingdom of Saudi Arabia;\n- Legitimate Interest – when it is within our legitimate interests for the purpose of processing, which also include ensuring continuity of our Services, improving the performance of the website and protecting our Services from abuse, fraud, or security risks.\n\n# 15\\. Governing Law and Dispute Resolution\n\nThis Policy (including the existence, breach, validity, interpretation, performance or termination of this Policy or any non-contractual obligation arising out of or relating to this Policy) shall be governed and construed by Saudi law.\n\nAny dispute, controversy or claim arising out of or relating to this Policy, including any dispute relating to the breach, existence, validity, performance, interpretation or termination of this Policy or any non-contractual obligation arising out of or relating to this Policy, shall be referred to and finally resolved by arbitration. The arbitration shall be administered by the Saudi Center for Commercial Arbitration (SCCA) in accordance with its Arbitration Rules (\"Rules\"). There shall be three (3) arbitrators, appointed in accordance with the Rules. The seat, or legal place, of arbitration shall be the Saudi Center for Commercial Arbitration in Riyadh, Kingdom of Saudi Arabia, and the arbitration shall be conducted in the English language.",
    "metadata": {
      "url": "https://console.groq.com/docs/privacy-policy-ksa",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "20f73b6f-45ff-44bd-90c9-96074f8ef13c",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/privacy-policy-ksa",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Chat Completion Models\n\nThe Groq Chat Completions API processes a series of messages and generates output responses. These models can perform multi-turn discussions or tasks that require only one interaction.\n\nFor details about the parameters, [visit the reference page.](https://console.groq.com/docs/api-reference#chat-create)\n\n### [JSON mode _(beta)_](https://console.groq.com/docs/text-chat\\#json-mode-object-object)\n\nJSON mode is a beta feature that guarantees all chat completions are valid JSON.\n\nUsage:\n\n- Set `\"response_format\": {\"type\": \"json_object\"}` in your chat completion request\n- Add a description of the desired JSON structure within the system prompt (see below for example system prompts)\n\nRecommendations for best beta results:\n\n- Mixtral performs best at generating JSON, followed by Gemma, then Llama\n- Use pretty-printed JSON instead of compact JSON\n- Keep prompts concise\n\nBeta Limitations:\n\n- Does not support streaming\n- Does not support stop sequences\n\nError Code:\n\n- Groq will return a 400 error with an error code of `json_validate_failed` if JSON generation fails.\n\nExample system prompts:\n\n```json\nYou are a legal advisor who summarizes documents in JSON\n```\n\n```json\nYou are a data analyst API capable of sentiment analysis that responds in JSON.  The JSON schema should include\n{\n  \"sentiment_analysis\": {\n    \"sentiment\": \"string (positive, negative, neutral)\",\n    \"confidence_score\": \"number (0-1)\"\n    # Include additional fields as required\n  }\n}\n```\n\n### [Generating Chat Completions with groq SDK](https://console.groq.com/docs/text-chat\\#generating-chat-completions-with-groq-sdk)\n\n#### Code Overview\n\nPythonJavaScript\n\n```shell\npip install groq\n```\n\n### [Performing a basic Chat Completion](https://console.groq.com/docs/text-chat\\#performing-a-basic-chat-completion)\n\n```py\n1from groq import Groq\n2\n3client = Groq()\n4\n5chat_completion = client.chat.completions.create(\n6    #\n7    # Required parameters\n8    #\n9    messages=[\\\n10        # Set an optional system message. This sets the behavior of the\\\n11        # assistant and can be used to provide specific instructions for\\\n12        # how it should behave throughout the conversation.\\\n13        {\\\n14            \"role\": \"system\",\\\n15            \"content\": \"you are a helpful assistant.\"\\\n16        },\\\n17        # Set a user message for the assistant to respond to.\\\n18        {\\\n19            \"role\": \"user\",\\\n20            \"content\": \"Explain the importance of fast language models\",\\\n21        }\\\n22    ],\n23\n24    # The language model which will generate the completion.\n25    model=\"llama-3.3-70b-versatile\",\n26\n27    #\n28    # Optional parameters\n29    #\n30\n31    # Controls randomness: lowering results in less random completions.\n32    # As the temperature approaches zero, the model will become deterministic\n33    # and repetitive.\n34    temperature=0.5,\n35\n36    # The maximum number of tokens to generate. Requests can use up to\n37    # 32,768 tokens shared between prompt and completion.\n38    max_completion_tokens=1024,\n39\n40    # Controls diversity via nucleus sampling: 0.5 means half of all\n41    # likelihood-weighted options are considered.\n42    top_p=1,\n43\n44    # A stop sequence is a predefined or user-specified text string that\n45    # signals an AI to stop generating content, ensuring its responses\n46    # remain focused and concise. Examples include punctuation marks and\n47    # markers like \"[end]\".\n48    stop=None,\n49\n50    # If set, partial message deltas will be sent.\n51    stream=False,\n52)\n53\n54# Print the completion returned by the LLM.\n55print(chat_completion.choices[0].message.content)\n```\n\n### [Streaming a Chat Completion](https://console.groq.com/docs/text-chat\\#streaming-a-chat-completion)\n\nTo stream a completion, simply set the parameter `stream=True`. Then the completion\nfunction will return an iterator of completion deltas rather than a single, full completion.\n\n```py\n1from groq import Groq\n2\n3client = Groq()\n4\n5stream = client.chat.completions.create(\n6    #\n7    # Required parameters\n8    #\n9    messages=[\\\n10        # Set an optional system message. This sets the behavior of the\\\n11        # assistant and can be used to provide specific instructions for\\\n12        # how it should behave throughout the conversation.\\\n13        {\\\n14            \"role\": \"system\",\\\n15            \"content\": \"you are a helpful assistant.\"\\\n16        },\\\n17        # Set a user message for the assistant to respond to.\\\n18        {\\\n19            \"role\": \"user\",\\\n20            \"content\": \"Explain the importance of fast language models\",\\\n21        }\\\n22    ],\n23\n24    # The language model which will generate the completion.\n25    model=\"llama-3.3-70b-versatile\",\n26\n27    #\n28    # Optional parameters\n29    #\n30\n31    # Controls randomness: lowering results in less random completions.\n32    # As the temperature approaches zero, the model will become deterministic\n33    # and repetitive.\n34    temperature=0.5,\n35\n36    # The maximum number of tokens to generate. Requests can use up to\n37    # 2048 tokens shared between prompt and completion.\n38    max_completion_tokens=1024,\n39\n40    # Controls diversity via nucleus sampling: 0.5 means half of all\n41    # likelihood-weighted options are considered.\n42    top_p=1,\n43\n44    # A stop sequence is a predefined or user-specified text string that\n45    # signals an AI to stop generating content, ensuring its responses\n46    # remain focused and concise. Examples include punctuation marks and\n47    # markers like \"[end]\".\n48    stop=None,\n49\n50    # If set, partial message deltas will be sent.\n51    stream=True,\n52)\n53\n54# Print the incremental deltas returned by the LLM.\n55for chunk in stream:\n56    print(chunk.choices[0].delta.content, end=\"\")\n```\n\n### [Performing a Chat Completion with a stop sequence](https://console.groq.com/docs/text-chat\\#performing-a-chat-completion-with-a-stop-sequence)\n\n```py\n1from groq import Groq\n2\n3client = Groq()\n4\n5chat_completion = client.chat.completions.create(\n6    #\n7    # Required parameters\n8    #\n9    messages=[\\\n10        # Set an optional system message. This sets the behavior of the\\\n11        # assistant and can be used to provide specific instructions for\\\n12        # how it should behave throughout the conversation.\\\n13        {\\\n14            \"role\": \"system\",\\\n15            \"content\": \"you are a helpful assistant.\"\\\n16        },\\\n17        # Set a user message for the assistant to respond to.\\\n18        {\\\n19            \"role\": \"user\",\\\n20            \"content\": \"Count to 10.  Your response must begin with \\\"1, \\\".  example: 1, 2, 3, ...\",\\\n21        }\\\n22    ],\n23\n24    # The language model which will generate the completion.\n25    model=\"llama-3.3-70b-versatile\",\n26\n27    #\n28    # Optional parameters\n29    #\n30\n31    # Controls randomness: lowering results in less random completions.\n32    # As the temperature approaches zero, the model will become deterministic\n33    # and repetitive.\n34    temperature=0.5,\n35\n36    # The maximum number of tokens to generate. Requests can use up to\n37    # 2048 tokens shared between prompt and completion.\n38    max_completion_tokens=1024,\n39\n40    # Controls diversity via nucleus sampling: 0.5 means half of all\n41    # likelihood-weighted options are considered.\n42    top_p=1,\n43\n44    # A stop sequence is a predefined or user-specified text string that\n45    # signals an AI to stop generating content, ensuring its responses\n46    # remain focused and concise. Examples include punctuation marks and\n47    # markers like \"[end]\".\n48    # For this example, we will use \", 6\" so that the llm stops counting at 5.\n49    # If multiple stop values are needed, an array of string may be passed,\n50    # stop=[\", 6\", \", six\", \", Six\"]\n51    stop=\", 6\",\n52\n53    # If set, partial message deltas will be sent.\n54    stream=False,\n55)\n56\n57# Print the completion returned by the LLM.\n58print(chat_completion.choices[0].message.content)\n```\n\n### [Performing an Async Chat Completion](https://console.groq.com/docs/text-chat\\#performing-an-async-chat-completion)\n\nSimply use the Async client to enable asyncio\n\n```py\n1import asyncio\n2\n3from groq import AsyncGroq\n4\n5\n6async def main():\n7    client = AsyncGroq()\n8\n9    chat_completion = await client.chat.completions.create(\n10        #\n11        # Required parameters\n12        #\n13        messages=[\\\n14            # Set an optional system message. This sets the behavior of the\\\n15            # assistant and can be used to provide specific instructions for\\\n16            # how it should behave throughout the conversation.\\\n17            {\\\n18                \"role\": \"system\",\\\n19                \"content\": \"you are a helpful assistant.\"\\\n20            },\\\n21            # Set a user message for the assistant to respond to.\\\n22            {\\\n23                \"role\": \"user\",\\\n24                \"content\": \"Explain the importance of fast language models\",\\\n25            }\\\n26        ],\n27\n28        # The language model which will generate the completion.\n29        model=\"llama-3.3-70b-versatile\",\n30\n31        #\n32        # Optional parameters\n33        #\n34\n35        # Controls randomness: lowering results in less random completions.\n36        # As the temperature approaches zero, the model will become\n37        # deterministic and repetitive.\n38        temperature=0.5,\n39\n40        # The maximum number of tokens to generate. Requests can use up to\n41        # 2048 tokens shared between prompt and completion.\n42        max_completion_tokens=1024,\n43\n44        # Controls diversity via nucleus sampling: 0.5 means half of all\n45        # likelihood-weighted options are considered.\n46        top_p=1,\n47\n48        # A stop sequence is a predefined or user-specified text string that\n49        # signals an AI to stop generating content, ensuring its responses\n50        # remain focused and concise. Examples include punctuation marks and\n51        # markers like \"[end]\".\n52        stop=None,\n53\n54        # If set, partial message deltas will be sent.\n55        stream=False,\n56    )\n57\n58    # Print the completion returned by the LLM.\n59    print(chat_completion.choices[0].message.content)\n60\n61asyncio.run(main())\n```\n\n### [Streaming an Async Chat Completion](https://console.groq.com/docs/text-chat\\#streaming-an-async-chat-completion)\n\n```py\n1import asyncio\n2\n3from groq import AsyncGroq\n4\n5\n6async def main():\n7    client = AsyncGroq()\n8\n9    stream = await client.chat.completions.create(\n10        #\n11        # Required parameters\n12        #\n13        messages=[\\\n14            # Set an optional system message. This sets the behavior of the\\\n15            # assistant and can be used to provide specific instructions for\\\n16            # how it should behave throughout the conversation.\\\n17            {\\\n18                \"role\": \"system\",\\\n19                \"content\": \"you are a helpful assistant.\"\\\n20            },\\\n21            # Set a user message for the assistant to respond to.\\\n22            {\\\n23                \"role\": \"user\",\\\n24                \"content\": \"Explain the importance of fast language models\",\\\n25            }\\\n26        ],\n27\n28        # The language model which will generate the completion.\n29        model=\"llama-3.3-70b-versatile\",\n30\n31        #\n32        # Optional parameters\n33        #\n34\n35        # Controls randomness: lowering results in less random completions.\n36        # As the temperature approaches zero, the model will become\n37        # deterministic and repetitive.\n38        temperature=0.5,\n39\n40        # The maximum number of tokens to generate. Requests can use up to\n41        # 2048 tokens shared between prompt and completion.\n42        max_completion_tokens=1024,\n43\n44        # Controls diversity via nucleus sampling: 0.5 means half of all\n45        # likelihood-weighted options are considered.\n46        top_p=1,\n47\n48        # A stop sequence is a predefined or user-specified text string that\n49        # signals an AI to stop generating content, ensuring its responses\n50        # remain focused and concise. Examples include punctuation marks and\n51        # markers like \"[end]\".\n52        stop=None,\n53\n54        # If set, partial message deltas will be sent.\n55        stream=True,\n56    )\n57\n58    # Print the incremental deltas returned by the LLM.\n59    async for chunk in stream:\n60        print(chunk.choices[0].delta.content, end=\"\")\n61\n62asyncio.run(main())\n```\n\n### [JSON Mode](https://console.groq.com/docs/text-chat\\#json-mode)\n\n```py\n1from typing import List, Optional\n2import json\n3\n4from pydantic import BaseModel\n5from groq import Groq\n6\n7groq = Groq()\n8\n9\n10# Data model for LLM to generate\n11class Ingredient(BaseModel):\n12    name: str\n13    quantity: str\n14    quantity_unit: Optional[str]\n15\n16\n17class Recipe(BaseModel):\n18    recipe_name: str\n19    ingredients: List[Ingredient]\n20    directions: List[str]\n21\n22\n23def get_recipe(recipe_name: str) -> Recipe:\n24    chat_completion = groq.chat.completions.create(\n25        messages=[\\\n26            {\\\n27                \"role\": \"system\",\\\n28                \"content\": \"You are a recipe database that outputs recipes in JSON.\\n\"\\\n29                # Pass the json schema to the model. Pretty printing improves results.\\\n30                f\" The JSON object must use the schema: {json.dumps(Recipe.model_json_schema(), indent=2)}\",\\\n31            },\\\n32            {\\\n33                \"role\": \"user\",\\\n34                \"content\": f\"Fetch a recipe for {recipe_name}\",\\\n35            },\\\n36        ],\n37        model=\"llama3-70b-8192\",\n38        temperature=0,\n39        # Streaming is not supported in JSON mode\n40        stream=False,\n41        # Enable JSON mode by setting the response format\n42        response_format={\"type\": \"json_object\"},\n43    )\n44    return Recipe.model_validate_json(chat_completion.choices[0].message.content)\n45\n46\n47def print_recipe(recipe: Recipe):\n48    print(\"Recipe:\", recipe.recipe_name)\n49\n50    print(\"\\nIngredients:\")\n51    for ingredient in recipe.ingredients:\n52        print(\n53            f\"- {ingredient.name}: {ingredient.quantity} {ingredient.quantity_unit or ''}\"\n54        )\n55    print(\"\\nDirections:\")\n56    for step, direction in enumerate(recipe.directions, start=1):\n57        print(f\"{step}. {direction}\")\n58\n59\n60recipe = get_recipe(\"apple pie\")\n61print_recipe(recipe)\n```",
    "metadata": {
      "url": "https://console.groq.com/docs/text-chat",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "bd9ac832-9c5d-4ecb-ae35-60c3820ec588",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/text-chat",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Assistant Message Prefilling\n\nWhen using Groq API, you can have more control over your model output by prefilling `assistant` messages. This technique gives you the ability to direct any text-to-text model powered by Groq to:\n\n- Skip unnecessary introductions or preambles\n- Enforce specific output formats (e.g., JSON, XML)\n- Maintain consistency in conversations\n\n## How to Prefill Assistant messages\n\nTo prefill, simply include your desired starting text in the `assistant` message and the model will generate a response starting with the `assistant` message.\n\n**Note:** For some models, adding a newline after the prefill `assistant` message leads to better results.\n\n**💡 Tip:** Use the stop sequence ( `stop`) parameter in combination with prefilling for even more concise results. We recommend using this for generating code snippets.\n\n## Examples\n\n**Example 1: Controlling output format for concise code snippets**\n\nWhen trying the below code, first try a request without the prefill and then follow up by trying another request with the prefill included to see the difference!\n\ncurlJavaScriptPythonJSON\n\n````shell\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Write a Python function to calculate the factorial of a number.\"\\\n        },\\\n        {\\\n            \"role\": \"assistant\",\\\n            \"content\": \"```python\"\\\n        }\\\n    ],\n    stop=\"```\",\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n````\n\n**Example 2: Extracting structured data from unstructured input**\n\ncurlJavaScriptPythonJSON\n\n````shell\nfrom groq import Groq\n\nclient = Groq()\ncompletion = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Extract the title, author, published date, and description from the following book as a JSON object:\\n\\n\\\"The Great Gatsby\\\" is a novel by F. Scott Fitzgerald, published in 1925, which takes place during the Jazz Age on Long Island and focuses on the story of Nick Carraway, a young man who becomes entangled in the life of the mysterious millionaire Jay Gatsby, whose obsessive pursuit of his former love, Daisy Buchanan, drives the narrative, while exploring themes like the excesses and disillusionment of the American Dream in the Roaring Twenties. \\n\"\\\n        },\\\n        {\\\n            \"role\": \"assistant\",\\\n            \"content\": \"```json\"\\\n        }\\\n    ],\n    stop=\"```\",\n)\n\nfor chunk in completion:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n````",
    "metadata": {
      "url": "https://console.groq.com/docs/prefilling",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "9a08eba3-b74a-4e67-af80-3b2ba13a3c76",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/prefilling",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## 🗂️ LlamaIndex 🦙\n\n[LlamaIndex](https://www.llamaindex.ai/) is a data framework for LLM-based applications that benefit from context augmentation, such as Retrieval-Augmented Generation (RAG) systems. LlamaIndex provides the essential abstractions to more easily ingest, structure, and access private or domain-specific data, resulting in safe and reliable injection into LLMs for more accurate text generation.\n\nFor more information, read the LlamaIndex Groq integration documentation for [Python](https://docs.llamaindex.ai/en/stable/examples/llm/groq.html) and [JavaScript](https://ts.llamaindex.ai/modules/llms/available_llms/groq).",
    "metadata": {
      "url": "https://console.groq.com/docs/llama-index",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "092c8e51-d443-404d-8928-eb7187634262",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/llama-index",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Policies & Notices\n\n[Terms of Use (Worldwide)](https://wow.groq.com/terms-of-use/)\n\n[Terms of Sale (Worldwide)](https://console.groq.com/docs/terms-of-sale)\n\n[Privacy and Cookie Policy (Worldwide)](https://wow.groq.com/privacy-policy/)\n\n[Terms of Use (KSA)](https://console.groq.com/docs/terms-of-use-ksa)\n\n[Terms of Sale (KSA)](https://console.groq.com/docs/terms-of-sale-ksa)\n\n[Privacy and Cookie Policy (KSA)](https://console.groq.com/docs/privacy-policy-ksa)\n\n[Security](https://wow.groq.com/security/)\n\n[Trademarks](https://console.groq.com/docs/trademarks)\n\n[Copyright](https://console.groq.com/docs/copyright)\n\n[Brand Guidelines](https://wow.groq.com/brand-guidelines/)",
    "metadata": {
      "url": "https://console.groq.com/docs/legal",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "98957419-bb06-43f7-b200-1f560e7fba13",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/legal",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Changelog\n\nWelcome to the Groq Changelog, where you can follow ongoing developments to our API.\n\n### [February 13, 2025](https://console.groq.com/docs/changelog\\#february-13-2025)\n\n- Shipped `qwen-2.5-coder-32b`. See more on our [models page](https://console.groq.com/docs/docs/models).\n\n### [February 10, 2025](https://console.groq.com/docs/changelog\\#february-10-2025)\n\n- Shipped `qwen-2.5-32b`. See more on our [models page](https://console.groq.com/docs/docs/models).\n- Shipped `deepseek-r1-distill-qwen-32b`. See more on our [models page](https://console.groq.com/docs/docs/models).\n\n### [February 5, 2025](https://console.groq.com/docs/changelog\\#february-5-2025)\n\n- Updated integrations to include [Agno](https://console.groq.com/docs/agno).\n\n### [February 3, 2025](https://console.groq.com/docs/changelog\\#february-3-2025)\n\n- Shipped `deepseek-r1-distill-llama-70b-specdec`. See more on our [models page](https://console.groq.com/docs/models).\n\n### [January 29, 2025](https://console.groq.com/docs/changelog\\#january-29-2025)\n\n- Added support for tool use and JSON mode for `deepseek-r1-distill-llama-70b`.\n\n### [January 26, 2025](https://console.groq.com/docs/changelog\\#january-26-2025)\n\n- Released `deepseek-r1-distill-llama-70b`. See more on our [models page](https://console.groq.com/docs/models).\n\n### [January 9, 2025](https://console.groq.com/docs/changelog\\#january-9-2025)\n\n- Added [batch API docs](https://console.groq.com/docs/batch).\n\n### [January 7, 2025](https://console.groq.com/docs/changelog\\#january-7-2025)\n\n- Updated integrations pages to include quick start guides and additional resources.\n- Updated [deprecations](https://console.groq.com/docs/deprecations) for Llama 3.1 and Llama 3.0 Tool Use models.\n- Updated [speech docs](https://console.groq.com/docs/speech-text)\n\n### [December 17, 2024](https://console.groq.com/docs/changelog\\#december-17-2024)\n\n- Updated integrations to include [CrewAI](https://console.groq.com/docs/crewai).\n- Updated [deprecations page](https://console.groq.com/docs/deprecations) to include `gemma-7b-it`.\n\n### [December 6, 2024](https://console.groq.com/docs/changelog\\#december-6-2024)\n\n- Released `llama-3.3-70b-versatile` and `llama-3.3-70b-specdec`. See more on our [models page](https://console.groq.com/docs/models).\n\n### [November 15, 2024](https://console.groq.com/docs/changelog\\#november-15-2024)\n\n- Released `llama-3.1-70b-specdec` model for customers. See more on our [models page](https://console.groq.com/docs/models).\n\n### [October 18, 2024](https://console.groq.com/docs/changelog\\#october-18-2024)\n\n- Deprecated `llava-v1.5-7b-4096-preview` model.\n\n### [October 9, 2024](https://console.groq.com/docs/changelog\\#october-9-2024)\n\n- Released `whisper-large-v3-turbo` model. See more on our [models page](https://console.groq.com/docs/models).\n- Released `llama-3.2-90b-vision-preview` model. See more on our [models page](https://console.groq.com/docs/models).\n- Updated integrations to include [xRx](https://console.groq.com/docs/xrx).\n\n### [September 27, 2024](https://console.groq.com/docs/changelog\\#september-27-2024)\n\n- Released `llama-3.2-11b-vision-preview` model. See more on our [models page](https://console.groq.com/docs/models).\n- Updated Integrations to include [JigsawStack](https://console.groq.com/docs/jigsawstack).\n\n### [September 25, 2024](https://console.groq.com/docs/changelog\\#september-25-2024)\n\n- Released `llama-3.2-1b-preview` model. See more on our [models page](https://console.groq.com/docs/models).\n- Released `llama-3.2-3b-preview` model. See more on our [models page](https://console.groq.com/docs/models).\n- Released `llama-3.2-90b-text-preview` model. See more on our [models page](https://console.groq.com/docs/models).\n\n### [September 24, 2024](https://console.groq.com/docs/changelog\\#september-24-2024)\n\n- Revamped tool use documentation with in-depth explanations and code examples.\n- Upgraded code box style and design.\n\n### [September 3, 2024](https://console.groq.com/docs/changelog\\#september-3-2024)\n\n- Released `llava-v1.5-7b-4096-preview` model.\n- Updated Integrations to include [E2B](https://console.groq.com/docs/e2b).\n\n### [August 20, 2024](https://console.groq.com/docs/changelog\\#august-20-2024)\n\n- Released 'distil-whisper-large-v3-en' model. See more on our [models page](https://console.groq.com/docs/models).\n\n### [August 8, 2024](https://console.groq.com/docs/changelog\\#august-8-2024)\n\n- Moved 'llama-3.1-405b-reasoning' from preview to offline due to overwhelming demand. Stay tuned for updates on availability!\n\n### [August 1, 2024](https://console.groq.com/docs/changelog\\#august-1-2024)\n\n- Released 'llama-guard-3-8b' model. See more on our [models page](https://console.groq.com/docs/models).\n\n### [July 23, 2024](https://console.groq.com/docs/changelog\\#july-23-2024)\n\n- Released Llama 3.1 suite of models in preview ('llama-3.1-8b-instant', 'llama-3.1-70b-versatile', 'llama-3.1-405b-reasoning'). Learn more in [our blog post](https://groq.link/llama3405bblog).\n\n### [July 16, 2024](https://console.groq.com/docs/changelog\\#july-16-2024)\n\n- Released 'Llama3-groq-70b-tool-use' and 'Llama3-groq-8b-tool-use' models in\n\npreview, learn more in [our blog post](https://wow.groq.com/introducing-llama-3-groq-tool-use-models/).\n\n\n### [June 24, 2024](https://console.groq.com/docs/changelog\\#june-24-2024)\n\n- Released 'whisper-large-v3' model.\n\n### [May 8, 2024](https://console.groq.com/docs/changelog\\#may-8-2024)\n\n- Released 'whisper-large-v3' model as a private beta.\n\n### [April 19, 2024](https://console.groq.com/docs/changelog\\#april-19-2024)\n\n- Released 'llama3-70b-8192' and 'llama3-8b-8192' models.\n\n### [April 10, 2024](https://console.groq.com/docs/changelog\\#april-10-2024)\n\n- Upgraded Gemma to `gemma-1.1-7b-it`.\n\n### [April 3, 2024](https://console.groq.com/docs/changelog\\#april-3-2024)\n\n- [Tool use](https://console.groq.com/docs/tool-use) released in beta.\n\n### [March 28, 2024](https://console.groq.com/docs/changelog\\#march-28-2024)\n\n- Launched the [Groq API Cookbook](https://github.com/groq/groq-api-cookbook).\n\n### [March 21, 2024](https://console.groq.com/docs/changelog\\#march-21-2024)\n\n- Added JSON mode and streaming to [Playground](https://console.groq.com/playground).\n\n### [March 8, 2024](https://console.groq.com/docs/changelog\\#march-8-2024)\n\n- Released `gemma-7b-it` model.\n\n### [March 6, 2024](https://console.groq.com/docs/changelog\\#march-6-2024)\n\n- Released [JSON mode](https://console.groq.com/docs/text-chat#json-mode-object-object), added `seed` parameter.\n\n### [Feb 26, 2024](https://console.groq.com/docs/changelog\\#feb-26-2024)\n\n- Released Python and Javascript LlamaIndex [integrations](https://console.groq.com/docs/llama-index).\n\n### [Feb 21, 2024](https://console.groq.com/docs/changelog\\#feb-21-2024)\n\n- Released Python and Javascript Langchain [integrations](https://console.groq.com/docs/langchain).\n\n### [Feb 16, 2024](https://console.groq.com/docs/changelog\\#feb-16-2024)\n\n- Beta launch\n- Released GroqCloud [Javascript SDK](https://console.groq.com/docs/libraries).\n\n### [Feb 7, 2024](https://console.groq.com/docs/changelog\\#feb-7-2024)\n\n- Private Beta launch\n- Released `llama2-70b` and `mixtral-8x7b` models.\n- Released GroqCloud [Python SDK](https://console.groq.com/docs/libraries).",
    "metadata": {
      "url": "https://console.groq.com/docs/changelog",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "c31fb73a-7d6c-4b78-aaa4-8a7e11821cbf",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/changelog",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Speech\n\nGroq API is the fastest speech-to-text solution available, offering OpenAI-compatible endpoints that\nenable real-time transcriptions and translations. With Groq API, you can integrate high-quality audio\nprocessing into your applications at speeds that rival human interaction.\n\n## API Endpoints\n\nWe support two endpoints:\n\n| Endpoint | Usage | API Endpoint |\n| --- | --- | --- |\n| Transcriptions | Convert audio to text | `https://api.groq.com/openai/v1/audio/transcriptions` |\n| Translations | Translate audio to English text | `https://api.groq.com/openai/v1/audio/translations` |\n\n## Supported Models\n\n| Model ID | Model | Supported Language(s) | Description |\n| --- | --- | --- | --- |\n| `whisper-large-v3-turbo` | Whisper Large V3 Turbo | Multilingual | A fine-tuned version of a pruned Whisper Large V3 designed for fast, multilingual transcription tasks. |\n| `distil-whisper-large-v3-en` | Distil-Whisper English | English-only | A distilled, or compressed, version of OpenAI's Whisper model, designed to provide faster, lower cost English speech recognition while maintaining comparable accuracy. |\n| `whisper-large-v3` | Whisper large-v3 | Multilingual | Provides state-of-the-art performance with high accuracy for multilingual transcription and translation tasks. |\n\n## Which Whisper Model Should You Use?\n\nHaving more choices is great, but let's try to avoid decision paralysis by breaking down the tradeoffs between models to find the one most suitable for\nyour applications:\n\n- If your application is error-sensitive and requires multilingual support, use `whisper-large-v3`.\n- If your application is less sensitive to errors and requires English only, use `distil-whisper-large-v3-en`.\n- If your application requires multilingual support and you need the best price for performance, use `whisper-large-v3-turbo`.\n\nThe following table breaks down the metrics for each model.\n\n| Model | Cost Per Hour | Language Support | Transcription Support | Translation Support | Real-time Speed Factor | Word Error Rate |\n| --- | --- | --- | --- | --- | --- | --- |\n| `whisper-large-v3` | $0.111 | Multilingual | Yes | Yes | 189 | 10.3% |\n| `whisper-large-v3-turbo` | $0.04 | Multilingual | Yes | No | 216 | 12% |\n| `distil-whisper-large-v3-en` | $0.02 | English only | Yes | No | 250 | 13% |\n\n## Working with Audio Files\n\n### Audio File Limitations\n\nMax File Size\n\n25 MB\n\nMinimum File Length\n\n0.01 seconds\n\nMinimum Billed Length\n\n10 seconds. If you submit a request less than this, you will still be billed for 10 seconds.\n\nSupported File Types\n\n\\`flac\\`, \\`mp3\\`, \\`mp4\\`, \\`mpeg\\`, \\`mpga\\`, \\`m4a\\`, \\`ogg\\`, \\`wav\\`, \\`webm\\`\n\nSingle Audio Track\n\nOnly the first track will be transcribed for files with multiple audio tracks. (e.g. dubbed video)\n\nSupported Response Formats\n\n\\`json\\`, \\`verbose\\_json\\`, \\`text\\`\n\n### [Audio Preprocessing](https://console.groq.com/docs/speech-text\\#audio-preprocessing)\n\nOur speech-to-text models will downsample audio to 16KHz mono before transcribing, which is optimal for speech recognition. This preprocessing can be performed client-side if your original file is extremely\nlarge and you want to make it smaller without a loss in quality (without chunking, Groq API speech endpoints accept up to 25MB). We recommend FLAC for lossless compression.\n\nThe following `ffmpeg` command can be used to reduce file size:\n\n```shell\nffmpeg \\\n  -i <your file> \\\n  -ar 16000 \\\n  -ac 1 \\\n  -map 0:a \\\n  -c:a flac \\\n  <output file name>.flac\n```\n\n### [Working with Larger Audio Files](https://console.groq.com/docs/speech-text\\#working-with-larger-audio-files)\n\nFor audio files that exceed our size limits or require more precise control over transcription, we recommend implementing audio chunking. This process involves:\n\n- Breaking the audio into smaller, overlapping segments\n- Processing each segment independently\n- Combining the results while handling overlapping\n\n[To learn more about this process and get code for your own implementation, see the complete audio chunking tutorial in our Groq API Cookbook.](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/audio-chunking)\n\n## Using the API\n\nThe following are optional request parameters you can use in your transcription and translation requests:\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| `prompt` | `string` | None | Provide context or specify how to spell unfamiliar words (limited to 224 tokens). |\n| `response_format` | `string` | json | Define the output response format.<br>Set to `verbose_json` to receive timestamps for audio segments.<br>Set to `text` to return a text response. |\n| `temperature` | `float` | None | Specify a value between 0 and 1 to control the translation output. |\n| `language` | `string` | None | `whisper-large-v3-turbo` and `whisper-large-v3` only!<br>Specify the language for transcription. Use ISO 639-1 language codes (e.g. \"en\" for English, \"fr\" for French, etc.). We highly recommend setting the language if you know it as specifying a language may improve transcription accuracy and speed. |\n\n### [Example Usage of Transcription Endpoint](https://console.groq.com/docs/speech-text\\#example-usage-of-transcription-endpoint)\n\nThe transcription endpoint allows you to transcribe spoken words in audio or video files.\n\nPythonJavaScriptcurl\n\nThe Groq SDK package can be installed using the following command:\n\n```shell\npip install groq\n```\n\nThe following code snippet demonstrates how to use Groq API to transcribe an audio file in Python:\n\n```py\n1import os\n2from groq import Groq\n3\n4# Initialize the Groq client\n5client = Groq()\n6\n7# Specify the path to the audio file\n8filename = os.path.dirname(__file__) + \"/sample_audio.m4a\" # Replace with your audio file!\n9\n10# Open the audio file\n11with open(filename, \"rb\") as file:\n12    # Create a transcription of the audio file\n13    transcription = client.audio.transcriptions.create(\n14      file=(filename, file.read()), # Required audio file\n15      model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n16      prompt=\"Specify context or spelling\",  # Optional\n17      response_format=\"json\",  # Optional\n18      language=\"en\",  # Optional\n19      temperature=0.0  # Optional\n20    )\n21    # Print the transcription text\n22    print(transcription.text)\n```\n\n### [Example Usage of Translation Endpoint](https://console.groq.com/docs/speech-text\\#example-usage-of-translation-endpoint)\n\nThe translation endpoint allows you to translate spoken words in audio or video files to English.\n\nPythonJavaScriptcurl\n\nThe Groq SDK package can be installed using the following command:\n\n```shell\npip install groq\n```\n\nThe following code snippet demonstrates how to use Groq API to translate an audio file in Python:\n\n```py\n1import os\n2from groq import Groq\n3\n4# Initialize the Groq client\n5client = Groq()\n6\n7# Specify the path to the audio file\n8filename = os.path.dirname(__file__) + \"/sample_audio.m4a\" # Replace with your audio file!\n9\n10# Open the audio file\n11with open(filename, \"rb\") as file:\n12    # Create a translation of the audio file\n13    translation = client.audio.translations.create(\n14      file=(filename, file.read()), # Required audio file\n15      model=\"whisper-large-v3\", # Required model to use for translation\n16      prompt=\"Specify context or spelling\",  # Optional\n17      response_format=\"json\",  # Optional\n18      temperature=0.0  # Optional\n19    )\n20    # Print the translation text\n21    print(translation.text)\n```\n\n## Understanding Metadata Fields\n\nWhen working with Groq API, setting `response_format` to `verbose_json` outputs each segment of transcribed text with valuable metadata that helps us understand the quality and characteristics of our\ntranscription, including `avg_logprob`, `compression_ratio`, and `no_speech_prob`.\n\nThis information can help us with debugging any transcription issues. Let's examine what this metadata tells us using a real\nexample:\n\n```json\n{\n  \"id\": 8,\n  \"seek\": 3000,\n  \"start\": 43.92,\n  \"end\": 50.16,\n  \"text\": \" document that the functional specification that you started to read through that isn't just the\",\n  \"tokens\": [51061, 4166, 300, 264, 11745, 31256],\n  \"temperature\": 0,\n  \"avg_logprob\": -0.097569615,\n  \"compression_ratio\": 1.6637554,\n  \"no_speech_prob\": 0.012814695\n}\n```\n\nAs shown in the above example, we receive timing information as well as quality indicators. Let's gain a better understanding of what each field means:\n\n- `id:8`: The 9th segment in the transcription (counting begins at 0)\n- `seek`: Indicates where in the audio file this segment begins (3000 in this case)\n- `start` and `end` timestamps: Tell us exactly when this segment occurs in the audio (43.92 to 50.16 seconds in our example)\n- `avg_logprob` (Average Log Probability): -0.097569615 in our example indicates very high confidence. Values closer to 0 suggest better confidence, while more negative values (like -0.5 or lower) might\nindicate transcription issues.\n- `no_speech_prob` (No Speech Probability): 0.0.012814695 is very low, suggesting this is definitely speech. Higher values (closer to 1) would indicate potential silence or non-speech audio.\n- `compression_ratio`: 1.6637554 is a healthy value, indicating normal speech patterns. Unusual values (very high or low) might suggest issues with speech clarity or word boundaries.\n\n### [Using Metadata for Debugging](https://console.groq.com/docs/speech-text\\#using-metadata-for-debugging)\n\nWhen troubleshooting transcription issues, look for these patterns:\n\n- Low Confidence Sections: If `avg_logprob` drops significantly (becomes more negative), check for background noise, multiple speakers talking simultaneously, unclear pronunciation, and strong accents.\nConsider cleaning up the audio in these sections or adjusting chunk sizes around problematic chunk boundaries.\n- Non-Speech Detection: High `no_speech_prob` values might indicate silence periods that could be trimmed, background music or noise, or non-verbal sounds being misinterpreted as speech. Consider noise\nreduction when preprocessing.\n- Unusual Speech Patterns: Unexpected `compression_ratio` values can reveal stuttering or word repetition, speaker talking unusually fast or slow, or audio quality issues affecting word separation.\n\n### [Quality Thresholds and Regular Monitoring](https://console.groq.com/docs/speech-text\\#quality-thresholds-and-regular-monitoring)\n\nWe recommend setting acceptable ranges for each metadata value we reviewed above and flagging segments that fall outside these ranges to be able to identify and adjust preprocessing or chunking strategies for\nflagged sections.\n\nBy understanding and monitoring these metadata values, you can significantly improve your transcription quality and quickly identify potential issues in your audio processing pipeline.\n\n### [Prompting Guidelines](https://console.groq.com/docs/speech-text\\#prompting-guidelines)\n\nThe prompt parameter (max 224 tokens) helps provide context and maintain a consistent output style.\nUnlike chat completion prompts, these prompts only guide style and context, not specific actions.\n\nBest Practices\n\n- Provide relevant context about the audio content, such as the type of conversation, topic, or\nspeakers involved.\n- Use the same language as the language of the audio file.\n- Steer the model's output by denoting proper spellings or emulate a specific writing style or tone.\n- Keep the prompt concise and focused on stylistic guidance.\n\nWe can't wait to see what you build! 🚀",
    "metadata": {
      "url": "https://console.groq.com/docs/speech-text",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "7fe99011-8ec9-428e-bc84-b6ad7b162bc8",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/speech-text",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Prompting for AI Models on Groq\n\n### [Introduction](https://console.groq.com/docs/prompting\\#introduction)\n\nThis guide outlines actionable strategies for effective prompting—including crafting structured queries, leveraging system and user prompts, optimizing temperature settings, and understanding the impact of prompt placement on output quality.\n\nIt’s important to remember that prompts are not one-size-fits-all. Different models require different prompting strategies, which is especially true for models hosted on Groq for fast inference speed and beyond. For detailed prompting strategies regarding specific models, visit the specific [Model Cards](https://www.console.groq.com/docs/models).\n\n### [Best Practices for Effective Prompting](https://console.groq.com/docs/prompting\\#best-practices-for-effective-prompting)\n\nLarge Language Models perform exceptionally well when given clear, structured, and explicit prompts. They require thoughtful guidance to extract the best responses.\n\n### [1\\. Clarity and Conciseness](https://console.groq.com/docs/prompting\\#1-clarity-and-conciseness)\n\nKeep prompts straightforward and unambiguous. Avoid unnecessary complexity or vague phrasing.\n\n**Example:**\n\n- _Less Effective:_ \"Tell me about AI.\"\n- _More Effective:_ \"Summarize the recent advancements in artificial intelligence in three bullet points.\"\n\n### [2\\. Explicit Instructions](https://console.groq.com/docs/prompting\\#2-explicit-instructions)\n\nAI models benefit from clear task definitions. Specify details like the output format, desired length, and tone whenever possible.\n\n**Example:**\n\n- _Less Effective:_ \"Write about climate change.\"\n- _More Effective:_ \"Write a 200-word summary of the impact of climate change on agriculture. Use a formal tone.\"\n\n### [3\\. Prompt Placement: Leading with Context](https://console.groq.com/docs/prompting\\#3-prompt-placement-leading-with-context)\n\nPlace the most critical instructions at the very beginning of your prompt. This ensures the model focuses on key objectives before processing any additional context.\n\n**Example:**\n\n- _Less Effective:_ \"Describe the history of quantum mechanics. Also, summarize the applications of quantum mechanics in modern computing.\"\n- _More Effective:_ \"Summarize the applications of quantum mechanics in modern computing. Provide a brief history afterward.\"\n\n### [4\\. System Prompts vs. User Prompts](https://console.groq.com/docs/prompting\\#4-system-prompts-vs-user-prompts)\n\nSystem prompts set the overall behavior and tone—acting as the “rulebook” for responses—while user prompts focus on specific queries or tasks.\n\n**Example:**\n\n- _System Prompt:_ \"You are an expert science communicator. Explain complex topics in simple terms.\"\n- _User Prompt:_ \"Explain Einstein’s theory of relativity for a high school student.\"\n\n### [5\\. Temperature: Balancing Creativity and Precision](https://console.groq.com/docs/prompting\\#5-temperature-balancing-creativity-and-precision)\n\nAdjusting the temperature parameter influences the output's randomness. Lower temperatures (e.g., 0.2) yield deterministic and precise responses—ideal for fact-based or technical answers—whereas higher temperatures (e.g., 0.8) promote creativity and are well-suited for brainstorming or narrative tasks.\n\n**Example for Low Temperature:**\n\n- \"List three key causes of the French Revolution with brief explanations.\"\n\n**Example for High Temperature:**\n\n- \"Imagine you are a French revolutionary in 1789. Write a diary entry describing your experiences.\"\n\n### [6\\. Use of Specific Examples](https://console.groq.com/docs/prompting\\#6-use-of-specific-examples)\n\nFew-shot learning enhances performance by providing clear expectations and context. This is especially useful for coding or data-related tasks.\n\n**Example for JSON Formatting:**\n\n- _Before:_ \"Provide the structure of a JSON response.\"\n- _After:_ \"Provide the structure of a JSON response. Example: `{ \"name\": \"John\", \"age\": 30, \"city\": \"New York\" }`.\"\n\n**Example for Coding Tasks:**\n\n- _Before:_ \"Write a Python function to calculate the factorial of a number.\"\n- _After:_ \"Write a Python function to calculate the factorial of a number. Example: `factorial(5) → 120`.\"\n\n### [7\\. Chain-of-Thought Prompting](https://console.groq.com/docs/prompting\\#7-chainofthought-prompting)\n\nEncourage the model to reason through problems step by step. This method supports logical reasoning and improves problem-solving.\n\n**Example:**\n\n- \"Solve this math problem: If a train travels at 60 mph for 2 hours, how far does it go? Explain step by step.\"\n\n### [8\\. Iterative Prompt Refinement](https://console.groq.com/docs/prompting\\#8-iterative-prompt-refinement)\n\nExperiment with different phrasings to fine-tune outputs. Adjust your prompts based on the model’s responses until you achieve the desired clarity and precision.\n\n**Example:**\n\n- Start with: \"Explain quantum computing.\"\n- If the response is too complex, refine it: \"Explain quantum computing in simple terms for a high school student.\"\n\n### [Conclusion](https://console.groq.com/docs/prompting\\#conclusion)\n\nEffective prompting is the foundation for achieving accurate, reliable, and creative outputs from AI models. Techniques such as clear instructions, thoughtful structure, and parameter tuning apply universally across AI platforms, enabling users to fully leverage model capabilities.\n\nPrompting is an iterative process—no single prompt will work perfectly for every situation. Experiment with different phrasing, structure, and parameters to discover what resonates best with your specific use case.\n\nFor advanced guidance, explore specific [Model Cards](https://www.console.groq.com/docs/models) or get started with a [project](https://github.com/groq/groq-api-cookbook).",
    "metadata": {
      "url": "https://console.groq.com/docs/prompting",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "3b874827-acd2-483e-b5e5-408156b6dd8f",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/prompting",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Trademarks\n\nGroq, GroqCloud™ and the GroqCloud logo are trademarks or registered trademarks of Groq, Inc in the United States of America and other countries.\n\nAll other marks are the property of their respective owners.",
    "metadata": {
      "url": "https://console.groq.com/docs/trademarks",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "58d7f781-6a08-482b-be95-632988b37fee",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/trademarks",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Groq Badge\n\nWe love seeing what you build with the millions of free tokens generated with Groq API each day. For projects and demos built with Groq, please use our\n**Powered by Groq** badge on your application user interface.\n\n![Powered by Groq](https://groq.com/wp-content/uploads/2024/03/PBG-mark1-color.svg)\n\n### [Installation](https://console.groq.com/docs/badge\\#installation)\n\nYou can use the following HTML code snippet to integrate our badge into your user interface:\n\n```bash\n<a href=\"https://groq.com\" target=\"_blank\" rel=\"noopener noreferrer\">\n  <img\n    src=\"https://groq.com/wp-content/uploads/2024/03/PBG-mark1-color.svg\"\n    alt=\"Powered by Groq for fast inference.\"\n  />\n</a>\n```\n\n## Brand Guidelines\n\nFor more badges, logos, and other assets for using our brand in your application and marketing communications, please see our [Brand Guidelines](https://groq.com/brand-guidelines/).",
    "metadata": {
      "url": "https://console.groq.com/docs/badge",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "9337518d-a2af-4bd1-929d-3de587a2ce19",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/badge",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n**GroqCloud Terms of Sale for Customers in the Kingdom of Saudi Arabia**\n\nEffective February 19, 2025\n\nThese terms and conditions (“Terms”) are an agreement between Groq Limited Company, a Saudi limited liability company registered in Riyadh under commercial register number 1009094094 and dated 02/03/1446H, having its head office in Riyadh, Kingdom of Saudi Arabia (“Groq”) and you (“Customer” or “you”) that governs your use of our Services (as defined below). By executing the Order Form for the Services, you agree to be bound by these Terms. These Terms also refer to and incorporate Groqʼs [Privacy Policy](https://console.groq.com/docs/privacy-policy-ksa) for the Kingdom of Saudi Arabia, [Terms of Use](https://console.groq.com/docs/terms-of-use-ksa) for the Kingdom of Saudi Arabia and any ordering document signed by you and Groq ( “Order Form”) or Groq webpage through which you purchased the Services (an “Online Order Form”) (collectively, the “Agreement”).\n\n- **Purchase and Use of Services**\n\n1.1. _Services._ “Services” means services Groq or its affiliates makes available for purchase or use, along with any of our associated software, tools, developer services, documentation, application programming interfaces (“APIs”), and websites, but excluding any Hosted Model, Customer Data or Non-Groq application (as defined below). For the purposes of this Agreement, \"affiliate\" means any entity that directly or indirectly controls, is controlled by, or is under common control with a party. In particular, you agree that some of the Services may be performed for you by Groq, Inc. as agent for and on behalf of Groq.\n\n1.2. _Purchase of Services._ Services and access to the Hosted Model are purchased pursuant to these Terms and include either applicable standard support for the Services that Groq provides to customers at no additional charge or upgraded support if purchased. “Hosted Model” means the artificial intelligence models obtained from publicly available or third-party providers and made available to Customer through Services. Groq will use commercially reasonable efforts to make the Services and Hosted Model available, except for: (a) planned downtime for which Groq can give reasonable notice; and (b) any unavailability beyond Groqʼs reasonable control.\n\n1.3. _Use Rights._ We grant you a non-exclusive right to access and use the Services during the Term (the “Use Rights”). Your Use Rights are non-transferable. Your Use Rights include the right to use Groqʼs APIs to integrate the Services into your applications, products, or services (each a “Customer Application”) and to make the Services available to End Users through your Customer Applications. An “End User” means the legal entity who is authorized by Customer to use the Services through Customer Application.\n\n1.4. _Customer Responsibilities._ Customer will (a) be responsible for End Usersʼ compliance with this Agreement, and (b) be responsible for the accuracy and legality of Customer Data, use of Customer Data with the Services, and Customer use of the Hosted Model. Customer further agrees to use the Services in accordance with the [Terms of Use for the Kingdom of Saudi Arabia](https://console.groq.com/docs/terms-of-use-ksa). You may not make account access credentials available to third parties, share individual login credentials between multiple users on an account, or resell or lease access to your account. You will promptly notify us if you become aware of any unauthorized access to or use of your account or our Services.\n\n1.5. _Third-Party Terms._ The Hosted Model and any Non-Groq Applications are provided by third parties and are subject to separate terms of use. “Non-Groq Application” means application or functionality provided by the Customer or a third-party that interoperates with the Services. Customer will comply with the terms and conditions or license of any Non-Groq Application and Hosted Model with which Customer uses Services. Non-Groq Applications accessible through the Services, including our APIs, may be subject to intellectual property rights, and, if so, you may not use it unless you are licensed to do so by the owner of that content or are otherwise permitted by law. Your access to the content provided by the API may be restricted, limited, or filtered in accordance with applicable law, regulation, and policy. Additionally, some of the software required by or included in our APIs may be offered under an open source license. Open source software licenses constitute separate written agreements. For certain APIs, open source software is listed in the documentation. To the limited extent the open source software license expressly supersedes the Terms, the open source license instead sets forth your agreement with Groq for the applicable open source software.\n\n1.6. _Suspension._ Any breach of this Agreement that in Groqʼs judgment threatens the security, integrity or availability of Services may result in the suspension of Services to Customer and End Users. Groq will restore access to the Services after the event giving rise to the suspension has been resolved to Groqʼs reasonable satisfaction.\n\n- **Payment and Pricing**\n\n2.1. _Fees._ All Services shall be paid for in accordance with the [Pricing Page](https://groq.com/pricing/) or your Order Form, which may also designate the Services as fee-free or otherwise available without triggering a payment due for a limited time. Except as otherwise specified herein or in the applicable Order Form; (a) pricing and fees are calculated based on usage during the Term; (b) payment obligations are non-cancellable and fees are non-refundable: and (c) quantities purchased cannot be decreased during the Term.\n\n2.2. _Taxes._ Unless required by applicable law, fees are considered exclusive of taxes. Customer will be solely responsible for any and all applicable taxes, including but not limited to sales and use taxes, value added tax, corporate tax, withholding tax, excise tax, consumption tax, customs duties or similar charges or fees, which Groq will charge as required in accordance with the prevailing domestic tax laws applicable when the supply is made. Where taxes are due on the transaction, Groq will issue valid tax invoices in the timeframe and form required by the prevailing domestic tax laws.\n\n2.3. _Price Changes._ Price changes will be effective thirty (30) days after they are posted to the [Pricing Page](https://groq.com/pricing/), unless otherwise agreed to in an Order Form. For Services purchased according to an Order Form with annual commitments, pricing may not be changed during the Term of the then-current Order Form. Groq reserves the right to correct pricing errors or mistakes at any time.\n\n2.4. _Payment Terms._ You authorize Groq or our third-party payment processor to charge the payment method on the periodic basis set forth in the Order Form or the [Pricing Page](https://groq.com/pricing/). Fees are payable in U.S. dollars.\n\n2.5. _Late Payments._ Overdue amounts may be subject to a review of Customer’s pricing, and we may suspend the Services immediately without liability to Groq until such payment is made.\n\n2.6. _Prepayment._ You may prepay for Services through the purchase of credits (“Prepayment Credits”). Prepayment Credits are subject to the Prepayment Credit terms that will be provided to you.\n\n- **Restrictions**\n\n3.1. _Restrictions._ We own all right, title, and interest in and to the Services and the APIs. You only receive Use Rights to the Services as explicitly granted in this Agreement. You will not, and will not permit End Users to: (a) use the Services, the APIs or a Non-Groq Application to transmit material in violation of any third-party intellectual property rights; (b) permit direct or indirect access to or use the Services or the APIs in any way that circumvents a contractual usage limit or use Groqʼs intellectual property except as permitted by this Agreement; (c) modify, copy, or create derivative works of a Service, an API or feature through which Customer accesses Services; (d) scrape or build databases with Output returned from the API; (e) disassemble, reverse engineer, or decompile the Services; (f) send any data or information of children under 13 or the applicable age of digital consent; (g) sell, resell, transfer, assign, distribute, license or sublicense access to the Services, any API or api log-ins of keys to a third party; (h) sublicense an API for use by a third party that functions substantially the same as the APIs and offer it for use by third parties; (i) introducing any viruses worms, defects, Trojan horses, malware, or any items of a destructive nature to the Services; (j) defame, abuse, harass, stalk, or threaten others; (k) interfere with or disrupt the APIs or the servers or networks providing the APIs; (l) attempting to or circumventing limitations of the Services, including any action that imposes, or may impose an unreasonable or disproportionately large load on our infrastructure, sending Groq traffic beyond rate limits, or that enforce limitations on use of the Service or any portion thereof; (m) promote or facilitate unlawful online gambling or disruptive commercial messages or advertisements; and (n) use the APIs to process or store any data that is subject to the International Traffic in Arms Regulations maintained by the U.S. Department of State.\n\n- **Data.**\n\n4.1. _Customer Data._ “Customer Data” means electronic data and information submitted by the Customer to the Services and the generated output of the Hosted Model. You and End Users may submit electronic data and information to the Services (“Input”), and receive output from the Services and the generated output of the Hosted Model (“Output”). Input and Output are collectively known as “Customer Data.” Groq acknowledges and agrees that all Customer Data shall remain the property of Customer, and except as explicitly granted in this Agreement, no license, express or implied to use any Customer Data is granted under this Agreement.\n\n4.2. _Access and Processing Customer Data._ We will access and process Customer Data only as necessary to provide you with the Services and comply with applicable law. We will never access Customer Data for training purposes. Customer Data will only be accessed as required for reliable operation of the Service.\n\n4.3. _Monitoring of APIs._ The APIs are designed to help you enhance your websites and applications (\"API Client(s)\"). YOU AGREE THAT GROQ MAY MONITOR USE OF THE APIS TO ENSURE QUALITY, IMPROVE PRODUCTS AND SERVICES, AND VERIFY YOUR COMPLIANCE WITH THE AGREEMENT. Groq may suspend access to the APIs by you or your API Client without liability to Groq or notice if we reasonably believe that you are in violation of the Terms.\n\n4.4. _Your Obligations for Customer Data._ You are responsible for all Input and represent and warrant that you have all rights, licenses, and permissions required to provide Input to the Services. As between you and Groq, the Customer is solely responsible for all use of the Output and the Hosted Model. If you use the Services to process personal data, you must (a) provide legally adequate privacy notices to the relevant individuals involved and obtain necessary consents for the processing of personal data by the Services, when required under applicable data protection law; and (b) process personal data in accordance with applicable law.\n\n- **Confidentiality**\n\n5.1. _Use and Nondisclosure._ “Confidential Information” means any information disclosed by either party (“Discloser”) to the other party (“Recipient”), directly or indirectly, in writing, orally, or by inspection of tangible objects (including documents, prototypes, samples, plant, and equipment), that is designated by the Discloser as confidential or proprietary, that reasonably appears to be confidential due to the nature of the information or circumstances of disclosure, or that is customarily considered confidential between business parties, including customer, product, financial, and strategic information. Recipient agrees it will: (a) only use Discloser's Confidential Information to exercise its rights and fulfill its obligations under this Agreement, (b) take reasonable measures to protect the Confidential Information, and (c) not disclose the Confidential Information to any third party except as expressly permitted in this Agreement.\n\n5.2. _Exceptions._ The obligations in Section 5.1 do not apply to any information that (a) is or becomes generally available to the public through no fault of Recipient, (b) was in Recipientʼs possession or known by it prior to receipt from Discloser, (c) was rightfully disclosed to Recipient without restriction by a third party, or (d) was independently developed without use of Discloserʼs Confidential Information. Recipient may disclose Confidential Information only to its employees, contractors, and agents who have a need to know and who are bound by confidentiality obligations at least as restrictive as those of this Agreement. Recipient will be responsible for any breach of this Section 5 by its employees, contractors, and agents. Recipient may disclose Confidential Information to the extent required by law, provided that Recipient uses reasonable efforts to notify Discloser in advance.\n\n- **Proprietary Rights and Licenses.**\n\n6.1. _Rights to Information._ Groq acknowledges and agrees that all Customer Data shall remain the property of Customer, and except as explicitly granted in this Agreement, no license, express or implied, to use any Customer Data or other Customer intellectual property is granted under this Agreement.\n\n6.2. _Documentation._ Groq shall own all right, title and interest in and to the Services, the APIs and documentation (including without limitation all intellectual property rights therein and all modifications, customizations or other derivative works of the Services and APIs) provided by Groq to Customer under this Agreement.\n\n6.3. _License by Customer._ Customer grants to Groq, a worldwide, limited license for the Term to host, copy, use, transmit and display Customer Data and Non-Groq Applications and program code created by or for Customer using a Service for the purpose of providing and ensuring operation of the Service. Before you submit content to our APIs you will ensure that you have the necessary rights (including the necessary rights from your End Users) to grant us the license.\n\n6.4. _Feedback._ At its option, Customer may provide feedback or suggestions about the Services to Groq (\"Feedback\"). If Customer provides Feedback, then Groq and its affiliates may use that Feedback without restriction and without obligation to Customer.\n\n- **Security**\n\n7.1. _Information Security._ We will maintain an information security program designed to reasonably (a) protect the Services and Customer Data against accidental or unlawful loss, access, or disclosure, (b) identify reasonably foreseeable and internal risks to security and unauthorized access, and (c) minimize security risks, including through regular risk assessments and testing.\n\n7.2. _Our Security Obligations._ As part of our information security program, we will: (a) implement and enforce policies related to electronic, network, and physical monitoring and data storage, transfer, and access; (b) configure network security, firewalls, accounts, and resources for least-privilege access; (c) maintain corrective action plans to respond to potential security threats; and (d) conduct periodic reviews of our security of our information security program as aligned to industry best practices and our own policies and procedures.\n\n- **Privacy**\n\n8.1. _Personal Data._ If you use the Services to process personal data, you must (a) provide legally adequate privacy notices to the relevant individuals and obtain any necessary consents for the processing of their personal data by the Services, when this is required under the applicable law, and (b) process personal data in accordance with applicable data protection law and relevant guidelines issued by the competent authority.\n\n8.2. If you use the Services to process personal data then you acknowledge that you are the controller of that personal data and that Groq acts as a processor of that personal data on your behalf in contexts where data protection law makes these role distinctions. Where data protection law requires, the parties shall execute Groqʼs Data Processing Agreement (\"DPA\") for Customers in the Kingdom of Saudi Arabia to govern such processing of personal data.\n\n8.3. With regard to the processing of your personal data that we conduct as controller in order to provide you with the Services, please refer to our [Privacy Policy for the Kingdom of Saudi Arabia](https://console.groq.com/docs/privacy-policy-ksa).\n\n- **Term; Termination**\n\n9.1. _Term._ This Agreement shall become effective on: (i) the Effective Date on the Order; or (ii) immediately upon execution of the Online Order Form, unless terminated sooner as provided below. This Agreement shall remain in effect for the length of time or date referenced in the applicable Order, or if silent, until one (1) year after the Effective Date (the “Term”). Upon the expiration or termination of the Term, the Services will also terminate. Orders placed through the Online Order Form are effective until terminated by the Customer or by Groq. Groq may terminate an Online Order Form at its convenience, at any time, with thirty (30) day’s notice to Customer.\n\n9.2. _Termination for Cause._ Groq may terminate this Agreement if Customer breaches any material term or condition of this Agreement and fails to cure such breach within thirty (30) days after receipt of written notice specifying the nature of the breach, except for breaches of Section 3 (“Restrictions”) which can result in immediate termination for cause.\n\n9.3. _Renewal._ Upon expiration of the Term, this Agreement will automatically renew for successive periods unless either Party provides intent not to renew. That notice must be given at least thirty days before the start of the next renewal period.\n\n9.4. _Survival._ Those provisions, which by their nature are intended to survive the termination or expiration of this Agreement, shall survive the termination or expiration of this Agreement, including but not limited to: Sections 9, 10, 11, 12, 14 and 16.\n\n- **Warranties; Disclaimer**\n\n10.1. _Right to Perform._ Each party represents that (a) it has the legal rights to enter into this Agreement; and (b) that the person executing this Agreement on behalf of such party has the authority to enter into this Agreement on their behalf and to bind such party to this Agreement.\n\n10.2. _Disclaimer and Limitation of Liability._ EXCEPT AS OTHERWISE EXPLICITLY SET FORTH IN THIS SECTION 10, CUSTOMER ACCEPTS THE SERVICE AND HOSTED MODEL AS-IS, WITH NO REPRESENTATION OR WARRANTY OR CONDITION OF ANY KIND, EITHER EXPRESS OR IMPLIED, STATUTORY OR OTHER WISE, INCLUDING, BUT NOT LIMITED TO, THE WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. GROQ MAKES NO REPRESENTATION OR WARRANTY AS TO THE AVAILABILITY, ACCURACY, SPEED OR PERFORMANCE OF THE SERVICES, THE APIS AND THE HOSTED MODEL. GROQ DOES NOT WARRANT THAT THE SERVICES, THE APIS OR THE HOSTED MODEL WILL PERFORM WITHOUT ERROR OR THAT WILL RUN WITHOUT MATERIAL INTERRUPTION. GROQ HAS NO OBLIGATION TO INDEMNIFY, DEFEND OR HOLD HARMLESS CUSTOMER, INCLUDING WITHOUT LIMITATION AGAINST CLAIMS RELATED TO INFRINGEMENT OR INTELLECTUAL PROPERTY RIGHTS. NEITHER PARTY SHALL BE LIABLE FOR ANY SPECIAL, INCIDENTAL, INDIRECT, EXEMPLARY OR CONSEQUENTIAL DAMAGES ARISING OUT OF THIS AGREEMENT OR ANY RESULTING OBLIGATIONS, WHETHER IN AN ACTION FOR OR ARISING OUT OF BREACH OF CONTRACT, TORT OR ANY OTHER CAUSE OF ACTION, AND EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. EACH PARTY AGREES THAT THESE LIMITATIONS SHALL APPLY NOTWITHSTANDING THE FAILURE OF AN ESSENTIAL PURPOSE OF ANY LIMITED REMEDY.\n\n- **Indemnification**\n\nCustomer shall indemnify, hold harmless, and defend Groq from and against any and all losses, _liabilities_, costs, expenses (including amounts paid in settlement and reasonable attorneysʼ fees), judgments and damages arising out of any third party claim (i) that the Customer Data, or any use of the Customer Data in accordance with this Agreement, infringes or misappropriates such third partyʼs intellectual property right; (ii) based on Customerʼs or End Userʼs use of the Hosted Model, including without limitation violation of third party licenses or use policies or infringement of third party intellectual property rights and privacy rights; or (iii) based on Customerʼs or any Userʼs negligence or willful misconduct or use of the Services in a manner not authorized by this Agreement. Customer may not settle any claim against Groq unless Groq consents to such settlement, and Groq will have the right, at its option, to defend itself against any such claim or to participate in the defense thereof by counsel of its own choice.\n\n- **Limitation of Liability**\n\n12.1. _Limitations on Damages._ Except for (i) a partyʼs gross negligence or willful misconduct, (ii) your breach of Section 3 (Restrictions), (iii) either partyʼs breach of its confidentiality obligations under Section 4 (Confidentiality), neither you nor Groq or our respective affiliates or licensors will be liable under this Agreement for any indirect, punitive, incidental, special, consequential, or exemplary damages (including lost profits) even if that party has been advised of the possibility of those damages.\n\n12.2. _Liability Cap._ Except for (i) a partyʼs gross negligence or willful misconduct or (ii) indemnification obligations under this Agreement and DPA, each partyʼs total liability under the Agreement will not exceed the total amount you have paid to us in the twelve (12) months immediately prior to the event giving rise to liability. The foregoing limitations will apply despite any failure of essential purpose of any limited remedy and to the maximum extent permitted under applicable law.\n\n- **Trade Controls**\n\nYou must comply with all applicable trade laws, including sanctions and export control laws. Our Services may not be used in or for the benefit of, or exported or re-exported to (a) any U.S. embargoed country or territory or (b) any individual or entity with whom dealings are prohibited or restricted under applicable trade laws. Our Services may not be used for any end use prohibited by applicable trade laws, and your Input may not include material or information that requires a government license for release or export.\n\n- **Dispute Resolution**\n\nAny dispute, controversy or claim arising out of or relating to this Agreement, including any dispute relating to the breach, existence, validity, performance, interpretation or termination of the Agreement or any non-contractual obligation arising out of or relating to the Agreement, shall be referred to and finally resolved by arbitration. The arbitration shall be administered by the Saudi Center for Commercial Arbitration (SCCA) in accordance with its Arbitration Rules (\"Rules\"). There shall be three (3) arbitrators, appointed in accordance with the Rules. The seat, or legal place, of arbitration shall be the Saudi Center for Commercial Arbitration in Riyadh, Kingdom of Saudi Arabia, and the arbitration shall be conducted in the English language.\n\n- **Modifications to these Terms**\n\nWe may update these Terms by providing you with reasonable notice, including by posting the update on our website. Your continued use of, or access to, the Services after an update goes into effect will constitute acceptance of the update. If you do not agree with an update, you may stop using the Services.\n\n- **Miscellaneous**\n\n16.1. _Headings._ Headings in these Terms are inserted solely for convenience and are not intended to affect the meaning or interpretation of these Terms.\n\n16.2. _Publicity._ Neither Party will use the other Partyʼs name or marks without prior written approval.\n\n16.3. _Entire Agreement; Conflicts._ This Agreement is the entire agreement between you and Groq with respect to its subject matter and supersedes all prior or contemporaneous agreements, communications and understandings, whether written or oral. You agree that any terms and conditions contained within any purchase order you send to us will not apply to this Agreement and are null and void. If there is a conflict among the documents that make up the Agreement, then the documents will control in the following order (of decreasing precedence) as relevant to the subject matter of the conflict: the DPA for Customers in the Kingdom of Saudi Arabia, these Terms of Sale, the applicable Order Form, and then the [Terms of Use](https://console.groq.com/docs/terms-of-use-ksa) for the Kingdom of Saudi Arabia.\n\n16.4. _Relationship of the Parties._ For all purposes under this Agreement, you and Groq will be and act as an independent contractor and will not bind nor attempt to bind the other to any contract.\n\n16.5. _No Third-Party Beneficiaries._ There are no intended third-party beneficiaries to this Agreement, and it is your and Groqʼs specific intent that nothing contained in this Agreement will give rise to any right or cause of action, contractual or otherwise, in or on behalf of any third party.\n\n16.6. _Force Majeure._ Except for payment obligations, neither you nor Groq will have any liability for failures or delays resulting from conditions beyond your or Groqʼs reasonable control, including but not limited to governmental action or acts of terrorism, earthquake or other acts of God, labor conditions, or power failures.\n\n16.7. _Assignment._ This Agreement cannot be assigned other than as permitted under this Section 16.7 (Assignment). We may assign this Agreement to an affiliate without notice or your consent. Both you and Groq may assign this Agreement to a successor to substantially all the respective partyʼs assets or business, provided that the assigning party provides reasonable (at least 30 days) prior written notice of the assignment. This Agreement will be binding upon the parties and their respective successors and permitted assigns.\n\n16.8. _Notices._ All notices will be in writing. We may provide you notice using the registration information or the email address associated with your account. Service will be deemed given on the date of receipt if delivered by email or on the date sent via courier if delivered by post. We accept service of process at this address: Groq Limited Company, 8476 King Fahad Branch, 4228 Al Muhammadiya Dist., 12363, Riyadh, Kingdom of Saudi Arabia, Attn: Legal Department or at [legal@groq.com](mailto:legal@groq.com).\n\n16.9. _Severability._ In the event that any provision of this Agreement is determined to be illegal or unenforceable, that provision will be limited or eliminated so that this Agreement will otherwise remain in full force and effect and enforceable.\n\n16.10. _Choice of Law._ This Agreement (including the existence, breach, validity, interpretation, performance or termination of this Agreement or any non-contractual obligation arising out of or relating to this Agreement) shall be governed and construed by Saudi law.",
    "metadata": {
      "url": "https://console.groq.com/docs/terms-of-sale-ksa",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "527d5971-cb4c-4464-8712-11f70a6ba84b",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/terms-of-sale-ksa",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)",
    "metadata": {
      "url": "https://console.groq.com/keys",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "68391f71-73f7-4b53-ac10-b018914674e6",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/keys",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## CrewAI + Groq: High-Speed Agent Orchestration\n\nCrewAI is a framework that enables the orchestration of multiple AI agents with specific roles, tools, and goals as a cohesive team to accomplish complex tasks and create sophisticated workflows.\n\nAgentic workflows require fast inference due to their complexity. Groq's fast inference optimizes response times for CrewAI agent teams, enabling rapid autonomous decision-making and collaboration for:\n\n- **Fast Agent Interactions:** Leverage Groq's fast inference speeds via Groq API for efficient agent communication\n- **Reliable Performance:** Consistent response times across agent operations\n- **Scalable Multi-Agent Systems:** Run multiple agents in parallel without performance degradation\n- **Simple Integration:** Get started with just a few lines of code\n\n### [Python Quick Start (2 minutes to hello world)](https://console.groq.com/docs/crewai\\#python-quick-start-2-minutes-to-hello-world)\n\n#### 1\\. Install the required packages:\n\n```bash\npip install crewai groq\n```\n\n#### 2\\. Configure your Groq API key:\n\n```bash\nexport GROQ_API_KEY=\"your-api-key\"\n```\n\n#### 3\\. Create your first Groq-powered CrewAI agent:\n\nIn CrewAI, **agents** are autonomous entities you can design to perform specific roles and achieve particular goals while **tasks** are specific assignments given to agents that detail the actions they\nneed to perform to achieve a particular goal. Tools can be assigned as tasks.\n\n```python\nfrom crewai import Agent, Task, Crew, LLM\n\n# Initialize Large Language Model (LLM) of your choice (see all models on our Models page)\nllm = LLM(model=\"groq/llama-3.1-70b-versatile\")\n\n# Create your CrewAI agents with role, main goal/objective, and backstory/personality\nsummarizer = Agent(\n    role='Documentation Summarizer', # Agent's job title/function\n    goal='Create concise summaries of technical documentation', # Agent's main objective\n    backstory='Technical writer who excels at simplifying complex concepts', # Agent's background/expertise\n    llm=llm, # LLM that powers your agent\n    verbose=True # Show agent's thought process as it completes its task\n)\n\ntranslator = Agent(\n    role='Technical Translator',\n    goal='Translate technical documentation to other languages',\n    backstory='Technical translator specializing in software documentation',\n    llm=llm,\n    verbose=True\n)\n\n# Define your agents' tasks\nsummary_task = Task(\n    description='Summarize this React hook documentation:\\n\\nuseFetch(url) is a custom hook for making HTTP requests. It returns { data, loading, error } and automatically handles loading states.',\n    expected_output=\"A clear, concise summary of the hook's functionality\",\n    agent=summarizer # Agent assigned to task\n)\n\ntranslation_task = Task(\n    description='Translate the summary to Turkish',\n    expected_output=\"Turkish translation of the hook documentation\",\n    agent=translator,\n    dependencies=[summary_task] # Must run after the summary task\n)\n\n# Create crew to manage agents and task workflow\ncrew = Crew(\n    agents=[summarizer, translator], # Agents to include in your crew\n    tasks=[summary_task, translation_task], # Tasks in execution order\n    verbose=True\n)\n\nresult = crew.kickoff()\nprint(result)\n```\n\nWhen you run the above code, you'll see that you've created a summarizer agent and a translator agent working together to summarize and translate documentation! This is a simple example to get you started,\nbut the agents are also able to use tools, which is a powerful combination for building agentic workflows.\n\n**Challenge**: Update the code to add an agent that will write up documentation for functions its given by the user!\n\n### [Advanced Model Configuration](https://console.groq.com/docs/crewai\\#advanced-model-configuration)\n\nFor finer control over your agents' responses, you can easily configure additional model parameters. These settings help you balance between creative and deterministic outputs, control response length,\nand manage token usage:\n\n```python\nllm = LLM(\n    model=\"llama-3.1-70b-versatile\",\n    temperature=0.5,\n    max_completion_tokens=1024,\n    top_p=0.9,\n    stop=None,\n    stream=False,\n)\n```\n\nFor more robust documentation and further resources, including using CrewAI agents with tools for building a powerful agentic workflow, see the following:\n\n- [Official Documentation: CrewAI](https://docs.crewai.com/concepts/llms)\n- [Groq API Cookbook: CrewAI Mixture of Agents Tutorial](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/crewai-mixture-of-agents)\n- [Webinar: Build CrewAI Agents with Groq](https://youtu.be/Q3fh0sWVRX4?si=fhMLPsBF5OBiMfjD)",
    "metadata": {
      "url": "https://console.groq.com/docs/crewai",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "a91f8324-2fc6-4b6c-bc39-fe9938417e44",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/crewai",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## API Error Codes and Responses\n\nOur API uses standard HTTP response status codes to indicate the success or failure of an API request. In cases of errors, the body of the response will contain a JSON object with details about the error. Below are the error codes you may encounter, along with their descriptions and example response bodies.\n\n### [Error Codes Documentation](https://console.groq.com/docs/errors\\#error-codes-documentation)\n\nOur API uses specific error codes to indicate the success or failure of an API request. Understanding these codes and their implications is essential for effective error handling and debugging.\n\n### [Success Codes](https://console.groq.com/docs/errors\\#success-codes)\n\n- **200 OK**: The request was successfully executed. No further action is needed.\n\n### [Client Error Codes](https://console.groq.com/docs/errors\\#client-error-codes)\n\n- **400 Bad Request**: The server could not understand the request due to invalid syntax. Review the request format and ensure it is correct.\n- **401 Unauthorized**: The request was not successful because it lacks valid authentication credentials for the requested resource. Ensure the request includes the necessary authentication credentials and the api key is valid.\n- **404 Not Found**: The requested resource could not be found. Check the request URL and the existence of the resource.\n- **422 Unprocessable Entity**: The request was well-formed but could not be followed due to semantic errors. Verify the data provided for correctness and completeness.\n- **429 Too Many Requests**: Too many requests were sent in a given timeframe. Implement request throttling and respect rate limits.\n- **498 Custom: Flex Tier Capacity Exceeded**: This is a custom status code we use and will return in the event that the flex tier is at capacity and the request won't be processed. You can try again later.\n- **499 Custom: Request Cancelled**: This is a custom status code we use in our logs page to signify when the request is cancelled by the caller.\n\n### [Server Error Codes](https://console.groq.com/docs/errors\\#server-error-codes)\n\n- **500 Internal Server Error**: A generic error occurred on the server. Try the request again later or contact support if the issue persists.\n- **502 Bad Gateway**: The server received an invalid response from an upstream server. This may be a temporary issue; retrying the request might resolve it.\n- **503 Service Unavailable**: The server is not ready to handle the request, often due to maintenance or overload. Wait before retrying the request.\n\n### [Informational Codes](https://console.groq.com/docs/errors\\#informational-codes)\n\n- **206 Partial Content**: Only part of the resource is being delivered, usually in response to range headers sent by the client. Ensure this is expected for the request being made.\n\n### [Error Object Explanation](https://console.groq.com/docs/errors\\#error-object-explanation)\n\nWhen an error occurs, our API returns a structured error object containing detailed information about the issue. This section explains the components of the error object to aid in troubleshooting and error handling.\n\n### [Error Object Structure](https://console.groq.com/docs/errors\\#error-object-structure)\n\nThe error object follows a specific structure, providing a clear and actionable message alongside an error type classification:\n\n```json\n{\n  \"error\": {\n    \"message\": \"String - description of the specific error\",\n    \"type\": \"invalid_request_error\"\n  }\n}\n```\n\n### [Components](https://console.groq.com/docs/errors\\#components)\n\n- **`error` (object):** The primary container for error details.\n  - **`message` (string):** A descriptive message explaining the nature of the error, intended to aid developers in diagnosing the problem.\n  - **`type` (string):** A classification of the error type, such as `\"invalid_request_error\"`, indicating the general category of the problem encountered.",
    "metadata": {
      "url": "https://console.groq.com/docs/errors",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "6eed05bd-d8b7-4ad1-a6b2-7b63da98fad4",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/errors",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## xRx + Groq: Easily Build Rich Multi-Modal Experiences\n\n[xRx](https://github.com/8090-inc/xrx-core) is an open-source framework for building AI-powered applications that interact with users across multiple modalities — multimodality input (x),\nreasoning (R), and multimodality output (x). It allows developers to create sophisticated AI systems that seamlessly integrate text, voice, and\nother interaction forms, providing users with truly immersive experiences.\n\n**Key Features:**\n\n- **Multimodal Interaction**: Effortlessly integrate audio, text, widgets and other modalities for both input and output.\n- **Advanced Reasoning**: Utilize comprehensive reasoning systems to enhance user interactions with intelligent and context-aware responses.\n- **Modular Architecture**: Easily extend and customize components with a modular system of reusable building blocks.\n- **Observability and Guardrails**: Built-in support for LLM observability and guardrails, allowing developers to monitor, debug, and optimize\nreasoning agents effectively.\n\n### [Quick Start Guide (2 minutes + build time)](https://console.groq.com/docs/xrx\\#quick-start-guide-2-minutes--build-time)\n\nThe easiest way to use xRx is to start with an example app and customize it. You can either explore the sample apps collection or try our AI voice tutor for calculus that includes a whiteboard and internal math engine.\n\n### [Option 1: Sample Apps Collection](https://console.groq.com/docs/xrx\\#option-1-sample-apps-collection)\n\n#### 1\\. Clone the Repository\n\n```bash\ngit clone --recursive https://github.com/8090-inc/xrx-sample-apps.git\n```\n\nNote: The `--recursive` flag is required as each app uses the xrx-core submodule.\n\n#### 2\\. Navigate to Sample Apps\n\n```bash\ncd xrx-sample-apps\n```\n\n#### 3\\. Choose and Configure an Application\n\n- Navigate to your chosen app's directory\n- Copy the environment template:\n\n\n\n\n\n\n\n\n```bash\ncp env-example.txt .env\n```\n\n- Configure the required environment variables:\n  - Each application has its own set of required variables\n  - Check the `.env.example` file in the app's directory\n  - Set all required API keys and configuration\n\n> **Tip**: We recommend opening only the specific app folder in your IDE for a cleaner workspace.\n\n#### 4\\. Follow App-Specific Setup\n\n- Each application has its own README with specific instructions\n- Complete any additional setup steps outlined in the app's README\n- Ensure all dependencies are properly configured\n\n#### 5\\. Launch the Application\n\n```bash\ndocker-compose up --build\n```\n\nYour app will be available at `localhost:3000`\n\nFor detailed instructions and troubleshooting, refer to the README in each application's directory.\n\n### [Option 2: AI Voice Tutor](https://console.groq.com/docs/xrx\\#option-2-ai-voice-tutor)\n\n[Math-Tutor on Groq](https://github.com/bklieger-groq/mathtutor-on-groq) is a voice-enabled math tutor powered by Groq that calculates and renders live problems and instruction with LaTeX in seconds! The application demonstrates voice interaction, whiteboard capabilities, and mathematical abilties.\n\n#### 1\\. Clone the Repository\n\n```bash\ngit clone --recursive https://github.com/bklieger-groq/mathtutor-on-groq.git\n```\n\n#### 2\\. Configure Environment\n\n```bash\ncp env-example.txt .env\n```\n\nEdit `.env` with your API keys:\n\n```bash\nLLM_API_KEY=\"your_groq_api_key_here\"\nGROQ_STT_API_KEY=\"your_groq_api_key_here\"\nELEVENLABS_API_KEY=\"your_elevenlabs_api_key\"  # For text-to-speech\n```\n\nYou can obtain:\n\n- Groq API key from the [Groq Console](https://console.groq.com/)\n- [ElevenLabs API key](https://elevenlabs.io/app/settings/api-keys) for voice synthesis\n\n#### 3\\. Launch the Tutor\n\n```bash\ndocker-compose up --build\n```\n\nAccess the tutor at `localhost:3000`\n\n**Challenge**: Modify the math tutor to teach another topic, such as economics, and accept images of problems as input!\n\nFor more information on building applications with xRx and Groq, see:\n\n- [xRx Documentation](https://github.com/8090-inc/xrx-sample-apps)\n- [xRx Example Applications](https://github.com/8090-inc/xrx-sample-apps)\n- [xRx Video Walkthrough](https://www.youtube.com/watch?v=qyXTjpLvg74)",
    "metadata": {
      "url": "https://console.groq.com/docs/xrx",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "785e37de-ab45-4720-a635-fca2e3fec8f7",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/xrx",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## GroqCloud Terms of Sale\n\nThese terms and conditions (Terms) are an agreement between Groq, Inc.,(“Groq”) and you (“Customer” or “you”) that governs your use of our Services (as defined below). By executing the Order for the Services, you agree to be bound by these Terms. These Terms also refer to and incorporate Groq’s Privacy Policy, Terms of Use and any ordering document signed by you and Groq ( “Order Form”) or Groq webpage through which you purchased the Services (an “Online Order Form”) (collectively, the “Agreement”).\n\n### [1\\. Purchase and Use of Services](https://console.groq.com/docs/terms-of-sale\\#1-purchase-and-use-of-services)\n\n_1.1 Services._ Services means services Groq makes available for purchase or use, along with any of our associated software, tools, developer services, documentation, application programming interfaces (“APIs”), and websites, but excluding any Hosted Model, Customer Data or Non-Groq application (as defined below).\n\n_1.2 Purchase of Services._ Services and access to the Hosted Model are purchased pursuant to these Terms and include either applicable standard support for the Services that Groq provides to customers at no additional charge or upgraded support if purchased. “ Hosted Model” means the artificial intelligence models obtained from publicly available or third party providers and made available to Customer through Services. Groq will use commercially reasonable efforts to make the Services and Hosted Model available, except for: (a) planned downtime for which Groq can give reasonable notice; and (b) any unavailability beyond Groq’s reasonable control.\n_1.3 Use Rights._ We grant you a non-exclusive right to access and use the Services during the Term (the “Use Rights”). Your Use Rights are non-transferable. Your Use Rights include the right to use Groq’s APIs to integrate the Services into your applications, products, or services (each a “Customer Application”) and to make the Services available to End Users through your Customer Applications. An “End User” means the legal entity who is authorized by Customer to use the Services through Customer Application.\n\n_1.4 Customer Responsibilities._ Customer will (a) be responsible for End Users’ compliance with this Agreement, and (b) be responsible for the accuracy and legality of Customer Data, use of Customer Data with the Services, and Customer use of the Hosted Model. Customer further agrees to use the Services in accordance with the Terms of Use found here. You may not make account access credentials available to third parties, share individual login credentials between multiple users on an account, or resell or lease access to your account. You will promptly notify us if you become aware of any unauthorized access to or use of your account or our Services.\n\n_1.5 Third-Party Terms._ The Hosted Model and any Non-Groq Applications are provided by third parties and are subject to separate terms of use. “Non-Groq Application” means application or functionality provided by the Customer or a third-party that interoperates with the Services. Customer will comply with the terms and conditions or license of any Non-Groq Application and Hosted Model with which Customer uses Services. Non-Groq Applications accessible through the Services, including our APIs, may be subject to intellectual property rights, and, if so, you may not use it unless you are licensed to do so by the owner of that content or are otherwise permitted by law. Your access to the content provided by the API may be restricted, limited, or filtered in accordance with applicable law, regulation, and policy. Additionally, some of the software required by or included in our APIs may be offered under an open source license. Open source software licenses constitute separate written agreements. For certain APIs, open source software is listed in the documentation. To the limited extent the open source software license expressly supersedes the Terms, the open source license instead sets forth your agreement with Groq for the applicable open source software.\n\n_1.6 Suspension._ Any breach of this Agreement that in Groq’s judgment threatens the security, integrity or availability of Services may result in the suspension of Services to Customer and End Users.\nGroq will restore access to the Services after the event giving rise to the suspension has been resolved to Groq’s reasonable satisfaction.\n\n### [2\\. Payment and Pricing](https://console.groq.com/docs/terms-of-sale\\#2-payment-and-pricing)\n\n_2.1 Fees._ All Services shall be paid for in accordance with the Pricing Page or your Order Form, which may also designate the Services as fee-free or otherwise available without triggering a payment due for a limited time. Except as otherwise specified herein or in the applicable Order Form; (a) pricing and fees are calculated based on usage during the Term; (b) payment obligations are non-cancellable and fees are non-refundable: and (c) quantities purchased cannot be decreased during the Term.\n_2.2 Taxes._ Unless required by applicable law, fees are exclusive of taxes. Customer will be solely responsible for any and all applicable taxes, including but not limited to sales and use taxes, value added tax, excise tax, consumption tax, customs duties or similar charges or fees, which Groq will charge as required by applicable law.\n\n_2.3 Price Changes._ Price changes will be effective thirty (30) days after they are posted to the Pricing Page, unless otherwise agreed to in an Order Form. For Services purchased according to an Order Form with annual commitments, pricing may not be changed during the Term of the then-current Order Form. Groq reserves the right to correct pricing errors or mistakes at any time.\n\n_2.4 Payment Terms._ You authorize Groq or our third-party payment processor to charge the payment method on the periodic basis set forth in the Order Form or the Pricing Page. Fees are payable in U.S. dollars.\n\n_2.5 Late Payments._ Overdue amounts may be subject to a finance charge of 1.5% of the unpaid balance per month, and we may suspend the Services immediately without liability to Groq until such payment is made.\n_2.6 Prepayment._ You may need to prepay for Services through the purchase of credits (“Prepayment Credits”). Prepayment Credits are subject to the Prepayment Credit Terms.\n\n### [3\\. Restrictions](https://console.groq.com/docs/terms-of-sale\\#3-restrictions)\n\n_3.1 Restrictions._ We own all right, title, and interest in and to the Services and the APIs. You only receive Use Rights to the Services as explicitly granted in this Agreement. You will not, and will not permit End Users to: (a) use the Services, the APIs or a Non-Groq Application to transmit material in violation of any third-party intellectual property rights; (b) permit direct or indirect access to or use the Services or the APIs in any way that circumvents a contractual usage limit or use Groq’s intellectual property except as permitted by this Agreement; (c) modify, copy, or create derivative works of a Service, an API or feature through which Customer accesses Services; or (d) scrape or build databases with Output returned from the API; (e) disassemble, reverse engineer, or decompile the Services; (f) send any data or information of children under 13 or the applicable age of digital consent; or (fg sell, resell, transfer, assign, distribute, license or sublicense access to the Services, any API or api log-ins of keys to a third party; (h) sublicense an API for use by a third party that functions substantially the same as the APIs and offer it for use by third parties; (i) introducing any viruses, worms, defects, Trojan horses, malware, or any items of a destructive nature to the Services; (j) defame, abuse, harass, stalk, or threaten others; (k) interfere with or disrupt the APIs or the servers or networks providing the APIs; (l) attempting to or circumventing limitations of the Services, including any action that imposes, or may impose an unreasonable or disproportionately large load on our infrastructure, sending Groq traffic beyond rate limits, or that enforce limitations on use of the Service or any portion thereof; (m) promote or facilitate unlawful online gambling or disruptive commercial messages or advertisements; (n) use the APIs to process or store any data that is subject to the International Traffic in Arms Regulations maintained by the U.S. Department of State.\n_3.2. HIPAA._ Unless otherwise specified in writing by Groq, Groq does not intend use of the APIs to create obligations under the Health Insurance Portability and Accountability Act, as amended (\"HIPAA\"), and makes no representations that the APIs satisfy HIPAA requirements. You agree not to use the Services to create, receive, maintain, transmit, or otherwise process any information that includes or constitutes “Protected Health Information,” as defined under the HIPAA Privacy Rule (45 C.F.R. Section 160.103). If you are (or become) a \"covered entity\" or \"business associate\" as defined in HIPAA, you will not use the APIs for any purpose or in any manner involving transmitting protected health information to Groq.\n\n### [4\\. Data.](https://console.groq.com/docs/terms-of-sale\\#4-data)\n\n_4.1 Customer Data._ “Customer Data” means electronic data and information submitted by the Customer to the Services and the generated output of the Hosted Model. You and End Users may submit electronic data and information to the Services (“Input”), and receive output from the Services and the generated output of the Hosted Model (“Output”). Input and Output are collectively known as “Customer Data.” Groq acknowledges and agrees that all Customer Data shall remain the property of Customer, and except as explicitly granted in this Agreement, no license, express or implied.\n\n_4.2 Access and Processing Customer Data._ We will access and process Customer Data only as necessary to provide you with the Services and comply with applicable law. We will never access Customer Data for training purposes. Customer Data will only be accessed as required for reliable operation of the Service.\n\n_4.3 Monitoring of APIs._ The APIs are designed to help you enhance your websites and applications (\"API Client(s)\"). YOU AGREE THAT GROQ MAY MONITOR USE OF THE APIS TO ENSURE QUALITY, IMPROVE PRODUCTS AND SERVICES, AND VERIFY YOUR COMPLIANCE WITH THE TERMS. Groq may suspend access to the APIs by you or your API Client without liability to Groq or notice if we reasonably believe that you are in violation of the Terms.\n\n_4.3 Your Obligations for Customer Data._ You are responsible for all Input and represent and warrant that you have all rights, licenses, and permissions required to provide Input to the Services. As between you and Groq, the Customer is solely responsible for all use of the Output and the Hosted Model. If you use the Services to process personal data, you must (a) provide legally adequate privacy notices and obtain necessary consents for the processing of personal data by the Services; and (b) process personal data in accordance with applicable law.\n\n### [5\\. Confidentiality](https://console.groq.com/docs/terms-of-sale\\#5-confidentiality)\n\n_5.1 Use and Nondisclosure._ “Confidential Information” means any information disclosed by either party (“Discloser”) to the other party (“Recipient”), directly or indirectly, in writing, orally, or by inspection of tangible objects (including documents, prototypes, samples, plant, and equipment), that is designated by the Discloser as confidential or proprietary, that reasonably appears to be confidential due to the nature of the information or circumstances of disclosure, or that is customarily considered confidential between business parties, including customer, product, financial, and strategic information. Recipient agrees it will: (a) only use Discloser's Confidential Information to exercise its rights and fulfill its obligations under this Agreement, (b) take reasonable measures to protect the Confidential Information, and (c) not disclose the Confidential Information to any third party except as expressly permitted in this Agreement.\n\n_5.2 Exceptions._ The obligations in Section 5.1 do not apply to any information that (a) is or becomes generally available to the public through no fault of Recipient, (b) was in Recipient’s possession or known by it prior to receipt from Discloser, (c) was rightfully disclosed to Recipient without restriction by a third party, or (d) was independently developed without use of Discloser’s Confidential Information. Recipient may disclose Confidential Information only to its employees, contractors, and agents who have a need to know and who are bound by confidentiality obligations at least as restrictive as those of this Agreement. Recipient will be responsible for any breach of this Section 5 by its employees, contractors, and agents. Recipient may disclose Confidential Information to the extent required by law, provided that Recipient uses reasonable efforts to notify Discloser in advance.\n\n### [6\\. Proprietary Rights and Licenses.](https://console.groq.com/docs/terms-of-sale\\#6-proprietary-rights-and-licenses)\n\n_6.1 Rights to Information._ Groq acknowledges and agrees that all Customer Data shall remain the property of Customer, and except as explicitly granted in this Agreement, no license, express or implied, to use any Customer Data or other Customer intellectual property is granted under this Agreement.\n\n_6.2 Documentation._ Groq shall own all right, title and interest in and to the Services, the APIs and documentation (including without limitation all intellectual property rights therein and all modifications, customizations or other derivative works of the Services and APIs) provided by Groq to Customer under this Agreement.\n\n_6.3 License by Customer._ Customer grants to Groq, a worldwide, limited license for the Term to host, copy, use, transmit and display Customer Data and Non-Groq Applications and program code created by or for Customer using a Service for the purpose of providing and ensuring operation of the Service. Before you submit content to our APIs you will ensure that you have the necessary rights (including the necessary rights from your End Users) to grant us the license.\n\n_6.4 License to Use Feedback._ Customer grants to Groq a worldwide, perpetual, irrevocable, royalty-free license to use, distribute, disclose, and make and incorporate into the Service any suggestion, enhancement request, recommendation, correction or other feedback related to the operation or functionality of the Service provided by Customer or an End User.\n\n### [7\\. Security](https://console.groq.com/docs/terms-of-sale\\#7-security)\n\n_7.1 Information Security._ We will maintain an information security program designed to reasonably (a) protect the Services and Customer Data against accidental or unlawful loss, access, or disclosure, (b) identify reasonably foreseeable and internal risks to security and unauthorized access, and (c) minimize security risks, including through regular risk assessments and testing.\n\n_7.2 Our Security Obligations._ As part of our information security program, we will: (a) implement and enforce policies related to electronic, network, and physical monitoring and data storage, transfer, and access; (b) configure network security, firewalls, accounts, and resources for least-privilege access; (c) maintain corrective action plans to respond to potential security threats; and (d) conduct periodic reviews of our security of our information security program as aligned to industry best practices and our own policies and procedures.\n\n### [8\\. Privacy](https://console.groq.com/docs/terms-of-sale\\#8-privacy)\n\n_8.1 Personal Data._ If you use the Services to process personal data, you must (a) provide legally adequate privacy notices to the relevant individuals and obtain any necessary consents for the processing of their personal data by the Services, and (b) process personal data in accordance with applicable data protection and privacy law.\n\n_8.2_ If you use the Services to process personal data then you acknowledge that you are the controller of that personal data and that Groq acts as a processor of that personal data on your behalf in contexts where data protection and privacy law makes these role distinctions. Where data protection and privacy law requires, the parties shall execute Groq’s Data Processing Agreement (\"DPA\") to govern such processing of personal data.\n\n### [9\\. Term; Termination](https://console.groq.com/docs/terms-of-sale\\#9-term-termination)\n\n_9.1 Term._ This Agreement shall become effective on: (i) the Effective Date on the Order; or (ii) immediately upon execution of the Online Order Form, unless terminated sooner as provided below. This Agreement shall remain in effect for the length of time or date referenced in the applicable Order, or if silent, until one (1) year after the Effective Date (the “Term”). Upon the expiration or termination of the Term, the Services will also terminate. Orders placed through the Online Order Form are effective until terminated by the Customer or by Groq. Groq may terminate an Online Order Form at its convenience, at any time, with thirty (30) days notice to Customer.\n\n_9.2 Termination for Cause._ Groq may terminate this Agreement if Customer breaches any material term or condition of this Agreement and fails to cure such breach within thirty (30) days after receipt of written notice specifying the nature of the breach, except for breaches of Section 3 (“Restrictions”) which can result in immediate termination for cause.\n\n_9.3 Renewal._ Upon expiration of the Term, this Agreement will automatically renew for successive periods unless either Party provides intent not to renew. That notice must be given at least thirty days before the start of the next renewal period.\n\n_9.4 Survival._ Those provisions, which by their nature are intended to survive the termination or expiration of this Agreement, shall survive the termination or expiration of this Agreement, including but not limited to: Sections 9, 10, 11, 12, 14 and 16.\n\n### [10\\. Warranties; Disclaimer](https://console.groq.com/docs/terms-of-sale\\#10-warranties-disclaimer)\n\n_10.1 Right to Perform._ Each party represents that (a) it has the legal rights to enter into this Agreement; and (b) that the person executing this Agreement on behalf of such party has the authority to enter into this Agreement on their behalf and to bind such party to this Agreement.\n\n_10.2 Disclaimer and Limitation of Liability._ EXCEPT AS OTHERWISE EXPLICITLY SET FORTH IN THIS SECTION 10, CUSTOMER ACCEPTS THE SERVICE AND HOSTED MODEL AS-IS, WITH NO REPRESENTATION OR WARRANTY OR CONDITION OF ANY KIND, EITHER EXPRESS OR IMPLIED, STATUTORY OR OTHER WISE, INCLUDING, BUT NOT LIMITED TO, THE WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. GROQ MAKES NO REPRESENTATION OR WARRANTY AS TO THE AVAILABILITY, ACCURACY, SPEED OR PERFORMANCE OF THE SERVICES, THE APIS AND THE HOSTED MODEL. GROQ DOES NOT WARRANT THAT THE SERVICES, THE APIS OR THE HOSTED MODEL WILL PERFORM WITHOUT ERROR OR THAT WILL RUN WITHOUT MATERIAL INTERRUPTION. GROQ HAS NO OBLIGATION TO INDEMNIFY, DEFEND OR HOLD HARMLESS CUSTOMER, INCLUDING WITHOUT LIMITATION AGAINST CLAIMS RELATED TO INFRINGEMENT OR INTELLECTUAL PROPERTY RIGHTS. NEITHER PARTY SHALL BE LIABLE FOR ANY SPECIAL, INCIDENTAL, INDIRECT, EXEMPLARY OR CONSEQUENTIAL DAMAGES ARISING OUT OF THIS AGREEMENT OR ANY RESULTING OBLIGATIONS, WHETHER IN AN ACTION FOR OR ARISING OUT OF BREACH OF CONTRACT, TORT OR ANY OTHER CAUSE OF ACTION, AND EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. EACH PARTY AGREES THAT THESE LIMITATIONS SHALL APPLY NOTWITHSTANDING THE FAILURE OF AN ESSENTIAL PURPOSE OF ANY LIMITED REMEDY.\n\n### [11\\. Indemnification](https://console.groq.com/docs/terms-of-sale\\#11-indemnification)\n\nCustomer shall indemnify, hold harmless, and defend Groq from and against any and all losses, liabilities, costs, expenses (including amounts paid in settlement and reasonable attorneys’ fees), judgments and damages arising out of any third party claim (i) that the Customer Data, or any use of the Customer Data in accordance with this Agreement, infringes or misappropriates such third party’s intellectual property right; (ii) based on Customer’s or End User’s use of the Hosted Model, including without limitation violation of third party licenses or use policies or infringement of third party intellectual property rights and privacy rights; or (iii) based on Customer’s or any User’s negligence or willful misconduct or use of the Services in a manner not authorized by this Agreement. Customer may not settle any claim against Groq unless Groq consents to such settlement, and Groq will have the right, at its option, to defend itself against any such claim or to participate in the defense thereof by counsel of its own choice.\n\n### [12\\. Limitation of Liability](https://console.groq.com/docs/terms-of-sale\\#12-limitation-of-liability)\n\n_12.1 Limitations on Damages._ Except for (i) a party’s gross negligence or willful misconduct, (ii) your breach of Section 3 (Restrictions), (iii) either party’s breach of its confidentiality obligations under Section 4 (Confidentiality), neither you nor Groq or our respective affiliates or licensors will be liable under this Agreement for any indirect, punitive, incidental, special, consequential, or exemplary damages (including lost profits) even if that party has been advised of the possibility of those damages.\n\n_12.2 Liability Cap._ Except for (i) a party’s gross negligence or willful misconduct or (ii) indemnification obligations under this Agreement and DPA, each party’s total liability under the Agreement will not exceed the total amount you have paid to us in the twelve (12) months immediately prior to the event giving rise to liability. The foregoing limitations will apply despite any failure of essential purpose of any limited remedy and to the maximum extent permitted under applicable law.\n\n### [13\\. Trade Controls](https://console.groq.com/docs/terms-of-sale\\#13-trade-controls)\n\nYou must comply with all applicable trade laws, including sanctions and export control laws. Our Services may not be used in or for the benefit of, or exported or re-exported to (a) any U.S. embargoed country or territory or (b) any individual or entity with whom dealings are prohibited or restricted under applicable trade laws. Our Services may not be used for any end use prohibited by applicable trade laws, and your Input may not include material or information that requires a government license for release or export.\n\n### [14\\. Dispute Resolution](https://console.groq.com/docs/terms-of-sale\\#14-dispute-resolution)\n\nYOU AGREE TO THE FOLLOWING MANDATORY ARBITRATION AND CLASS ACTION WAIVER PROVISIONS:\n\n\\_14.1 MANDATORY ARBITRATION. You and Groq agree to resolve any claims arising out of or relating to this Agreement or our Services, regardless of when the claim arose, even if it was before this Agreement existed (a “Dispute”), through final and binding arbitration.\n\n\\_14.2 Informal Dispute Resolution. We would like to understand and try to address your concerns prior to formal legal action. Before either of us files a claim against the other, we both agree to try to resolve the Dispute informally. If we are unable to resolve a Dispute within 60 days, either of us has the right to initiate arbitration. We also both agree to attend an individual settlement conference if either party requests one during this time. Any statute of limitations will be tolled during this informal resolution process.\n\n\\_14.3 Arbitration Forum. Both you or Groq may commence binding arbitration through National Arbitration and Mediation (NAM), an alternative dispute resolution provider, and if NAM is not available, you and Groqwill select an alternative arbitral forum. The initiating party must pay all filing fees for the arbitration and payment for other administrative and arbitrator’s costs will be governed by the arbitration provider’s rules. If your claim is determined to be frivolous, you are responsible for reimbursing us for all administrative, hearing, and other fees that we have incurred as a result of the frivolous claim.\n\n\\_14.4 Arbitration Procedures. The arbitration will be conducted by telephone, based on written submissions, video conference, or in person in Santa Clara, California or at another mutually agreed location. The arbitration will be conducted by a sole arbitrator by NAM under its then-prevailing rules. All issues are for the arbitrator to decide, except a California court has the authority to determine (a) whether any provision of this arbitration agreement should be severed and the consequences of said severance, (b) whether you have complied with conditions precedent to arbitration, and (c) whether an arbitration provider is available to hear the arbitration(s) under Section 14.3. The amount of any settlement offer will not be disclosed to the arbitrator by either party until after the arbitrator determines the final award, if any.\n\n\\_14.5 Exceptions. Nothing in this Agreement requires arbitration of the following claims: (a) individual claims brought in small claims court; and (b) injunctive or other equitable relief to stop unauthorized use or abuse of the Services or intellectual property infringement.\n\n\\_14.8 Severability. If any part of this Section 14 is found to be illegal or unenforceable, the remainder will remain in effect, except that if a finding of partial illegality or unenforceability would allow class or representative arbitration, this Section 14 will be unenforceable in its entirety. Nothing in this section will be deemed to waive or otherwise limit the right to seek public injunctive relief or any other non-waivable right, pending a ruling on the substance of that claim from the arbitrator.\n\n### [15\\. Modifications to these Terms](https://console.groq.com/docs/terms-of-sale\\#15-modifications-to-these-terms)\n\n_15.1 Updates._ We may update these Terms by providing you with reasonable notice, including by posting the update on our website. Your continued use of, or access to, the Services after an update goes into effect will constitute acceptance of the update. If you do not agree with an update, you may stop using the Services.\n\n### [16\\. Miscellaneous](https://console.groq.com/docs/terms-of-sale\\#16-miscellaneous)\n\n_16.1 Headings._ Headings in these Terms are inserted solely for convenience and are not intended to affect the meaning or interpretation of these Terms.\n\n_16.2 Publicity._ Neither Party will use the other Party’s name or marks without prior written approval.\n\n_16.3 U.S. Federal Agency Entities._ The Services were developed solely at private expense and are commercial computer software and related documentation within the meaning of the applicable U.S. Federal Acquisition Regulation and agency supplements thereto.\n\n_16.4 Entire Agreement._ This Agreement is the entire agreement between you and Groq with respect to its subject matter and supersedes all prior or contemporaneous agreements, communications and understandings, whether written or oral. You agree that any terms and conditions contained within any purchase order you send to us will not apply to this Agreement and are null and void.\n\n_16.5 Relationship of the Parties._ For all purposes under this Agreement, you and Groqwill be and act as an independent contractor and will not bind nor attempt to bind the other to any contract.\n\n_16.6 No Third Party Beneficiaries._ There are no intended third party beneficiaries to this Agreement, and it is your and Groq’s specific intent that nothing contained in this Agreement will give rise to any right or cause of action, contractual or otherwise, in or on behalf of any third party.\n\n_16.7 Force Majeure._ Except for payment obligations, neither you nor Groqwill have any liability for failures or delays resulting from conditions beyond your or Groq’s reasonable control, including but not limited to governmental action or acts of terrorism, earthquake or other acts of God, labor conditions, or power failures.\n\n_16.8 Assignment._ This Agreement cannot be assigned other than as permitted under this Section 16.8 (Assignment). We may assign this Agreement to an affiliate without notice or your consent. Both you and Groq may assign this Agreement to a successor to substantially all the respective party’s assets or business, provided that the assigning party provides reasonable (at least 30 days) prior written notice of the assignment. This Agreement will be binding upon the parties and their respective successors and permitted assigns.\n\n_16.9 Notices._ All notices will be in writing. We may provide you notice using the registration information or the email address associated with your account. Service will be deemed given on the date of receipt if delivered by email or on the date sent via courier if delivered by post. We accept service of process at this address: Groq Inc, 301 Castro St Suite 200, Mountain View, CA 94041, Attn: [legal@groq.com](mailto:legal@groq.com).\n\n_16.10 Severability._ In the event that any provision of this Agreement is determined to be illegal or unenforceable, that provision will be limited or eliminated so that this Agreement will otherwise remain in full force and effect and enforceable.\n\n_16.11 Jurisdiction, Venue, and Choice of Law._ This Agreement will be governed by the laws of the State of California, excluding California’s conflicts of law rules or principles. Except as provided in Section 14 (Dispute Resolution), all claims arising out of or relating to this Agreement will be brought exclusively in the federal or state courts of Santa Clara, California, USA.\n\nLast updated: 07/31/24",
    "metadata": {
      "url": "https://console.groq.com/docs/terms-of-sale",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "43e61f4f-c150-4557-9431-dd2297023582",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/terms-of-sale",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)",
    "metadata": {
      "url": "https://console.groq.com/settings/team",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "25954ef7-1da8-47a7-ac0e-36cc1176f0d3",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/settings/team",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "Experience the fastest inference in the world\n\n# Create account or login\n\nEmail\n\nLogin with Email\n\nOr continue with\n\n![Github Logo](https://console.groq.com/_next/image?url=%2Fgithub-mark.png&w=48&q=75)Login with GitHub![Google Logo](https://console.groq.com/google.svg)Login with Google\n\nBy continuing, I accept the [Terms of Sale](https://console.groq.com/docs/terms-of-sale) and acknowledge that I have read the [Privacy Policy](https://groq.com/wp-content/uploads/2024/05/Groq-Privacy-Policy_Final_30MAY2024.pdf).",
    "metadata": {
      "url": "https://console.groq.com/login",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "afec6aca-e3b9-4c63-a7fc-57949b783b0c",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/login",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)",
    "metadata": {
      "url": "https://console.groq.com/settings/usage",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "8d038657-5cfa-4253-ab10-3664df257713",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/settings/usage",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# 404\n\n## This page could not be found.",
    "metadata": {
      "url": "https://console.groq.com/docs/guides/speech-to-text/prompting",
      "error": "Not Found",
      "ogUrl": "https://console.groq.com",
      "title": "404: This page could not be found.",
      "og:url": "https://console.groq.com",
      "robots": "noindex",
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "82c5a872-b17c-4e3c-9feb-7293e8206522",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/guides/speech-to-text/prompting",
      "statusCode": 404,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Groq client libraries\n\nGroq provides both a Python and JavaScript/Typescript client library.\n\nPythonJavaScript\n\n### [Groq Python Library](https://console.groq.com/docs/libraries\\#groq-python-library)\n\nThe Groq Python library provides convenient access to the Groq REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients.\n\n## Installation\n\n```shell\npip install groq\n```\n\n## Usage\n\nUse the library and your secret key to run:\n\n```py\n1import os\n2\n3from groq import Groq\n4\n5client = Groq(\n6    # This is the default and can be omitted\n7    api_key=os.environ.get(\"GROQ_API_KEY\"),\n8)\n9\n10chat_completion = client.chat.completions.create(\n11    messages=[\\\n12        {\\\n13            \"role\": \"system\",\\\n14            \"content\": \"you are a helpful assistant.\"\\\n15        },\\\n16        {\\\n17            \"role\": \"user\",\\\n18            \"content\": \"Explain the importance of fast language models\",\\\n19        }\\\n20    ],\n21    model=\"llama-3.3-70b-versatile\",\n22)\n23\n24print(chat_completion.choices[0].message.content)\n```\n\nWhile you can provide an `api_key` keyword argument, we recommend using [python-dotenv](https://github.com/theskumar/python-dotenv) to add `GROQ_API_KEY=\"My API Key\"` to your `.env` file so that your API Key is not stored in source control.\n\nThe following response is generated:\n\n```json\n{\n  \"id\": \"34a9110d-c39d-423b-9ab9-9c748747b204\",\n  \"object\": \"chat.completion\",\n  \"created\": 1708045122,\n  \"model\": \"mixtral-8x7b-32768\",\n  \"system_fingerprint\": \"fp_dbffcd8265\",\n  \"choices\": [\\\n    {\\\n      \"index\": 0,\\\n      \"message\": {\\\n        \"role\": \"assistant\",\\\n        \"content\": \"Low latency Large Language Models (LLMs) are important in the field of artificial intelligence and natural language processing (NLP) for several reasons:\\n\\n1. Real-time applications: Low latency LLMs are essential for real-time applications such as chatbots, voice assistants, and real-time translation services. These applications require immediate responses, and high latency can lead to a poor user experience.\\n\\n2. Improved user experience: Low latency LLMs provide a more seamless and responsive user experience. Users are more likely to continue using a service that provides quick and accurate responses, leading to higher user engagement and satisfaction.\\n\\n3. Competitive advantage: In today's fast-paced digital world, businesses that can provide quick and accurate responses to customer inquiries have a competitive advantage. Low latency LLMs can help businesses respond to customer inquiries more quickly, potentially leading to increased sales and customer loyalty.\\n\\n4. Better decision-making: Low latency LLMs can provide real-time insights and recommendations, enabling businesses to make better decisions more quickly. This can be particularly important in industries such as finance, healthcare, and logistics, where quick decision-making can have a significant impact on business outcomes.\\n\\n5. Scalability: Low latency LLMs can handle a higher volume of requests, making them more scalable than high-latency models. This is particularly important for businesses that experience spikes in traffic or have a large user base.\\n\\nIn summary, low latency LLMs are essential for real-time applications, providing a better user experience, enabling quick decision-making, and improving scalability. As the demand for real-time NLP applications continues to grow, the importance of low latency LLMs will only become more critical.\"\\\n      },\\\n      \"finish_reason\": \"stop\",\\\n      \"logprobs\": null\\\n    }\\\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 24,\n    \"completion_tokens\": 377,\n    \"total_tokens\": 401,\n    \"prompt_time\": 0.009,\n    \"completion_time\": 0.774,\n    \"total_time\": 0.783\n  },\n  \"x_groq\": {\n    \"id\": \"req_01htzpsmfmew5b4rbmbjy2kv74\"\n  }\n}\n```\n\n## Groq community libraries\n\nGroq encourages our developer community to build on our SDK. If you would like your library added, please fill out this [form](https://docs.google.com/forms/d/e/1FAIpQLSfkg3rPUnmZcTwRAS-MsmVHULMtD2I8LwsKPEasuqSsLlF0yA/viewform?usp=sf_link).\n\nPlease note that Groq does not verify the security of these projects. **Use at your own risk.**\n\n### [C\\#](https://console.groq.com/docs/libraries\\#c)\n\n- [jgravelle.GroqAPILibrary](https://github.com/jgravelle/GroqApiLibrary) by [jgravelle](https://github.com/jgravelle)\n\n### [Dart/Flutter](https://console.groq.com/docs/libraries\\#dartflutter)\n\n- [TAGonSoft.groq-dart](https://github.com/TAGonSoft/groq-dart) by [TAGonSoft](https://github.com/TAGonSoft)\n\n### [PHP](https://console.groq.com/docs/libraries\\#php)\n\n- [lucianotonet.groq-php](https://github.com/lucianotonet/groq-php) by [lucianotonet](https://github.com/lucianotonet)\n\n### [Ruby](https://console.groq.com/docs/libraries\\#ruby)\n\n- [drnic.groq-ruby](https://github.com/drnic/groq-ruby) by [drnic](https://github.com/drnic)",
    "metadata": {
      "url": "https://console.groq.com/docs/libraries",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "a6aec2cf-acbf-4ed4-bfea-b3c1679c44b0",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/libraries",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Flex Processing\n\nFlex Processing is a service tier optimized for high-throughput workloads that prioritizes fast inference and can handle occasional request failures. This tier offers significantly higher rate limits while maintaining the same pricing as on-demand processing during beta.\n\n### [Availability](https://console.groq.com/docs/flex-processing\\#availability)\n\nFlex processing is available for all [models](https://console.groq.com/docs/models) to paid customers only with 10x higher rate limits compared to on-demand processing. While in beta, pricing will remain the same as our on-demand tier.\n\n## Service Tiers\n\n- **On-demand ( `\"service_tier\":\"on_demand\"`):** The on-demand tier is the default tier and the one you are used to. We have kept rate limits low in order to ensure fairness and a consistent experience.\n- **Flex ( `\"service_tier\":\"flex\"`):** The flex tier offers on-demand processing when capacity is available, with rapid timeouts if resources are constrained. This tier is perfect for workloads that prioritize fast inference and can gracefully handle occasional request failures. It provides an optimal balance between performance and reliability for workloads that don't require guaranteed processing.\n- **Auto ( `\"service_tier\":\"auto\"`):** The auto tier uses on-demand rate limits, then falls back to flex tier if those limits are exceeded.\n\n## Using Service Tiers\n\n### [Service Tier Parameter](https://console.groq.com/docs/flex-processing\\#service-tier-parameter)\n\nThe `service_tier` parameter is an additional, optional parameter that you can include in your chat completion request to specify the service tier you'd like to use. The possible values are:\n\n| Option | Description |\n| --- | --- |\n| `flex` | Only uses flex tier limits |\n| `on_demand` (default) | Only uses on\\_demand rate limits |\n| `auto` | First uses on\\_demand rate limits, then falls back to flex tier if exceeded |\n\n### [Example Usage](https://console.groq.com/docs/flex-processing\\#example-usage)\n\ncurlJavaScriptPythonJSON\n\n```shell\nimport os\nimport requests\n\nGROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n\ndef main():\n    try:\n        response = requests.post(\n            \"https://api.groq.com/openai/v1/chat/completions\",\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": f\"Bearer {GROQ_API_KEY}\"\n            },\n            json={\n                \"service_tier\": \"flex\",\n                \"model\": \"llama-3.3-70b-versatile\",\n                \"messages\": [{\\\n                    \"role\": \"user\",\\\n                    \"content\": \"whats 2 + 2\"\\\n                }]\n            }\n        )\n        print(response.json())\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n\nif __name__ == \"__main__\":\n    main()\n```",
    "metadata": {
      "url": "https://console.groq.com/docs/flex-processing",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "bab125f9-d9b2-4589-b817-a6d8eea544a1",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/flex-processing",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Copyright\n\nCopyright 2025 Groq, Inc. All rights reserved.",
    "metadata": {
      "url": "https://console.groq.com/docs/copyright",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "8c001d58-9267-4472-be0c-a8917fdc2025",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/copyright",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Vision\n\nGroq API offers fast inference and low latency for multimodal models with vision capabilities for understanding and interpreting visual data from images. By analyzing the content of an image, multimodal models can generate\nhuman-readable text for providing insights about given visual data.\n\n### [Supported Model](https://console.groq.com/docs/vision\\#supported-model)\n\nGroq API supports powerful multimodal models that can be easily integrated into your applications to provide fast and accurate image processing for tasks such as visual question answering, caption generation,\nand Optical Character Recognition (OCR):\n\n### Llama 3.2 90B Vision (Preview)\n\nModel ID\n\n\\`llama-3.2-90b-vision-preview\\`\n\nDescription\n\nA powerful multimodal model capable of processing both text and image inputs that supports multilingual, multi-turn conversations, tool use, and JSON mode.\n\nContext Window\n\n8,192 tokens\n\nPreview Model\n\nCurrently in preview and should be used for experimentation.\n\nImage Size Limit\n\nMaximum allowed size for a request containing an image URL as input is 20MB. Requests larger than this limit will return a 400 error.\n\nRequest Size Limit (Base64 Encoded Images)\n\nMaximum allowed size for a request containing a base64 encoded image is 4MB. Requests larger than this limit will return a 413 error.\n\nSingle Image per Request\n\nOnly one image can be processed per request in the preview release. Requests with multiple images will return a 400 error.\n\nSystem Prompt\n\nDoes not support system prompts and images in the same request.\n\n### Llama 3.2 11B Vision (Preview)\n\nModel ID\n\n\\`llama-3.2-11b-vision-preview\\`\n\nDescription\n\nA powerful multimodal model capable of processing both text and image inputs that supports multilingual, multi-turn conversations, tool use, and JSON mode.\n\nContext Window\n\n8,192 tokens\n\nPreview Model\n\nCurrently in preview and should be used for experimentation.\n\nImage Size Limit\n\nMaximum allowed size for a request containing an image URL as input is 20MB. Requests larger than this limit will return a 400 error.\n\nRequest Size Limit (Base64 Encoded Images)\n\nMaximum allowed size for a request containing a base64 encoded image is 4MB. Requests larger than this limit will return a 413 error.\n\nSingle Image per Request\n\nOnly one image can be processed per request in the preview release. Requests with multiple images will return a 400 error.\n\nSystem Prompt\n\nDoes not support system prompts and images in the same request.\n\n### [How to Use Vision](https://console.groq.com/docs/vision\\#how-to-use-vision)\n\nUse Groq API vision features via:\n\n- **[GroqCloud Console Playground](https://console.groq.com/playground)**: Select `llama-3.2-90b-vision-preview` or `llama-3.2-11b-vision-preview` as the model and\nupload your image.\n- **Groq API Request:** Call the `chat.completions` API endpoint (i.e. `https://api.groq.com/openai/v1/chat/completions`) and set `model_id` to `llama-3.2-90b-vision-preview` or `llama-3.2-11b-vision-preview`.\nSee code examples below.\n\n#### How to Pass Images from URLs as Input\n\nThe following are code examples for passing your image to the model via a URL:\n\ncurlJavaScriptPythonJSON\n\n```py\n1from groq import Groq\n2\n3client = Groq()\n4completion = client.chat.completions.create(\n5    model=\"llama-3.2-11b-vision-preview\",\n6    messages=[\\\n7        {\\\n8            \"role\": \"user\",\\\n9            \"content\": [\\\n10                {\\\n11                    \"type\": \"text\",\\\n12                    \"text\": \"What's in this image?\"\\\n13                },\\\n14                {\\\n15                    \"type\": \"image_url\",\\\n16                    \"image_url\": {\\\n17                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg\"\\\n18                    }\\\n19                }\\\n20            ]\\\n21        }\\\n22    ],\n23    temperature=1,\n24    max_completion_tokens=1024,\n25    top_p=1,\n26    stream=False,\n27    stop=None,\n28)\n29\n30print(completion.choices[0].message)\n```\n\n#### How to Pass Locally Saved Images as Input\n\nTo pass locally saved images, we'll need to first encode our image to a base64 format string before passing it as the `image_url` in our API request as follows:\n\n```py\n1from groq import Groq\n2import base64\n3\n4\n5# Function to encode the image\n6def encode_image(image_path):\n7  with open(image_path, \"rb\") as image_file:\n8    return base64.b64encode(image_file.read()).decode('utf-8')\n9\n10# Path to your image\n11image_path = \"sf.jpg\"\n12\n13# Getting the base64 string\n14base64_image = encode_image(image_path)\n15\n16client = Groq()\n17\n18chat_completion = client.chat.completions.create(\n19    messages=[\\\n20        {\\\n21            \"role\": \"user\",\\\n22            \"content\": [\\\n23                {\"type\": \"text\", \"text\": \"What's in this image?\"},\\\n24                {\\\n25                    \"type\": \"image_url\",\\\n26                    \"image_url\": {\\\n27                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\\\n28                    },\\\n29                },\\\n30            ],\\\n31        }\\\n32    ],\n33    model=\"llama-3.2-11b-vision-preview\",\n34)\n35\n36print(chat_completion.choices[0].message.content)\n```\n\n#### Tool Use with Images\n\nThe `llama-3.2-90b-vision-preview` and `llama-3.2-11b-vision-preview` models support tool use! The following cURL example defines a `get_current_weather` tool that the model can leverage to answer a user query that contains a question about the\nweather along with an image of a location that the model can infer location (i.e. New York City) from:\n\n```shell\ncurl https://api.groq.com/openai/v1/chat/completions -s \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GROQ_API_KEY\" \\\n-d '{\n\"model\": \"llama-3.2-11b-vision-preview\",\n\"messages\": [\\\n{\\\n    \"role\": \"user\",\\\n    \"content\": [{\"type\": \"text\", \"text\": \"Whats the weather like in this state?\"}, {\"type\": \"image_url\", \"image_url\": { \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"}}]\\\n}\\\n],\n\"tools\": [\\\n{\\\n    \"type\": \"function\",\\\n    \"function\": {\\\n    \"name\": \"get_current_weather\",\\\n    \"description\": \"Get the current weather in a given location\",\\\n    \"parameters\": {\\\n        \"type\": \"object\",\\\n        \"properties\": {\\\n        \"location\": {\\\n            \"type\": \"string\",\\\n            \"description\": \"The city and state, e.g. San Francisco, CA\"\\\n        },\\\n        \"unit\": {\\\n            \"type\": \"string\",\\\n            \"enum\": [\"celsius\", \"fahrenheit\"]\\\n        }\\\n        },\\\n        \"required\": [\"location\"]\\\n    }\\\n    }\\\n}\\\n],\n\"tool_choice\": \"auto\"\n}' | jq '.choices[0].message.tool_calls'\n```\n\nThe following is the output from our example above that shows how our model inferred the state as New York from the given image and called our example function:\n\n```python\n[\\\n  {\\\n    \"id\": \"call_q0wg\",\\\n    \"function\": {\\\n      \"arguments\": \"{\\\"location\\\": \\\"New York, NY\\\",\\\"unit\\\": \\\"fahrenheit\\\"}\",\\\n      \"name\": \"get_current_weather\"\\\n    },\\\n    \"type\": \"function\"\\\n  }\\\n]\n```\n\n#### JSON Mode with Images\n\nThe `llama-3.2-90b-vision-preview` and `llama-3.2-11b-vision-preview` models support JSON mode! The following Python example queries the model with an image and text (i.e. \"Please pull out relevant information as a JSON object.\") with `response_format`\nset for JSON mode:\n\n```py\n1from groq import Groq\n2\n3client = Groq()\n4completion = client.chat.completions.create(\n5    model=\"llama-3.2-90b-vision-preview\",\n6    messages=[\\\n7        {\\\n8            \"role\": \"user\",\\\n9            \"content\": [\\\n10                {\\\n11                    \"type\": \"text\",\\\n12                    \"text\": \"List what you observe in this photo in JSON format.\"\\\n13                },\\\n14                {\\\n15                    \"type\": \"image_url\",\\\n16                    \"image_url\": {\\\n17                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/d/da/SF_From_Marin_Highlands3.jpg\"\\\n18                    }\\\n19                }\\\n20            ]\\\n21        }\\\n22    ],\n23    temperature=1,\n24    max_completion_tokens=1024,\n25    top_p=1,\n26    stream=False,\n27    response_format={\"type\": \"json_object\"},\n28    stop=None,\n29)\n30\n31print(completion.choices[0].message)\n```\n\n#### Multi-turn Conversations with Images\n\nThe `llama-3.2-90b-vision-preview` and `llama-3.2-11b-vision-preview` models support multi-turn conversations! The following Python example shows a multi-turn user conversation about an image:\n\n```py\n1from groq import Groq\n2\n3client = Groq()\n4completion = client.chat.completions.create(\n5    model=\"llama-3.2-11b-vision-preview\",\n6    messages=[\\\n7        {\\\n8            \"role\": \"user\",\\\n9            \"content\": [\\\n10                {\\\n11                    \"type\": \"text\",\\\n12                    \"text\": \"What is in this image?\"\\\n13                },\\\n14                {\\\n15                    \"type\": \"image_url\",\\\n16                    \"image_url\": {\\\n17                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/d/da/SF_From_Marin_Highlands3.jpg\"\\\n18                    }\\\n19                }\\\n20            ]\\\n21        },\\\n22        {\\\n23            \"role\": \"user\",\\\n24            \"content\": \"Tell me more about the area.\"\\\n25        }\\\n26    ],\n27    temperature=1,\n28    max_completion_tokens=1024,\n29    top_p=1,\n30    stream=False,\n31    stop=None,\n32)\n33\n34print(completion.choices[0].message)\n```\n\n### [Venture Deeper into Vision](https://console.groq.com/docs/vision\\#venture-deeper-into-vision)\n\n#### Use Cases to Explore\n\nVision models can be used in a wide range of applications. Here are some ideas:\n\n- **Accessibility Applications:** Develop an application that generates audio descriptions for images by using a vision model to generate text descriptions for images, which can then\nbe converted to audio with one of our audio endpoints.\n- **E-commerce Product Description Generation:** Create an application that generates product descriptions for e-commerce websites.\n- **Multilingual Image Analysis:** Create applications that can describe images in multiple languages.\n- **Multi-turn Visual Conversations:** Develop interactive applications that allow users to have extended conversations about images.\n\nThese are just a few ideas to get you started. The possibilities are endless, and we're excited to see what you create with vision models powered by Groq for low latency and fast inference!\n\n#### Next Steps\n\nCheck out our [Groq API Cookbook](https://github.com/groq/groq-api-cookbook) repository on GitHub (and give us a ⭐) for practical examples and tutorials:\n\n- [Image Moderation](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/image_moderation.ipynb)\n- [Multimodal Image Processing (Tool Use, JSON Mode)](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/multimodal-image-processing)\n\nWe're always looking for contributions. If you have any cool tutorials or guides to share, submit a pull request for review to help our open-source community!",
    "metadata": {
      "url": "https://console.groq.com/docs/vision",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "b5c1f766-8305-4de9-be9a-f6fe170c55fc",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/vision",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)",
    "metadata": {
      "url": "https://console.groq.com/settings/profile",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "d44513b0-4d17-4d43-9207-a9eb504f98d8",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/settings/profile",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Composio\n\n[Composio](https://composio.ai/) is a platform for managing and integrating tools with LLMs and AI agents. You can build fast, Groq-based assistants to seamlessly interact with external applications\nthrough features including:\n\n- **Tool Integration:** Connect AI agents to APIs, RPCs, shells, file systems, and web browsers with 90+ readily available tools\n- **Authentication Management:** Secure, user-level auth across multiple accounts and tools\n- **Optimized Execution:** Improve security and cost-efficiency with tailored execution environments\n- **Comprehensive Logging:** Track and analyze every function call made by your LLMs\n\n### [Python Quick Start (5 minutes to hello world)](https://console.groq.com/docs/composio\\#python-quick-start-5-minutes-to-hello-world)\n\n#### 1\\. Install the required packages:\n\n```bash\npip install composio-langchain langchain-groq\n```\n\n#### 2\\. Configure your Groq and [Composio](https://app.composio.dev/) API keys:\n\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\nexport COMPOSIO_API_KEY=\"your-composio-api-key\"\n```\n\n#### 3\\. Connect your first Composio tool:\n\n```bash\n# Connect GitHub (you'll be guided through OAuth flow to get things going)\ncomposio add github\n\n# View all available tools\ncomposio apps\n```\n\n#### 4\\. Create your first Composio-enabled Groq agent:\n\nRunning this code will create an agent that can interact with GitHub through natural language in mere seconds! Your agent will be able to:\n\n- Perform GitHub operations like starring repos and creating issues for you\n- Securely manage your OAuth flows and API keys\n- Process natural language to convert your requests into specific tool actions\n- Provide feedback to let you know about the success or failure of operations\n\n```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_groq import ChatGroq\nfrom composio_langchain import ComposioToolSet, App\n\n# Initialize LLM\nllm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n\n# Get Composio tools (GitHub in this example)\ncomposio_toolset = ComposioToolSet()\ntools = composio_toolset.get_tools(apps=[App.GITHUB])\n\n# Create agent\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\n# Define task and run\ntask = \"Star groq/groq-api-cookbook repo on GitHub\"\nagent.run(task)\n```\n\n**Challenge**: Create a Groq-powered agent that can summarize your GitHub issues and post updates to Slack through Composio tools!\n\nFor more detailed documentation and resources on building AI agents with Groq and Composio, see:\n\n- [Composio documentation](https://docs.composio.dev/framework/groq)\n- [Guide to Building Agents with Composio and Llama 3.1 models powered by Groq](https://composio.dev/blog/tool-calling-in-llama-3-a-guide-to-build-agents/)\n- [Groq API Cookbook tutorial](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/composio-newsletter-summarizer-agent)",
    "metadata": {
      "url": "https://console.groq.com/docs/composio",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "9eb2cf00-604c-404b-b6ca-0462c11d7ba1",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/composio",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n# **Groq Terms of Use for the Kingdom of Saudi Arabia**\n\nEffective February 19, 2025\n\n1\\. **General Information Regarding These Terms of Use**\n\nThese terms of use and any other terms and conditions that may accompany the materials made through this website (collectively, the “Terms”) apply to all Groq.com webpages (collectively, the “Websites”). The Terms also apply to all information and services provided by Groq Limited Company, a Saudi limited liability company registered in Riyadh under commercial register number 1009094094 and dated 02/03/1446H, having its head office in Riyadh Kingdom of Saudi Arabia, (\"Groq\"), or its affiliates, through the Websites, (together with the Websites, the “Services”). Services offered are provided subject to these Terms, the Groq [Privacy Policy](https://console.groq.com/docs/privacy-policy-ksa) for the Kingdom of Saudi Arabia (available on the Groq Websites), and any additional terms specified on the relevant Website(s) or provided when Services are obtained, all of which are hereby incorporated by reference into these Terms. By accessing, visiting, or otherwise using the Websites, you agree to be bound by the Terms and have the legal authority to accept the terms. If you are an individual who is entering into these Terms on behalf of an entity, you represent and warrant that you have the power to bind that entity, and you hereby agree on that entity’s behalf to be bound by these Terms, with the terms “you,” and “your” applying to you, that entity, and other users accessing the Services on behalf of that entity. For the purpose of these Terms, \"affiliate\" means any entity that directly or indirectly controls, is controlled by, or is under common control with a party. In particular, you agree that some of the Services may be performed for you by Groq, Inc., as agent for and on behalf of Groq.\n\n2\\. **Use of the Groq.com Site**\n\nGroq is committed to ensuring a secure experience for everyone that accesses or uses the Websites. In order to achieve this, there are basic rules you must follow.\n\nGroq follows the laws, and you are required to do the same\\*\\*.\\*\\* You may use the Websites only for lawful purposes and in accordance with these Terms. Additionally, You agree not to:\n\n- Use the Website or Services in any way that violates any applicable federal, state, local, or international law or regulation (including, without limitation, any laws regarding the export of data or software to and from the US or other countries).\n- Exploit, harm, or attempt to exploit or harm minors in any way by exposing them to inappropriate content, asking for personally identifiable information, or otherwise.\n- Violate copyright, trademark, or other intellectual property laws.\n- Distribute unwanted, unsolicited, or harassing mass email or other messages, promotions, advertising, or solicitations (“spam”).\n- Attack, abuse, interfere with, intercept, disrupt, or exploit any users, systems, or services, regardless of how accomplished and notwithstanding anything to the contrary in these Terms, including but not limited to Denial of Service (DoS), monitoring, crawling, spamming, using bots or scripts, or distributing malware (such as viruses, Trojan horses, worms, spyware, or adware).\n- Attempt to collect, store, or publish personally identifiable information (a) without having informed the data subject as required by the applicable data protection law and having obtained their consent when required under the law or (b) of a minor under the age of thirteen (18) in any circumstance.\n- Post or transmit content on or through the Websites or Services that is harmful, offensive, obscene, abusive, invasive of privacy, defamatory, hateful or otherwise discriminatory, false or misleading, or incites an illegal act.\n- You may not impersonate another person or entity or use or attempt to use another’s account or personal information without authorization.\n- Access or use any application, system, service, tool, data, account, network, or content without authorization or for unintended purposes.\n- Attempting to or circumventing the free tier limits of the Services, including any action that imposes, or may impose an unreasonable or disproportionately large load on our infrastructure, or sending Groq traffic beyond rate limits.\n- Use the Services in any manner that impacts (i) the stability of the hardware running the Service, (ii) the operation or performance of the Service or other user’s experience or use of the Service, or (iii) the behavior of other applications that use the Service.\n- Create or develop features that prevent or restrict use or copying of any content or that enforce limitations on use of the Service or any portion thereof.\n- Buy, sell or transfer API keys without prior written consent.\n\n3\\. **Changes to the Terms**\n\nWe may revise and update these Terms from time to time in our sole discretion. We will provide you at least 30 days advance notice for any material change to these Terms. All other changes are effective immediately when we post them. Your continued use of any Website or Services following the posting of revised Terms means that you accept and agree to the changes.\n\n4\\. **Content Available through the Services**\n\nGroq attempts to provide accurate information on its Websites. However, we take no responsibility for the accuracy of the information, content or materials which you may have access to as part of, or through your use of the Websites or Services. All information, content and material are provided as-is.\n\nYou agree that you are solely responsible for your reuse of information, content or materials made available through the Websites or Services, including providing proper attribution. You should review the terms of the applicable license prior to reuse.\n\n_Licensing_: Nothing in these Terms grants you any license right or other rights to the Website, Groq Services, products or materials provided through the website.\n\n5\\. **Generative AI.**\n\nIn addition to these Terms, use of and access to generative AI models made available through the Website (“Generative AI Services”) are also subject to terms and conditions specified by the owner of the generative AI models. You agree that your use of any of these models will comply with the additional terms and conditions as specified on the Model Card for the generative AI models identified at [https://console.groq.com/docs/models](https://console.groq.com/docs/models).\n\nNotwithstanding the restriction on licensing set forth in section 4, _Content Available through the Services_, User Data shall be owned by you and is not retained or used by Groq other than to perform the Generative AI Services hereunder. User Data includes any and all content generated by the Generative AI Services in response to a Prompt (“Outputs”) as well as your Prompts and Training Data. “Prompts” are defined as any and all instructions, queries, visual or textual cues given by you to the Generative AI Services in order to generate an Output. “Training Data” is user supplied data for fine tuning or customization of models and may comprise a collection of textual, visual, and/or multimedia data that provides the model with the necessary context, knowledge, and inspiration to generate meaningful, coherent and responsive Outputs.\n\nIf you supply any Training Data for the purpose of prompting, fine-tuning or customizing the Generative AI Services to your specific needs or use-case, Groq will not use the Training Data other than to perform the Generative AI Services for you. Groq does not permanently retain Prompts, Output or your Training Data on its servers.\n\nWhen you use the Generative AI Services, you provide Prompts to generate Outputs in return on a third-party model. You are solely responsible for your use of the Prompts and the Outputs and for complying with the terms of use specified by the third-party model owner. You shall only use Prompts to which you own all required rights under applicable law and do so in a manner that is consistent with the applicable law. You shall not intentionally make the Generative AI Services generate Outputs infringing intellectual property rights, third party rights or applicable law, or use such infringing Outputs after you become aware of such infringement.\n\nYou hereby represent that you own your Prompts. You retain all the rights, including but not limited to the intellectual property rights to Your Prompts. You grant Groq a worldwide, revocable, non-exclusive, non-sublicensable, non-transferable right to use the Prompts, together with your Training Data, for the purpose of performing the Generative AI Services, for the term of these Terms.\n\nWhere applicable, Groq assigns to you, all the intellectual property rights Groq may have in the Outputs generated by your Prompts. This assignment is granted worldwide and for the entire legal term of protection of the Outputs by the intellectual property rights applicable as provided for by the applicable law. However, you are expressly prohibited to use the Outputs and/or any modified or derived version of the Outputs to (directly or indirectly) to reverse engineer any aspect of the Generative AI Services.\n\nYou agree that, due to the nature of Generative AI Services, if another user uses a Prompt similar to yours, the Generative AI Services may generate an Output similar or identical to yours. We do not warrant that your Output is not similar or identical to another user’s Output. Consequently, we will not indemnify you in case your Output is similar or identical to another user’s Output or Training Data.\n\nYou acknowledge and agree that Generative AI Services are inherently subject to certain unpredictabilities, as such Outputs depend on your Prompt and the technology behind the Generative AI Services which is complex and continuously evolving.\n\nFOR THE AVOIDANCE OF ANY DOUBT AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE GENERATIVE AI SERVICES ARE PROVIDED WITHOUT ANY EXPRESS OR IMPLIED WARRANTY REGARDING THE QUALITY OR THE ACCURACY OF THE OUTPUTS. CONSEQUENTLY, YOU AGREE THAT THE OUTPUTS GENERATED BY GENERATIVE AI SERVICES MAY BE INCOMPLETE, NOT UP-TO-DATE OR NOT ENTIRELY ACCURATE.\n\nThe Generative AI Services may sometimes provide inaccurate or offensive content that doesn’t represent Groq’s views. You agree to use discretion before relying on, publishing, or otherwise using Output provided by the Generative AI Services. If you provide any feedback pertaining to the accuracy, relevance, and effectiveness of the Outputs, you grant Groq a worldwide, non-revocable, non-exclusive, non-sublicensable, non-transferable right to use your Feedback for the purpose of improving the Generative AI Services. You are responsible for making commercially reasonable efforts to make sure the Generative AI Services do not generate Outputs that contain offensive, inappropriate or illicit content. To this end, Groq strongly recommends that you implement appropriate moderation mechanisms for the model(s), by way of example, including the use of a system prompt that instructs the model to generate responses that are safe, respectful, and appropriate as well as providing clear and accessible guidelines for users. You are responsible for continuously monitoring and evaluating the model’s performance to identify and address any issues or concerns arising from your use. Because, neither we nor the model owners warrant that the Output generated will not be offensive, inappropriate or illicit, you are solely responsible for the use of the Output and you shall in no way use the Output for any illicit or unlawful purpose and/or to harm others. **Do not rely on the Generative AI Services for medical, legal, financial, or other professional advice. Any content regarding those topics is provided for informational purposes only and is not a substitute for advice from a qualified professional.**\n\n6\\. **Registered Users**\n\n_Registration_: You must be 18 years of age or older to register for an account. You agree to (a) only provide accurate and current information about yourself, (b) maintain the security of your passwords and identification, (c) promptly update the email address listed in connection with your account to keep it accurate so that we can contact you, and (d) be fully responsible for all uses of your account. You must not set up an account on behalf of another individual or entity unless you are authorized to do so.\n\n_Termination_: Groq reserves the right to modify or discontinue your account at any time for any reason or no reason at all.\n\n**7\\. DISCLAIMER OF WARRANTIES**\n\nTO THE FULLEST EXTENT PERMITTED BY APPLICABLE LAW, GROQ OFFERS THE SERVICES (INCLUDING ALL CONTENT AVAILABLE ON OR THROUGH THE SERVICES) AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WEBSITES OR SERVICES, EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, INCLUDING WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT. GROQ DOES NOT WARRANT THAT THE FUNCTIONS OF THE WEBSITES OR SERVICES WILL BE UNINTERRUPTED OR ERROR-FREE, THAT CONTENT MADE AVAILABLE ON OR THROUGH THE WEBSITES OR SERVICES WILL BE ERROR-FREE, THAT DEFECTS WILL BE CORRECTED, OR THAT ANY SERVERS USED BY GROQ ARE FREE OF VIRUSES OR OTHER HARMFUL COMPONENTS. GROQ DOES NOT WARRANT OR MAKE ANY REPRESENTATION REGARDING USE OF THE CONTENT AVAILABLE THROUGH THE WEBSITES OR SERVICES IN TERMS OF ACCURACY, RELIABILITY, OR OTHERWISE.\n\n**8\\. LIMITATION OF LIABILITY**\n\nNEITHER GROQ NOR IT’S SUPPLIERS WILL BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY INCIDENTAL, DIRECT, INDIRECT, PUNITIVE, ACTUAL, CONSEQUENTIAL, SPECIAL, EXEMPLARY, OR OTHER DAMAGES, INCLUDING WITHOUT LIMITATION, LOSS OF REVENUE OR INCOME, LOST PROFITS, PAIN AND SUFFERING, EMOTIONAL DISTRESS, COST OF SUBSTITUTE GOODS OR SERVICES, OR SIMILAR DAMAGES SUFFERED OR INCURRED BY YOU OR ANY THIRD PARTY THAT ARISE IN CONNECTION WITH THE WEBSITES OR SERVICES (OR THE TERMINATION THEREOF FOR ANY REASON), EVEN IF GROQ HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\nTO THE FULLEST EXTENT PERMITTED BY APPLICABLE LAW, GROQ IS NOT RESPONSIBLE OR LIABLE WHATSOEVER IN ANY MANNER FOR ANY CONTENT POSTED ON OR AVAILABLE THROUGH THE WEBSITES OR SERVICES (INCLUDING CLAIMS OF INFRINGEMENT RELATING TO THAT CONTENT), FOR YOUR USE OF THE WEBSITES OR SERVICES, OR FOR THE CONDUCT OF THIRD PARTIES ON OR THROUGH THE WEBSITES OR SERVICES.\n\n**9\\. Indemnification**\n\nTo the extent authorized by law, you agree to indemnify and hold harmless Groq, its employees, officers, directors, affiliates, and agents (“Indemnified Parties”) from and against any and all claims, losses, expenses, damages, and costs, including reasonable attorneys’ fees, incurred by the Indemnified Parties in connection with any claim arising out of your breach of these Terms.\n\n**10\\. Privacy Policy**\n\nGroq is committed to responsibly handling the information and data we collect through our Services in compliance with our Privacy Policy, which is incorporated by reference into these Terms. Please review Groq’s [Privacy Policy](https://console.groq.com/docs/privacy-policy-ksa) for the Kingdom of Saudi Arabia so you are aware of how we collect and use your personal information. You acknowledge and agree that Groq has the right to track and analyze anonymized statistics regarding your access to the Services for purposes of improving Groq products and services and such statistics will not be considered your content and cannot be considered personal data under the applicable data protection law.\n\n**11\\. Termination**\n\nWe may, in our sole discretion, (i) modify, suspend, or terminate your access to any or all of the Website, or (iii) cease to provide and maintain the Website, at any time, for any or no reason, with or without prior notice, and without liability. Your right to access and use the Websites and Services will be automatically terminated if you violate any of the Terms.\n\nThe disclaimer of warranties, the limitation of liability, and the jurisdiction and applicable law provisions will survive any termination. Your warranties and indemnification obligations will survive for one year after termination.\n\n**12\\. Miscellaneous Terms**\n\nYou may not use or material in violation of US export laws and regulations.\n\nThe Terms (including the existence, breach, validity, interpretation, performance or termination of these Terms or any non-contractual obligation arising out of or relating to these Terms) shall be governed and construed by Saudi law.\n\nAny dispute, controversy or claim arising out of or relating to these Terms, including any dispute relating to the breach, existence, validity, performance, interpretation or termination of these Terms or any non-contractual obligation arising out of or relating to these Terms, shall be referred to and finally resolved by arbitration. The arbitration shall be administered by the Saudi Center for Commercial Arbitration (SCCA) in accordance with its Arbitration Rules (\" **Rules**\"). There shall be three (3) arbitrators, appointed in accordance with the Rules. The seat, or legal place, of arbitration shall be the Saudi Center for Commercial Arbitration in Riyadh, Kingdom of Saudi Arabia, and the arbitration shall be conducted in the English language.\n\nIf any provision of these Terms is found to be invalid or unenforceable, that provision will be struck and the remaining provisions will remain in full effect.\n\nIf you or others violate these Terms and we take no immediate action, this in no way limits or waives our rights, such as our right to take action in the future or in similar situations.",
    "metadata": {
      "url": "https://console.groq.com/docs/terms-of-use-ksa",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "7e2328d3-e24b-4a50-b97e-e58a49670658",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/terms-of-use-ksa",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Deprecation\n\nDeprecation refers to the process of retiring older models or endpoints in favor of hosting better models with better capabilities for you to\nleverage. When we announce that a model or endpoint is being deprecated, we will provide a shutdown date on which the model or endpoint\nwill no longer be accessible. As such, your applications relying on Groq may need occasional updates to continue working.\n\nOnce a model is announced as deprecated, make sure to migrate usage to a recommended replacement before the shutdown date to avoid failing\nrequests. All API deprecations along with recommended replacements are listed below.\n\n## Deprecation History\n\n### [January 24, 2025: Llama 3.1 70B and Llama 3.1 70B (Speculative Decoding)](https://console.groq.com/docs/deprecations\\#january-24-2025-llama-31-70b-and-llama-31-70b-speculative-decoding)\n\nOn December 6, 2024, in partnership with Meta, we released `llama-3.3-70b-versatile` and `llama-3.3-70b-specdec`, and notified users that we would deprecate their 3.1 counterparts in favor of hosting Llama 3.3\nwith significant quality improvements for a better experience.\n\nTo facilitate a smooth transition, we will maintain the current `llama-3.1-70b-versatile` and `llama-3.1-70b-specdec` model IDs until December 20, 2024. At that time, requests to these model IDs will automatically\nupgrade to their respective 3.3 versions. Beginning January 24, 2025, requests to both 3.1 model IDs will return errors.\n\nWhile these new models deliver improved quality, they may produce different responses than their predecessors. We recommend migrating to explicitly using `llama-3.3-70b-versatile` and `llama-3.3-70b-specdec` before\nDecember 20, 2024, for testing.\n\n| Model ID | Shutdown Date | Recommended Replacement Model ID |\n| --- | --- | --- |\n| `llama-3.1-70b-versatile` | 01/24/25 | `llama-3.3-70b-versatile` |\n| `llama-3.1-70b-specdec` | 01/24/25 | `llama-3.3-70b-specdec` |\n\n### [January 6, 2025: Llama 3 Groq Tool Use Models](https://console.groq.com/docs/deprecations\\#january-6-2025-llama-3-groq-tool-use-models)\n\nOn January 6th, we deprecated our preview versions of Llama 3 fine-tuned for tool use, `llama3-groq-8b-8192-tool-use-preview` and `llama3-groq-70b-8192-tool-use-preview`, from GroqCloud™ in favor of\ntransitioning users to our production-ready `llama-3.30-70b-versatile` model.\n\nUsers of the tool use models were notified about the upcoming deprecation via email. The recommended replacement model, `llama-3.3-70b-versatile`, offers superior tool use capabilities and we strongly encourage\nusers to migrate applications to this model for improved reliability and performance.\n\n| Model ID | Shutdown Date | Recommended Replacement Model ID |\n| --- | --- | --- |\n| `llama3-groq-8b-8192-tool-use-preview` | 1/6/25 | `llama-3.3-70b-versatile` |\n| `llama3-groq-70b-8192-tool-use-preview` | 1/6/25 | `llama-3.3-70b-versatile` |\n\n### [December 18, 2024: Gemma 7B](https://console.groq.com/docs/deprecations\\#december-18-2024-gemma-7b)\n\nOn December 11, 2024, we emailed all Gemma 7B users that we would deprecate it in favor of keeping the Gemma 9B model as it offers better performance.\n\n| Model ID | Shutdown Date | Recommended Replacement Model ID |\n| --- | --- | --- |\n| `gemma-7b-it` | 12/18/24 | `gemma2-9b-it` |\n\n### [November 25, 2024: Llama 3.2 90B Text Preview](https://console.groq.com/docs/deprecations\\#november-25-2024-llama-32-90b-text-preview)\n\nIn November 2024, we emailed all Llama 3.2 90B Text Preview users that we would deprecate it in favor of hosting the Llama 3.2 90B Vision Preview\nmodel for vision capabilities.\n\n| Model ID | Shutdown Date | Recommended Replacement Model ID |\n| --- | --- | --- |\n| `llama-3.2-90b-text-preview` | 11/25/24 | `llama-3.2-90b-vision-preview` `llama-3.1-70b-versatile` (text-only workloads) |\n\n### [October 18, 2024: LLaVA 1.5 7B and Llama 3.2 11B Text Preview](https://console.groq.com/docs/deprecations\\#october-18-2024-llava-15-7b-and-llama-32-11b-text-preview)\n\nIn September 2024, we made Meta's Llama 3.2 vision models available on GroqCloud and emailed all LLaVA 1.5 7B and Llama 3.2 11B Text Preview\nusers that we would deprecate it in favor of hosting Llama 3.2 11B Vision for better performance and more robust vision capabilities.\n\n| Model ID | Shutdown Date | Recommended Replacement Model ID |\n| --- | --- | --- |\n| `llava-v1.5-7b-4096-preview` | 10/28/24 | `llama-3.2-11b-vision-preview` |\n| `llama-3.2-11b-text-preview` | 10/28/24 | `llama-3.2-11b-vision-preview` `llama-3.1-8b-instant` (text-only workloads) |",
    "metadata": {
      "url": "https://console.groq.com/docs/deprecations",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "414bd2c9-bcd1-475e-bdc5-501fc9d79774",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/deprecations",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## JigsawStack 🧩\n\n[JigsawStack](https://jigsawstack.com/) is a powerful AI SDK designed to integrate into any backend, automating tasks such as web scraping, Optical Character Recognition (OCR), translation, and more, using\nLarge Language Models (LLMs). By plugging JigsawStack into your existing application infrastructure, you can offload the heavy lifting and focus on building.\n\nThe [JigsawStack Prompt Engine](https://console.groq.com/docs/jigsawstack#) is a feature that allows you to not only leverage LLMs but automatically choose the best LLM for every one of your prompts, delivering lightning-fast inference speed and performance\npowered by Groq with features including:\n\n- **Mixture-of-Agents (MoA) Approach:** Automatically selects the best LLMs for your task, combining outputs for higher quality and faster results.\n- **Prompt Caching:** Optimizes performance for repeated prompt runs.\n- **Automatic Prompt Optimization:** Improves performance without manual intervention.\n- **Response Schema Validation:** Ensures accuracy and consistency in outputs.\n\nThe Propt Engine also comes with a built-in prompt guard feature via Llama Guard 3 powered by Groq, which helps prevent prompt injection and a wide range of unsafe categories when activated, such as:\n\n- Privacy Protection\n- Hate Speech Filtering\n- Sexual Content Blocking\n- Election Misinformation Prevention\n- Code Interpreter Abuse Protection\n- Unauthorized Professional Advice Prevention\n\nTo get started, refer to the JigsawStack documentation [here](https://docs.jigsawstack.com/integration/groq) and learn how to set up your Prompt\nEngine [here](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/jigsawstack-prompt-engine).",
    "metadata": {
      "url": "https://console.groq.com/docs/jigsawstack",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "aa34e148-991b-4de8-b2b3-1692ed7d0d84",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/jigsawstack",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## AutoGen + Groq: Building Multi-Agent AI Applications\n\n[AutoGen](https://microsoft.github.io/autogen/) developed by [Microsoft Research](https://www.microsoft.com/research/) is an open-source framework for building multi-agent AI applications. By powering the\nAutoGen agentic framework with Groq's fast inference speed, you can create sophisticated AI agents that work together to solve complex tasks fast with features including:\n\n- **Multi-Agent Orchestration:** Create and manage multiple agents that can collaborate in realtime\n- **Tool Integration:** Easily connect agents with external tools and APIs\n- **Flexible Workflows:** Support both autonomous and human-in-the-loop conversation patterns\n- **Code Generation & Execution:** Enable agents to write, review, and execute code safely\n\n### [Python Quick Start (3 minutes to hello world)](https://console.groq.com/docs/autogen\\#python-quick-start-3-minutes-to-hello-world)\n\n#### 1\\. Install the required packages:\n\n```bash\npip install autogen-agentchat~=0.2 groq\n```\n\n#### 2\\. Configure your Groq API key:\n\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\n```\n\n#### 3\\. Create your first multi-agent application with Groq:\n\nIn AutoGen, **agents** are autonomous entities that can engage in conversations and perform tasks. The example below shows how to create a simple two-agent system with `llama-3.3-70b-versatile` where\n`UserProxyAgent` initiates the conversation with a question and `AssistantAgent` responds:\n\n```python\nimport os\nfrom autogen import AssistantAgent, UserProxyAgent\n\n# Configure\nconfig_list = [{\\\n    \"model\": \"llama-3.3-70b-versatile\",\\\n    \"api_key\": os.environ.get(\"GROQ_API_KEY\"),\\\n    \"api_type\": \"groq\"\\\n}]\n\n# Create an AI assistant\nassistant = AssistantAgent(\n    name=\"groq_assistant\",\n    system_message=\"You are a helpful AI assistant.\",\n    llm_config={\"config_list\": config_list}\n)\n\n# Create a user proxy agent (no code execution in this example)\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    code_execution_config=False\n)\n\n# Start a conversation between the agents\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"What are the key benefits of using Groq for AI apps?\"\n)\n```\n\n### [Advanced Features](https://console.groq.com/docs/autogen\\#advanced-features)\n\n#### Code Generation and Execution\n\nYou can enable secure code execution by configuring the `UserProxyAgent` that allows your agents to write and execute Python code in a controlled environment:\n\n```python\nfrom pathlib import Path\nfrom autogen.coding import LocalCommandLineCodeExecutor\n\n# Create a directory to store code files\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\ncode_executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n\n# Configure the UserProxyAgent with code execution\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    code_execution_config={\"executor\": code_executor}\n)\n```\n\n#### Tool Integration\n\nYou can add tools for your agents to use by creating a function and registering it with the assistant. Here's an example of a weather forecast tool:\n\n```python\nfrom typing import Annotated\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the weather for some location\"\"\"\n    weather_data = {\n        \"berlin\": {\"temperature\": \"13\"},\n        \"istanbul\": {\"temperature\": \"40\"},\n        \"san francisco\": {\"temperature\": \"55\"}\n    }\n\n    location_lower = location.lower()\n    if location_lower in weather_data:\n        return json.dumps({\n            \"location\": location.title(),\n            \"temperature\": weather_data[location_lower][\"temperature\"],\n            \"unit\": unit\n        })\n    return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\n# Register the tool with the assistant\n@assistant.register_for_llm(description=\"Weather forecast for cities.\")\ndef weather_forecast(\n    location: Annotated[str, \"City name\"],\n    unit: Annotated[str, \"Temperature unit (fahrenheit/celsius)\"] = \"fahrenheit\"\n) -> str:\n    weather_details = get_current_weather(location=location, unit=unit)\n    weather = json.loads(weather_details)\n    return f\"{weather['location']} will be {weather['temperature']} degrees {weather['unit']}\"\n```\n\n#### Complete Code Example\n\nHere is our quick start agent code example combined with code execution and tool use that you can play with:\n\n```python\nimport os\nimport json\nfrom pathlib import Path\nfrom typing import Annotated\nfrom autogen import AssistantAgent, UserProxyAgent\nfrom autogen.coding import LocalCommandLineCodeExecutor\n\n# Configure Groq\nconfig_list = [{\\\n    \"model\": \"llama-3.3-70b-versatile\",\\\n    \"api_key\": os.environ.get(\"GROQ_API_KEY\"),\\\n    \"api_type\": \"groq\"\\\n}]\n\n# Create a directory to store code files from code executor\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\ncode_executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n\n# Define weather tool\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the weather for some location\"\"\"\n    weather_data = {\n        \"berlin\": {\"temperature\": \"13\"},\n        \"istanbul\": {\"temperature\": \"40\"},\n        \"san francisco\": {\"temperature\": \"55\"}\n    }\n\n    location_lower = location.lower()\n    if location_lower in weather_data:\n        return json.dumps({\n            \"location\": location.title(),\n            \"temperature\": weather_data[location_lower][\"temperature\"],\n            \"unit\": unit\n        })\n    return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\n# Create an AI assistant that uses the weather tool\nassistant = AssistantAgent(\n    name=\"groq_assistant\",\n    system_message=\"\"\"You are a helpful AI assistant who can:\n    - Use weather information tools\n    - Write Python code for data visualization\n    - Analyze and explain results\"\"\",\n    llm_config={\"config_list\": config_list}\n)\n\n# Register weather tool with the assistant\n@assistant.register_for_llm(description=\"Weather forecast for cities.\")\ndef weather_forecast(\n    location: Annotated[str, \"City name\"],\n    unit: Annotated[str, \"Temperature unit (fahrenheit/celsius)\"] = \"fahrenheit\"\n) -> str:\n    weather_details = get_current_weather(location=location, unit=unit)\n    weather = json.loads(weather_details)\n    return f\"{weather['location']} will be {weather['temperature']} degrees {weather['unit']}\"\n\n# Create a user proxy agent that only handles code execution\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    code_execution_config={\"executor\": code_executor}\n)\n\n# Start the conversation\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"\"\"Let's do two things:\n    1. Get the weather for Berlin, Istanbul, and San Francisco\n    2. Write a Python script to create a bar chart comparing their temperatures\"\"\"\n)\n```\n\n**Challenge:** Add to the above example and create a multi-agent [`GroupChat`](https://microsoft.github.io/autogen/0.2/docs/topics/groupchat/customized_speaker_selection) workflow!\n\nFor more detailed documentation and resources on building agentic applications with Groq and AutoGen, see:\n\n- [AutoGen Documentation](https://microsoft.github.io/autogen/0.2/docs/topics/non-openai-models/cloud-groq/)\n- [AutoGroq](https://github.com/jgravelle/AutoGroq)",
    "metadata": {
      "url": "https://console.groq.com/docs/autogen",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "29fd71fb-38ae-4194-b454-f4b87d3c3412",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/autogen",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## 🚅 LiteLLM + Groq for Production Deployments\n\n[LiteLLM](https://docs.litellm.ai/docs/) provides a simple framework with features to help productionize your application infrastructure, including:\n\n- **Cost Management:** Track spending, set budgets, and implement rate limiting for optimal resource utilization\n- **Smart Caching:** Cache frequent responses to reduce API calls while maintaining Groq's speed advantage\n- **Spend Tracking:** Track spend for individual API keys, users, and teams\n\n### [Quick Start (2 minutes to hello world)](https://console.groq.com/docs/litellm\\#quick-start-2-minutes-to-hello-world)\n\n#### 1\\. Install the package:\n\n```bash\npip install litellm\n```\n\n#### 2\\. Set up your API key:\n\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\n```\n\n#### 3\\. Send your first request:\n\n```python\nimport os\nimport litellm\n\napi_key = os.environ.get('GROQ_API_KEY')\n\nresponse = litellm.completion(\n    model=\"groq/llama-3.3-70b-versatile\",\n    messages=[\\\n       {\"role\": \"user\", \"content\": \"hello from litellm\"}\\\n   ],\n)\nprint(response)\n```\n\n### [Next Steps](https://console.groq.com/docs/litellm\\#next-steps)\n\nFor detailed setup of advanced features:\n\n- [Configuration of Spend Tracking for Keys, Users, and Teams](https://docs.litellm.ai/docs/proxy/cost_tracking)\n- [Configuration for Budgets and Rate Limits](https://docs.litellm.ai/docs/proxy/users)\n\nFor more information on building production-ready applications with LiteLLM and Groq, see:\n\n- [Official Documentation: LiteLLM](https://docs.litellm.ai/docs/providers/groq)\n- [Tutorial: Groq API Cookbook](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/litellm-proxy-groq)",
    "metadata": {
      "url": "https://console.groq.com/docs/litellm",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "af71fddf-605c-49a9-bde2-9c6109fb1bcd",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/litellm",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Rate Limits\n\nRate limits act as control measures to regulate how frequently users and applications can access our API within specified timeframes. These limits help ensure service stability, fair access, and protection\nagainst misuse so that we can serve reliable and fast inference for all.\n\n### [Understanding Rate Limits](https://console.groq.com/docs/rate-limits\\#understanding-rate-limits)\n\nRate limits are measured in:\n\n- **RPM:** Requests per minute\n- **RPD:** Requests per day\n- **TPM:** Tokens per minute\n- **TPD:** Tokens per day\n\nRate limits apply at the organization level, not individual users. You can hit any limit type depemding on which threshold you reach first.\n\n**Example:** Let's say your RPM = 50 and your TPM = 200K. If you were to send 50 requests with only 100 tokens within a minute, you would reach your limit even though you did not send 200K tokens within those\n50 requests.\n\n## Rate Limits\n\nThe following is a high level summary and there may be exceptions to these limits. You can view the current, exact rate limits for your organization on the [limits page](https://console.groq.com/settings/limits) in your account settings.\n\nFree TierDeveloper Tier\n\n| MODEL ID | RPM | RPD | TPM | TPD | ASH | ASD |\n| --- | --- | --- | --- | --- | --- | --- |\n| deepseek-r1-distill-llama-70b | 30 | 1,000 | 6,000 | - | - | - |\n| llama-3.3-70b-versatile | 30 | 1,000 | 6,000 | 100,000 | - | - |\n| llama-3.3-70b-specdec | 30 | 1,000 | 6,000 | 100,000 | - | - |\n| llama-3.2-1b-preview | 30 | 7,000 | 7,000 | 500,000 | - | - |\n| llama-3.2-3b-preview | 30 | 7,000 | 7,000 | 500,000 | - | - |\n| llama-3.1-8b-instant | 30 | 14,400 | 6,000 | 500,000 | - | - |\n| llama3-70b-8192 | 30 | 14,400 | 6,000 | 500,000 | - | - |\n| llama3-8b-8192 | 30 | 14,400 | 6,000 | 500,000 | - | - |\n| llama-guard-3-8b | 30 | 14,400 | 15,000 | 500,000 | - | - |\n| mixtral-8x7b-32768 | 30 | 14,400 | 5,000 | 500,000 | - | - |\n| gemma2-9b-it | 30 | 14,400 | 15,000 | 500,000 | - | - |\n| whisper-large-v3 | 20 | 2,000 | - | - | 7,200 | 28,800 |\n| whisper-large-v3-turbo | 20 | 2,000 | - | - | 7,200 | 28,800 |\n| distil-whisper-large-v3-en | 20 | 2,000 | - | - | 7,200 | 28,800 |\n| llama-3.2-11b-vision-preview | 30 | 7,000 | 7,000 | 500,000 | - | - |\n| llama-3.2-90b-vision-preview | 15 | 3,500 | 7,000 | 250,000 | - | - |\n\n## Rate Limit Headers\n\nIn addition to viewing your limits on your account's [limits](https://console.groq.com/settings/limits) page, you can also view rate limit information such as remaining requests and tokens in HTTP response\nheaders as follows:\n\nThe following headers are set (values are illustrative):\n\n| Header | Value | Notes |\n| --- | --- | --- |\n| retry-after | 2 | In seconds |\n| x-ratelimit-limit-requests | 14400 | Always refers to Requests Per Day (RPD) |\n| x-ratelimit-limit-tokens | 18000 | Always refers to Tokens Per Minute (TPM) |\n| x-ratelimit-remaining-requests | 14370 | Always refers to Requests Per Day (RPD) |\n| x-ratelimit-remaining-tokens | 17997 | Always refers to Tokens Per Minute (TPM) |\n| x-ratelimit-reset-requests | 2m59.56s | Always refers to Requests Per Day (RPD) |\n| x-ratelimit-reset-tokens | 7.66s | Always refers to Tokens Per Minute (TPM) |\n\n## Handling Rate Limits\n\nWhen you exceed rate limits, our API returns a `429 Too Many Requests` HTTP status code.\n\n**Note**: `retry-after` is only set if you hit the rate limit and status code 429 is returned. The other headers are always included.",
    "metadata": {
      "url": "https://console.groq.com/docs/rate-limits",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "d6a03b51-e855-4789-87ed-8e3636f3539f",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/rate-limits",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)\n\n[iframe](https://js.stripe.com/v3/controller-with-preconnect-98d4b3f9bf9c2357b309c56d1cb07205.html#apiKey=pk_live_51Of9kmEnzmVEg9qP5bfEuBfP8Rq9sHTKFWvsH14oyyTVB6GX3FkOdWJvFpMrtvwbxP6Wg5ZbhFYyIUMpS61vbzla00hbxM1pUA&stripeJsId=4857ae8b-cab8-493e-aa0b-51e234158e8b&controllerCount=1&isCheckout=false&stripeJsLoadTime=1740367155018&manualBrowserDeprecationRollout=false&mids[guid]=NA&mids[muid]=NA&mids[sid]=NA&referrer=https%3A%2F%2Fconsole.groq.com%2Fsettings%2Fbilling&controllerId=__privateStripeController6951)",
    "metadata": {
      "url": "https://console.groq.com/settings/billing",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "df0b5e63-2eb6-4808-b814-77c99d3d6121",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/settings/billing",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Quickstart\n\nGet up and running with the Groq API in a few minutes.\n\n### [Create an API Key](https://console.groq.com/docs/quickstart\\#create-an-api-key)\n\nPlease visit [here](https://console.groq.com/keys) to create an API Key.\n\n### [Set up your API Key (recommended)](https://console.groq.com/docs/quickstart\\#set-up-your-api-key-recommended)\n\nConfigure your API key as an environment variable. This approach streamlines your API usage by eliminating the need to include your API key in each request. Moreover, it enhances security by minimizing the risk of inadvertently including your API key in your codebase.\n\n#### In your terminal of choice:\n\n```shell\nexport GROQ_API_KEY=<your-api-key-here>\n```\n\n### [Requesting your first chat completion](https://console.groq.com/docs/quickstart\\#requesting-your-first-chat-completion)\n\ncurlJavaScriptPythonJSON\n\n#### Install the Groq Python library:\n\n```shell\npip install groq\n```\n\n#### Performing a Chat Completion:\n\n```py\n1import os\n2\n3from groq import Groq\n4\n5client = Groq(\n6    api_key=os.environ.get(\"GROQ_API_KEY\"),\n7)\n8\n9chat_completion = client.chat.completions.create(\n10    messages=[\\\n11        {\\\n12            \"role\": \"user\",\\\n13            \"content\": \"Explain the importance of fast language models\",\\\n14        }\\\n15    ],\n16    model=\"llama-3.3-70b-versatile\",\n17)\n18\n19print(chat_completion.choices[0].message.content)\n```\n\nNow that you have successfully received a chat completion, you can try out the other endpoints in the API.\n\n### [Next Steps](https://console.groq.com/docs/quickstart\\#next-steps)\n\n- Check out the [Playground](https://console.groq.com/playground) to try out the Groq API in your browser\n- Join our GroqCloud developer community on [Discord](https://discord.gg/groq)\n- [Chat with our Docs](https://docs-chat.groqcloud.com/) at lightning speed using the Groq API!\n- Add a how-to on your project to the [Groq API Cookbook](https://github.com/groq/groq-api-cookbook)",
    "metadata": {
      "url": "https://console.groq.com/docs/quickstart",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "cf961ab5-b021-4805-89d7-edee692df8b5",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/quickstart",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Toolhouse 🛠️🏠\n\n[Toolhouse](https://app.toolhouse.ai/) is the first complete infrastructure for tool use. With Toolhouse, you can equip your LLM with tools like Code Interpreter, Web Search, and Email tools, among others.\nThis equips your LLMs with the ability to search the web, send the emails they write, or run the code they generate, without the need for your to code or prompt these tools. These tools can be used across any\nLLM supported by Groq.\n\n### [Python Quick Start (3 minutes to hello world)](https://console.groq.com/docs/toolhouse\\#python-quick-start-3-minutes-to-hello-world)\n\n#### 1\\. Install Required Libraries\n\n```bash\npip install toolhouse groq\n```\n\n#### 2\\. Configure your API keys:\n\n**Note:** You can get your free Toolhouse API key [here](https://app.toolhouse.ai/settings/api-keys)!\n\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\nexport TOOLHOUSE_API_KEY=\"your-toolhouse-api-key\"\n```\n\n#### 3\\. Initialize the Groq and Toolhouse clients:\n\n```python\nimport os\nfrom toolhouse import Toolhouse\nfrom groq import Groq\n\nclient = Groq(api_key=os.environ.get('GROQ_API_KEY'))\nth = Toolhouse(api_key=os.environ.get('TOOLHOUSE_API_KEY'))\n```\n\n#### 4\\. Define and register a custom, local tool to perform arithmetic operations:\n\n- `@th.register_local_tool`: Registers a local function as a tool accessible via Toolhouse.\n- The `calculate` function takes an operation ( `add`, `subtract`, `multiply`, or `divide`) and two numbers ( `x` and `y`) as input and performs the operation.\n\n```python\n@th.register_local_tool(\"calculate\")\n@th.register_local_tool(\"calculate\")\ndef calculate(operation: str, x: float, y: float) -> str:\n    operations = {\n        \"add\": lambda: x + y,\n        \"subtract\": lambda: x - y,\n        \"multiply\": lambda: x * y,\n        \"divide\": lambda: x / y if y != 0 else \"Error: Cannot divide by zero\"\n    }\n    if operation not in operations:\n        return f\"Error: Invalid operation. Please use add, subtract, multiply, or divide.\"\n    result = operations[operation]()\n    return f\"The result of {x} {operation} {y} is {result}\"\n```\n\n#### 5\\. Clearly specify our tool definition for our LLM to be able understand what parameters are required along with their expected types to be able to use correctly when needed:\n\n```python\nmy_local_tools = [{\\\n    \"type\": \"function\",\\\n    \"function\": {\\\n        \"name\": \"calculate\",\\\n        \"description\": \"This tool can be used to perform basic arithmetic operations on two numbers\",\\\n        \"parameters\": {\\\n            \"type\": \"object\",\\\n            \"properties\": {\\\n                \"operation\": {\\\n                    \"type\": \"string\",\\\n                    \"description\": \"The arithmetic operation to perform (add, subtract, multiply, or divide)\",\\\n                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]\\\n                },\\\n                \"x\": {\\\n                    \"type\": \"number\",\\\n                    \"description\": \"The first number\"\\\n                },\\\n                \"y\": {\\\n                    \"type\": \"number\",\\\n                    \"description\": \"The second number\"\\\n                }\\\n            },\\\n            \"required\": [\"operation\", \"x\", \"y\"]\\\n        }\\\n    }\\\n}]\n```\n\n**Challenge**: Update the code to add other custom, local tools and [tools from Toolhouse](https://docs.toolhouse.ai/toolhouse/how-to-leverage-tools)!\n\n#### 6\\. Set up our conversation using multiple tool calls:\n\n`role` specifies the speaker (system, user, or assistant) and `content` provides our query. Here, we first send our query to Groq API and retrieve any required tool calls. The first call identifies tool usage\nbased on the user query and `th.run_tools(response)` executes the tools selected by the LLM. The second Groq API call incorporates the tool’s output to refine the response before printing the end result!\n\n```python\nmessages = [\\\n    {\\\n        \"role\": \"user\",\\\n        \"content\": \"perform 1024 + 1024. Then using the scraper, scrape https://console.groq.com/docs/changelog and tell me which is the latest model added\"\\\n    }\\\n]\n\n# First call - Get the tool function call\nresponse = client.chat.completions.create(\n    model=\"llama-3.3-70b-versatile\",\n    messages=messages,\n    tools=th.get_tools() + my_local_tools  # Combine local and cloud tools\n)\n\nprint(\"\\n****** Tools Used in Response ******\")\nif hasattr(response.choices[0].message, 'tool_calls') and response.choices[0].message.tool_calls:\n    idx = 1\n    for tool_call in response.choices[0].message.tool_calls:\n        print(f\"DEBUG MSG {idx}: {response.choices[0]}\")\n        print(f\"Tool {idx} used: {tool_call.function.name}\")\n        print(f\"Arguments {idx}: {tool_call.function.arguments}\\n\")\n\n        # Execute the tool\n        tool_run = th.run_tools(response)\n        messages.extend(tool_run)\n\n        # Second call - Get the final response\n        response = client.chat.completions.create(\n            model=\"llama-3.3-70b-versatile\",\n            messages=messages,\n            tools=th.get_tools() + my_local_tools\n        )\n        print(f\"DEBUG RESPONSE {idx}: {response.choices[0].message.content}\")\n        idx += 1\nelse:\n    print(\"No tools were used in this response\")\nprint(\"FINAL RESPONSE\", response.choices[0].message.content)\n```\n\n### [Additional Resources](https://console.groq.com/docs/toolhouse\\#additional-resources)\n\nFor more robust documentation and further resources, including using Toolhouse with Groq building agentic workflows, see the following:\n\n- [Toolhouse Documentation](https://docs.toolhouse.ai/toolhouse)\n- [Tutorial: Toolhouse with Groq API](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/toolhouse-for-tool-use-with-groq-api/Groq%20%3C%3E%20Toolhouse.ipynb)\n- [Webinar: Overcoming the Challenges of Building Agentic AI](https://youtu.be/bazR8dJzpI0?si=v9Fz_j5b9kTFIPQJ)",
    "metadata": {
      "url": "https://console.groq.com/docs/toolhouse",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "b127ea45-e9e2-4149-8881-01d51443705f",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/toolhouse",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## 🦜️🔗 LangChain + Groq\n\nWhile you could use the Groq SDK directly, [LangChain](https://www.langchain.com/) is a framework that makes it easy to build sophisticated applications\nwith LLMs. Combined with Groq API for fast inference speed, you can leverage LangChain components such as:\n\n- **Chains:** Compose multiple operations into a single workflow, connecting LLM calls, prompts, and tools together seamlessly (e.g., prompt → LLM → output parser)\n- **Prompt Templates:** Easily manage your prompts and templates with pre-built structures to consisently format queries that can be reused across different models\n- **Memory:** Add state to your applications by storing and retrieving conversation history and context\n- **Tools:** Extend your LLM applications with external capabilities like calculations, external APIs, or data retrievals\n- **Agents:** Create autonomous systems that can decide which tools to use and how to approach complex tasks\n\n### [Quick Start (3 minutes to hello world)](https://console.groq.com/docs/langchain\\#quick-start-3-minutes-to-hello-world)\n\n#### 1\\. Install the package:\n\n```bash\npip install langchain-groq\n```\n\n#### 2\\. Set up your API key:\n\n```bash\nexport GROQ_API_KEY=\"your-groq-api-key\"\n```\n\n#### 3\\. Create your first LangChain assistant:\n\nRunning the below code will create a simple chain that calls a model to extract product information from text and output it\nas structured JSON. The chain combines a prompt that tells the model what information to extract, a parser that ensures the output follows a\nspecific JSON format, and `llama-3.3-70b-versatile` to do the actual text processing.\n\n```python\nfrom langchain_groq import ChatGroq\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\nimport json\n\n# Initialize Groq LLM\nllm = ChatGroq(\n    model_name=\"llama-3.3-70b-versatile\",\n    temperature=0.7\n)\n\n# Define the expected JSON structure\nparser = JsonOutputParser(pydantic_object={\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"price\": {\"type\": \"number\"},\n        \"features\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"}\n        }\n    }\n})\n\n# Create a simple prompt\nprompt = ChatPromptTemplate.from_messages([\\\n    (\"system\", \"\"\"Extract product details into JSON with this structure:\\\n        {{\\\n            \"name\": \"product name here\",\\\n            \"price\": number_here_without_currency_symbol,\\\n            \"features\": [\"feature1\", \"feature2\", \"feature3\"]\\\n        }}\"\"\"),\\\n    (\"user\", \"{input}\")\\\n])\n\n# Create the chain that guarantees JSON output\nchain = prompt | llm | parser\n\ndef parse_product(description: str) -> dict:\n    result = chain.invoke({\"input\": description})\n    print(json.dumps(result, indent=2))\n\n\n# Example usage\ndescription = \"\"\"The Kees Van Der Westen Speedster is a high-end, single-group espresso machine known for its precision, performance,\nand industrial design. Handcrafted in the Netherlands, it features dual boilers for brewing and steaming, PID temperature control for\nconsistency, and a unique pre-infusion system to enhance flavor extraction. Designed for enthusiasts and professionals, it offers\ncustomizable aesthetics, exceptional thermal stability, and intuitive operation via a lever system. The pricing is approximatelyt $14,499\ndepending on the retailer and customization options.\"\"\"\n\nparse_product(description)\n```\n\n**Challenge:** Make the above code your own! Try extending it to include memory with conversation history handling via LangChain to enable\nusers to ask follow-up questions.\n\nFor more information on how to build robust, realtime applications with LangChain and Groq, see:\n\n- [Official Documentation: LangChain](https://python.langchain.com/docs/integrations/chat/groq)\n- [Groq API Cookbook: Benchmarking a RAG Pipeline with LangChain and LLama](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/benchmarking-rag-langchain/benchmarking_rag.ipynb)\n- [Webinar: Build Blazing-Fast LLM Apps with Groq, Langflow, & LangChain](https://youtu.be/4ukqsKajWnk?si=ebbbnFH0DySdoWbX)",
    "metadata": {
      "url": "https://console.groq.com/docs/langchain",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "f9e16ecc-f7fd-41d1-a82c-d628506a21f2",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/langchain",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# 404\n\n## This page could not be found.",
    "metadata": {
      "url": "https://console.groq.com/docs/docs/models",
      "error": "Not Found",
      "ogUrl": "https://console.groq.com",
      "title": "404: This page could not be found.",
      "og:url": "https://console.groq.com",
      "robots": "noindex",
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "85ed7431-2044-4e0f-ada3-ff272b3a0af7",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/docs/models",
      "statusCode": 404,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Llama-Guard-3-8B\n\n[‹ Back to models page](https://console.groq.com/docs/models)\n\n![Meta Logo](https://console.groq.com/Meta_logo.png)\n\nLlama-Guard-3-8B is Meta's specialized content moderation model designed to identify and classify potentially harmful content. Fine-tuned specifically for content safety, this model analyzes both user inputs and AI-generated outputs using [categories based on the MLCommons Taxonomy framework](https://console.groq.com/docs/content-moderation). The model delivers efficient, consistent content screening while maintaining transparency in its classification decisions.\n\n[Try now on Groq](https://chat.groq.com/?model=llama-guard-3-8b)\n\n### [Key Technical Specifications](https://console.groq.com/docs/model/llama-guard-3-8b\\#key-technical-specifications)\n\n### Model Architecture\n\nBuilt upon Meta's Llama architecture, the model is comprised of 3.8 billion parameters and is specifically fine-tuned for content moderation and safety classification tasks.\n\n### Performance Metrics\n\nThe model demonstrates strong performance in content moderation tasks:\n\n- High accuracy in identifying harmful content\n- Low false positive rate for safe content\n- Efficient processing of large-scale content\n\n### Technical Details\n\n| FEATURE | VALUE |\n| --- | --- |\n| Context Window (Tokens) | 8,192 |\n| Max Output Tokens | - |\n| Max File Size | - |\n| Token Generation Speed (as of 2025-01-28) | 765 tps |\n| Input Token Price | $0.2/1M tokens |\n| Output Token Price | $0.2/1M tokens |\n| Tool Use | ![Not Supported](https://console.groq.com/cross.svg) |\n| JSON Mode | ![Not Supported](https://console.groq.com/cross.svg) |\n| Image Support | ![Not Supported](https://console.groq.com/cross.svg) |\n\n### Use Cases\n\nContent Moderation\n\nEnsures that online interactions remain safe by filtering harmful content in chatbots, forums, and AI-powered systems.\n\n- Content filtering for online platforms and communities\n- Automated screening of user-generated content in corporate channels, forums, social media, and messaging applications\n- Proactive detection of harmful content before it reaches users\n\nAI Safety\n\nHelps LLM applications adhere to content safety policies by identifying and flagging inappropriate prompts and responses.\n\n- Pre-deployment screening of AI model outputs to ensure policy compliance\n- Real-time analysis of user prompts to prevent harmful interactions\n- Safety guardrails for chatbots and generative AI applications\n\n### Best Practices\n\n- Safety Thresholds: Configure appropriate safety thresholds based on your application's requirements\n- Context Length: Provide sufficient context for accurate content evaluation\n\n### [Get Started with Llama-Guard-3-8B](https://console.groq.com/docs/model/llama-guard-3-8b\\#get-started-with-llamaguard38b)\n\nUnlock the full potential of content moderation with Llama-Guard-3-8B - optimized for exceptional performance on Groq hardware now:\n\ncurlJavaScriptPythonJSON\n\n#### Install Groq and Perform Chat Completion Using Python:\n\n```shell\npip install groq\n```\n\n```py\n1from groq import Groq\n2client = Groq()\n3completion = client.chat.completions.create(\n4    model=\"llama-guard-3-8b\",\n5    messages=[\\\n6        {\\\n7            \"role\": \"user\",\\\n8            \"content\": \"Explain why fast inference is critical for reasoning models\"\\\n9        }\\\n10    ]\n11)\n12print(completion.choices[0].message.content)\n```",
    "metadata": {
      "url": "https://console.groq.com/docs/model/llama-guard-3-8b",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "e937a849-9190-4027-a7db-e3407664ca50",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/model/llama-guard-3-8b",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)",
    "metadata": {
      "url": "https://console.groq.com/settings/logs",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "eeb3db99-2701-4d8f-9711-a2fea859ed4c",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/settings/logs",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Examples\n\nBelow are some simple python applications you can fork and run in [Replit](https://replit.com/) to get you started building with Groq:\n\n[**Groq Quickstart Conversational Chatbot** \\\\\nA simple application that allows users to interact with a conversational chatbot powered by Groq. This application is designed to get users up and running quickly with building a chatbot.](https://replit.com/t/groqcloud/dhm5vk/repls/Groq-Quickstart-Conversational-Chatbot/view#README.md)\n\n[**Chatbot with Conversational Memory on LangChain** \\\\\nA simple application that allows users to interact with a conversational chatbot powered by LangChain. The application uses the Groq API to generate responses and maintains a history of the conversation to provide context for the chatbot's responses.](https://replit.com/@GroqCloud/Chatbot-with-Conversational-Memory-on-LangChain?v=1#README.md)\n\n[**Building a Text-to-SQL app with Groq's JSON mode** \\\\\nA command line application that allows users to ask questions about their DuckDB data. The application uses the Groq API to generate SQL queries based on the user's questions and execute them on a DuckDB database.](https://replit.com/t/groqcloud/dhm5vk/repls/Building-a-Text-to-SQL-app-with-Groqs-JSON-mode/view#README.md)\n\n[**Execute Verified SQL Queries with Function Calling** \\\\\nA command line application that allows users to ask questions about their DuckDB data using the Groq API by using function calling to execute pre-verified SQL queries.](https://replit.com/t/groqcloud/dhm5vk/repls/Execute-Verified-SQL-Queries-with-Function-Calling/view#README.md)\n\n[**CrewAI Machine Learning Assistant** \\\\\nA command line application designed to kickstart your machine learning projects. It leverages a team of AI agents to guide you through the initial steps of defining, assessing, and solving machine learning problems.](https://replit.com/t/groqcloud/dhm5vk/repls/CrewAI-Machine-Learning-Assistant/view#README.md)\n\n[**Presidential Speeches RAG with Pinecone** \\\\\nAn application that allows users to ask questions about US presidental speeches by applying Retrieval-Augmented Generation (RAG) over a Pinecone vector database.](https://replit.com/t/groqcloud/dhm5vk/repls/Presidential-Speeches-RAG-with-Pinecone/view#README.md)\n\n[**Groqing the Stock Market: Function Calling with Llama3** \\\\\nA simple application that leverages the yfinance API to provide insights into stocks and their prices. The application uses the Llama 3 model on Groq in conjunction with Langchain to call functions based on the user prompt.](https://replit.com/t/groqcloud/dhm5vk/repls/Groqing-the-Stock-Market-Function-Calling-with-Llama3/view#README.md)\n\n### [Groq API Cookbook](https://console.groq.com/docs/examples\\#groq-api-cookbook)\n\nFor more in-depth tutorials and use cases for Groq API features, check out our Cookbook [here](https://github.com/groq/groq-api-cookbook)",
    "metadata": {
      "url": "https://console.groq.com/docs/examples",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "d516529e-1071-420a-899c-953336cbf531",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/examples",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Introduction to Tool Use\n\nTool use is a powerful feature that allows Large Language Models (LLMs) to interact with external resources, such as APIs,\ndatabases, and the web, to gather dynamic data they wouldn't otherwise have access to in their pre-trained (or static) state\nand perform actions beyond simple text generation.\n\nTool use bridges the gap between the data that the LLMs were trained on with dynamic data and real-world actions, which\nopens up a wide array of realtime use cases for us to build powerful applications with, especially with Groq's insanely fast\ninference speed. 🚀\n\n### [Supported Models](https://console.groq.com/docs/tool-use\\#supported-models)\n\n| Model ID | Tool Use Support? | Parallel Tool Use Support? | JSON Mode Support? |\n| --- | --- | --- | --- |\n| `qwen-2.5-32b` | Yes | Yes | Yes |\n| `deepseek-r1-distill-qwen-32b` | Yes | Yes | Yes |\n| `deepseek-r1-distill-llama-70b` | Yes | Yes | Yes |\n| `llama-3.3-70b-versatile` | Yes | Yes | Yes |\n| `llama-3.1-8b-instant` | Yes | Yes | Yes |\n| `mixtral-8x7b-32768` | Yes | No | Yes |\n| `gemma2-9b-it` | Yes | No | Yes |\n\n## How Tool Use Works\n\nGroq API tool use structure is compatible with OpenAI's tool use structure, which allows for easy integration. See the following cURL example of a tool use request:\n\n```bash\ncurl https://api.groq.com/openai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GROQ_API_KEY\" \\\n-d '{\n  \"model\": \"llama-3.3-70b-versatile\",\n  \"messages\": [\\\n    {\\\n      \"role\": \"user\",\\\n      \"content\": \"What'\\''s the weather like in Boston today?\"\\\n    }\\\n  ],\n  \"tools\": [\\\n    {\\\n      \"type\": \"function\",\\\n      \"function\": {\\\n        \"name\": \"get_current_weather\",\\\n        \"description\": \"Get the current weather in a given location\",\\\n        \"parameters\": {\\\n          \"type\": \"object\",\\\n          \"properties\": {\\\n            \"location\": {\\\n              \"type\": \"string\",\\\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\\\n            },\\\n            \"unit\": {\\\n              \"type\": \"string\",\\\n              \"enum\": [\"celsius\", \"fahrenheit\"]\\\n            }\\\n          },\\\n          \"required\": [\"location\"]\\\n        }\\\n      }\\\n    }\\\n  ],\n  \"tool_choice\": \"auto\"\n}'\n```\n\nTo integrate tools with Groq API, follow these steps:\n\n- Provide tools (or predefined functions) to the LLM for performing actions and accessing external data in\nreal-time in addition to your user prompt within your Groq API request\n- Define how the tools should be used to teach the LLM how to use them effectively (e.g. by defining input and\noutput formats)\n- Let the LLM autonomously decide whether or not the provided tools are needed for a user query by evaluating the user\nquery, determining whether the tools can enhance its response, and utilizing the tools accordingly\n- Extract tool input, execute the tool code, and return results\n- Let the LLM use the tool result to formulate a response to the original prompt\n\nThis process allows the LLM to perform tasks such as real-time data retrieval, complex calculations, and external API\ninteraction, all while maintaining a natural conversation with our end user.\n\n## Tool Use with Groq\n\nGroq API endpoints support tool use to almost instantly deliver structured JSON output that can be used to directly invoke functions from\ndesired external resources.\n\n## Tools Specifications\n\nTool use is part of the [Groq API chat completion request payload](https://console.groq.com/docs/api-reference#chat-create).\n\n### [Tool Call and Tool Response Structure](https://console.groq.com/docs/tool-use\\#tool-call-and-tool-response-structure)\n\n**Tool Call Structure**\n\nGroq API tool calls are structured to be OpenAI-compatible. The following is an example tool call structure:\n\n```json\n{\n  \"model\": \"llama-3.3-70b-versatile\",\n  \"messages\": [\\\n    {\\\n      \"role\": \"system\",\\\n      \"content\": \"You are a weather assistant. Use the get_weather function to retrieve weather information for a given location.\"\\\n    },\\\n    {\\\n      \"role\": \"user\",\\\n      \"content\": \"What's the weather like in New York today?\"\\\n    }\\\n  ],\n  \"tools\": [\\\n    {\\\n      \"type\": \"function\",\\\n      \"function\": {\\\n        \"name\": \"get_weather\",\\\n        \"description\": \"Get the current weather for a location\",\\\n        \"parameters\": {\\\n          \"type\": \"object\",\\\n          \"properties\": {\\\n            \"location\": {\\\n              \"type\": \"string\",\\\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\\\n            },\\\n            \"unit\": {\\\n              \"type\": \"string\",\\\n              \"enum\": [\"celsius\", \"fahrenheit\"],\\\n              \"description\": \"The unit of temperature to use. Defaults to fahrenheit.\"\\\n            }\\\n          },\\\n          \"required\": [\"location\"]\\\n        }\\\n      }\\\n    }\\\n  ],\n  \"tool_choice\": \"auto\",\n  \"max_completion_tokens\": 4096\n}'\n```\n\n**Tool Call Response**\n\nThe following is an example tool call response based on the above:\n\n```json\n\"model\": \"llama-3.3-70b-versatile\",\n\"choices\": [{\\\n    \"index\": 0,\\\n    \"message\": {\\\n        \"role\": \"assistant\",\\\n        \"tool_calls\": [{\\\n            \"id\": \"call_d5wg\",\\\n            \"type\": \"function\",\\\n            \"function\": {\\\n                \"name\": \"get_weather\",\\\n                \"arguments\": \"{\\\"location\\\": \\\"New York, NY\\\"}\"\\\n            }\\\n        }]\\\n    },\\\n    \"logprobs\": null,\\\n    \"finish_reason\": \"tool_calls\"\\\n}],\n```\n\nWhen a model decides to use a tool, it returns a response with a `tool_calls` object containing:\n\n- `id`: a unique identifier for the tool call\n- `type`: the type of tool call, i.e. function\n- `name`: the name of the tool being used\n- `parameters`: an object containing the input being passed to the tool\n\n### [Setting Up Tools](https://console.groq.com/docs/tool-use\\#setting-up-tools)\n\nTo get started, let's go through an example of tool use with Groq API that you can use as a base to build more tools on\nyour own.\n\n#### Step 1: Create Tool\n\nLet's install Groq SDK, set up our Groq client, and create a function called `calculate` to evaluate a mathematical\nexpression that we will represent as a tool.\n\nNote: In this example, we're defining a function as our tool, but your tool can be any function or an external\nresource (e.g. dabatase, web search engine, external API).\n\nPythonJavaScript\n\n```shell\npip install groq\n```\n\n```py\n1from groq import Groq\n2import json\n3\n4# Initialize the Groq client\n5client = Groq()\n6# Specify the model to be used (we recommend Llama 3.3 70B)\n7MODEL = 'llama-3.3-70b-versatile'\n8\n9def calculate(expression):\n10    \"\"\"Evaluate a mathematical expression\"\"\"\n11    try:\n12        # Attempt to evaluate the math expression\n13        result = eval(expression)\n14        return json.dumps({\"result\": result})\n15    except:\n16        # Return an error message if the math expression is invalid\n17        return json.dumps({\"error\": \"Invalid expression\"})\n```\n\n#### Step 2: Pass Tool Definition and Messages to Model\n\nNext, we'll define our `calculate` tool within an array of available `tools` and call our Groq API chat completion. You\ncan read more about tool schema and supported required and optional fields above in **Tool Specifications.**\n\nBy defining our tool, we'll inform our model about what our tool does and have the model decide whether or not to use the\ntool. We should be as descriptive and specific as possible for our model to be able to make the correct tool use decisions.\n\nIn addition to our `tools` array, we will provide our `messages` array (e.g. containing system prompt, assistant prompt, and/or\nuser prompt).\n\n#### Step 3: Receive and Handle Tool Results\n\nAfter executing our chat completion, we'll extract our model's response and check for tool calls.\n\nIf the model decides that no tools should be used and does not generate a tool or function call, then the response will\nbe a normal chat completion (i.e. `response_message = response.choices[0].message`) with a direct model reply to the user query.\n\nIf the model decides that tools should be used and generates a tool or function call, we will:\n\n- Define available tool or function,\n- Add the model's response to the conversation by appending our message\n- Process the tool call and add the tool response to our message\n- Make a second Groq API call with the updated conversation\n- Return the final response\n\nPythonJavaScript\n\n```py\n1# imports calculate function from step 1\n2def run_conversation(user_prompt):\n3    # Initialize the conversation with system and user messages\n4    messages=[\\\n5        {\\\n6            \"role\": \"system\",\\\n7            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\"\\\n8        },\\\n9        {\\\n10            \"role\": \"user\",\\\n11            \"content\": user_prompt,\\\n12        }\\\n13    ]\n14    # Define the available tools (i.e. functions) for our model to use\n15    tools = [\\\n16        {\\\n17            \"type\": \"function\",\\\n18            \"function\": {\\\n19                \"name\": \"calculate\",\\\n20                \"description\": \"Evaluate a mathematical expression\",\\\n21                \"parameters\": {\\\n22                    \"type\": \"object\",\\\n23                    \"properties\": {\\\n24                        \"expression\": {\\\n25                            \"type\": \"string\",\\\n26                            \"description\": \"The mathematical expression to evaluate\",\\\n27                        }\\\n28                    },\\\n29                    \"required\": [\"expression\"],\\\n30                },\\\n31            },\\\n32        }\\\n33    ]\n34    # Make the initial API call to Groq\n35    response = client.chat.completions.create(\n36        model=MODEL, # LLM to use\n37        messages=messages, # Conversation history\n38        stream=False,\n39        tools=tools, # Available tools (i.e. functions) for our LLM to use\n40        tool_choice=\"auto\", # Let our LLM decide when to use tools\n41        max_completion_tokens=4096 # Maximum number of tokens to allow in our response\n42    )\n43    # Extract the response and any tool call responses\n44    response_message = response.choices[0].message\n45    tool_calls = response_message.tool_calls\n46    if tool_calls:\n47        # Define the available tools that can be called by the LLM\n48        available_functions = {\n49            \"calculate\": calculate,\n50        }\n51        # Add the LLM's response to the conversation\n52        messages.append(response_message)\n53\n54        # Process each tool call\n55        for tool_call in tool_calls:\n56            function_name = tool_call.function.name\n57            function_to_call = available_functions[function_name]\n58            function_args = json.loads(tool_call.function.arguments)\n59            # Call the tool and get the response\n60            function_response = function_to_call(\n61                expression=function_args.get(\"expression\")\n62            )\n63            # Add the tool response to the conversation\n64            messages.append(\n65                {\n66                    \"tool_call_id\": tool_call.id,\n67                    \"role\": \"tool\", # Indicates this message is from tool use\n68                    \"name\": function_name,\n69                    \"content\": function_response,\n70                }\n71            )\n72        # Make a second API call with the updated conversation\n73        second_response = client.chat.completions.create(\n74            model=MODEL,\n75            messages=messages\n76        )\n77        # Return the final response\n78        return second_response.choices[0].message.content\n79# Example usage\n80user_prompt = \"What is 25 * 4 + 10?\"\n81print(run_conversation(user_prompt))\n```\n\n### [Routing System](https://console.groq.com/docs/tool-use\\#routing-system)\n\nIf you use our models fine-tuned for tool use, we recommended to use them as part of a routing system:\n\n- **Query Analysis**: Implement a routing system that analyzes incoming user queries to determine their nature and requirements.\n- **Model Selection**: Based on the query analysis, route the request to the most appropriate model:\n  - For queries involving function calling, API interactions, or structured data manipulation, use the Llama 3 Groq Tool Use models.\n  - For general knowledge, open-ended conversations, or tasks not specifically related to tool use, route to a general-purpose language model, such as Llama 3 70B.\n\nThe following is the `calculate` tool we built in the above steps enhanced to include a routing system that routes our request\nto Llama 3.3 70B if the user query does not require the tool:\n\nPythonJavaScript\n\n```py\n1from groq import Groq\n2import json\n3\n4# Initialize the Groq client\n5client = Groq()\n6\n7# Define models\n8ROUTING_MODEL = \"llama3-70b-8192\"\n9TOOL_USE_MODEL = \"llama-3.3-70b-versatile\"\n10GENERAL_MODEL = \"llama3-70b-8192\"\n11\n12def calculate(expression):\n13    \"\"\"Tool to evaluate a mathematical expression\"\"\"\n14    try:\n15        result = eval(expression)\n16        return json.dumps({\"result\": result})\n17    except:\n18        return json.dumps({\"error\": \"Invalid expression\"})\n19\n20def route_query(query):\n21    \"\"\"Routing logic to let LLM decide if tools are needed\"\"\"\n22    routing_prompt = f\"\"\"\n23    Given the following user query, determine if any tools are needed to answer it.\n24    If a calculation tool is needed, respond with 'TOOL: CALCULATE'.\n25    If no tools are needed, respond with 'NO TOOL'.\n26\n27    User query: {query}\n28\n29    Response:\n30    \"\"\"\n31\n32    response = client.chat.completions.create(\n33        model=ROUTING_MODEL,\n34        messages=[\\\n35            {\"role\": \"system\", \"content\": \"You are a routing assistant. Determine if tools are needed based on the user query.\"},\\\n36            {\"role\": \"user\", \"content\": routing_prompt}\\\n37        ],\n38        max_completion_tokens=20  # We only need a short response\n39    )\n40\n41    routing_decision = response.choices[0].message.content.strip()\n42\n43    if \"TOOL: CALCULATE\" in routing_decision:\n44        return \"calculate tool needed\"\n45    else:\n46        return \"no tool needed\"\n47\n48def run_with_tool(query):\n49    \"\"\"Use the tool use model to perform the calculation\"\"\"\n50    messages = [\\\n51        {\\\n52            \"role\": \"system\",\\\n53            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\",\\\n54        },\\\n55        {\\\n56            \"role\": \"user\",\\\n57            \"content\": query,\\\n58        }\\\n59    ]\n60    tools = [\\\n61        {\\\n62            \"type\": \"function\",\\\n63            \"function\": {\\\n64                \"name\": \"calculate\",\\\n65                \"description\": \"Evaluate a mathematical expression\",\\\n66                \"parameters\": {\\\n67                    \"type\": \"object\",\\\n68                    \"properties\": {\\\n69                        \"expression\": {\\\n70                            \"type\": \"string\",\\\n71                            \"description\": \"The mathematical expression to evaluate\",\\\n72                        }\\\n73                    },\\\n74                    \"required\": [\"expression\"],\\\n75                },\\\n76            },\\\n77        }\\\n78    ]\n79    response = client.chat.completions.create(\n80        model=TOOL_USE_MODEL,\n81        messages=messages,\n82        tools=tools,\n83        tool_choice=\"auto\",\n84        max_completion_tokens=4096\n85    )\n86    response_message = response.choices[0].message\n87    tool_calls = response_message.tool_calls\n88    if tool_calls:\n89        messages.append(response_message)\n90        for tool_call in tool_calls:\n91            function_args = json.loads(tool_call.function.arguments)\n92            function_response = calculate(function_args.get(\"expression\"))\n93            messages.append(\n94                {\n95                    \"tool_call_id\": tool_call.id,\n96                    \"role\": \"tool\",\n97                    \"name\": \"calculate\",\n98                    \"content\": function_response,\n99                }\n100            )\n101        second_response = client.chat.completions.create(\n102            model=TOOL_USE_MODEL,\n103            messages=messages\n104        )\n105        return second_response.choices[0].message.content\n106    return response_message.content\n107\n108def run_general(query):\n109    \"\"\"Use the general model to answer the query since no tool is needed\"\"\"\n110    response = client.chat.completions.create(\n111        model=GENERAL_MODEL,\n112        messages=[\\\n113            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\\n114            {\"role\": \"user\", \"content\": query}\\\n115        ]\n116    )\n117    return response.choices[0].message.content\n118\n119def process_query(query):\n120    \"\"\"Process the query and route it to the appropriate model\"\"\"\n121    route = route_query(query)\n122    if route == \"calculate\":\n123        response = run_with_tool(query)\n124    else:\n125        response = run_general(query)\n126\n127    return {\n128        \"query\": query,\n129        \"route\": route,\n130        \"response\": response\n131    }\n132\n133# Example usage\n134if __name__ == \"__main__\":\n135    queries = [\\\n136        \"What is the capital of the Netherlands?\",\\\n137        \"Calculate 25 * 4 + 10\"\\\n138    ]\n139\n140    for query in queries:\n141        result = process_query(query)\n142        print(f\"Query: {result['query']}\")\n143        print(f\"Route: {result['route']}\")\n144        print(f\"Response: {result['response']}\\n\")\n```\n\n## Parallel Tool Use\n\nWe learned about tool use and built single-turn tool use examples above. Now let's take tool use a step further and imagine\na workflow where multiple tools can be called simultaneously, enabling more efficient and effective responses.\n\nThis concept is known as **parallel tool use** and is key for building agentic workflows that can deal with complex queries,\nwhich is a great example of where inference speed becomes increasingly important (and thankfully we can access fast inference\nspeed with Groq API).\n\nHere's an example of parallel tool use with a tool for getting the temperature and the tool for getting the weather condition\nto show parallel tool use with Groq API in action:\n\nPythonJavaScript\n\n```py\n1import json\n2from groq import Groq\n3import os\n4\n5# Initialize Groq client\n6client = Groq()\n7model = \"llama-3.3-70b-versatile\"\n8\n9# Define weather tools\n10def get_temperature(location: str):\n11    # This is a mock tool/function. In a real scenario, you would call a weather API.\n12    temperatures = {\"New York\": 22, \"London\": 18, \"Tokyo\": 26, \"Sydney\": 20}\n13    return temperatures.get(location, \"Temperature data not available\")\n14\n15def get_weather_condition(location: str):\n16    # This is a mock tool/function. In a real scenario, you would call a weather API.\n17    conditions = {\"New York\": \"Sunny\", \"London\": \"Rainy\", \"Tokyo\": \"Cloudy\", \"Sydney\": \"Clear\"}\n18    return conditions.get(location, \"Weather condition data not available\")\n19\n20# Define system messages and tools\n21messages = [\\\n22    {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\\\n23    {\"role\": \"user\", \"content\": \"What's the weather like in New York and London?\"},\\\n24]\n25\n26tools = [\\\n27    {\\\n28        \"type\": \"function\",\\\n29        \"function\": {\\\n30            \"name\": \"get_temperature\",\\\n31            \"description\": \"Get the temperature for a given location\",\\\n32            \"parameters\": {\\\n33                \"type\": \"object\",\\\n34                \"properties\": {\\\n35                    \"location\": {\\\n36                        \"type\": \"string\",\\\n37                        \"description\": \"The name of the city\",\\\n38                    }\\\n39                },\\\n40                \"required\": [\"location\"],\\\n41            },\\\n42        },\\\n43    },\\\n44    {\\\n45        \"type\": \"function\",\\\n46        \"function\": {\\\n47            \"name\": \"get_weather_condition\",\\\n48            \"description\": \"Get the weather condition for a given location\",\\\n49            \"parameters\": {\\\n50                \"type\": \"object\",\\\n51                \"properties\": {\\\n52                    \"location\": {\\\n53                        \"type\": \"string\",\\\n54                        \"description\": \"The name of the city\",\\\n55                    }\\\n56                },\\\n57                \"required\": [\"location\"],\\\n58            },\\\n59        },\\\n60    }\\\n61]\n62\n63# Make the initial request\n64response = client.chat.completions.create(\n65    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_completion_tokens=4096\n66)\n67\n68response_message = response.choices[0].message\n69tool_calls = response_message.tool_calls\n70\n71# Process tool calls\n72messages.append(response_message)\n73\n74available_functions = {\n75    \"get_temperature\": get_temperature,\n76    \"get_weather_condition\": get_weather_condition,\n77}\n78\n79for tool_call in tool_calls:\n80    function_name = tool_call.function.name\n81    function_to_call = available_functions[function_name]\n82    function_args = json.loads(tool_call.function.arguments)\n83    function_response = function_to_call(**function_args)\n84\n85    messages.append(\n86        {\n87            \"role\": \"tool\",\n88            \"content\": str(function_response),\n89            \"tool_call_id\": tool_call.id,\n90        }\n91    )\n92\n93# Make the final request with tool call results\n94final_response = client.chat.completions.create(\n95    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_completion_tokens=4096\n96)\n97\n98print(final_response.choices[0].message.content)\n```\n\n## Error Handling\n\nGroq API tool use is designed to verify whether a model generates a valid tool call object. When a model fails to generate a valid tool call object,\nGroq API will return a 400 error with an explanation in the \"failed\\_generation\" field of the JSON body that is returned.\n\n### [Next Steps](https://console.groq.com/docs/tool-use\\#next-steps)\n\nFor more information and examples of working with multiple tools in parallel using Groq API and Instructor, see our Groq API Cookbook\ntutorial [here](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/parallel-tool-use/parallel-tool-use.ipynb).\n\n## Tool Use with Structured Outputs (Python)\n\nGroq API offers best-effort matching for parameters, which means the model could occasionally miss parameters or\nmisinterpret types for more complex tool calls. We recommend the [Instuctor](https://python.useinstructor.com/hub/groq/)\nlibrary to simplify the process of working with structured data and to ensure that the model's output adheres to a predefined\nschema.\n\nHere's an example of how to implement tool use using the Instructor library with Groq API:\n\n```shell\npip install instructor pydantic\n```\n\n```py\n1import instructor\n2from pydantic import BaseModel, Field\n3from groq import Groq\n4\n5# Define the tool schema\n6tool_schema = {\n7    \"name\": \"get_weather_info\",\n8    \"description\": \"Get the weather information for any location.\",\n9    \"parameters\": {\n10        \"type\": \"object\",\n11        \"properties\": {\n12            \"location\": {\n13                \"type\": \"string\",\n14                \"description\": \"The location for which we want to get the weather information (e.g., New York)\"\n15            }\n16        },\n17        \"required\": [\"location\"]\n18    }\n19}\n20\n21# Define the Pydantic model for the tool call\n22class ToolCall(BaseModel):\n23    input_text: str = Field(description=\"The user's input text\")\n24    tool_name: str = Field(description=\"The name of the tool to call\")\n25    tool_parameters: str = Field(description=\"JSON string of tool parameters\")\n26\n27class ResponseModel(BaseModel):\n28    tool_calls: list[ToolCall]\n29\n30# Patch Groq() with instructor\n31client = instructor.from_groq(Groq(), mode=instructor.Mode.JSON)\n32\n33def run_conversation(user_prompt):\n34    # Prepare the messages\n35    messages = [\\\n36        {\\\n37            \"role\": \"system\",\\\n38            \"content\": f\"You are an assistant that can use tools. You have access to the following tool: {tool_schema}\"\\\n39        },\\\n40        {\\\n41            \"role\": \"user\",\\\n42            \"content\": user_prompt,\\\n43        }\\\n44    ]\n45\n46    # Make the Groq API call\n47    response = client.chat.completions.create(\n48        model=\"llama-3.3-70b-versatile\",\n49        response_model=ResponseModel,\n50        messages=messages,\n51        temperature=0.7,\n52        max_completion_tokens=1000,\n53    )\n54\n55    return response.tool_calls\n56\n57# Example usage\n58user_prompt = \"What's the weather like in San Francisco?\"\n59tool_calls = run_conversation(user_prompt)\n60\n61for call in tool_calls:\n62    print(f\"Input: {call.input_text}\")\n63    print(f\"Tool: {call.tool_name}\")\n64    print(f\"Parameters: {call.tool_parameters}\")\n65    print()\n```\n\n### [Benefits of Using Structured Outputs](https://console.groq.com/docs/tool-use\\#benefits-of-using-structured-outputs)\n\n- Type Safety: Pydantic models ensure that output adheres to the expected structure, reducing the risk of errors.\n- Automatic Validation: Instructor automatically validates the model's output against the defined schema.\n\n### [Next Steps](https://console.groq.com/docs/tool-use\\#next-steps)\n\nFor more information and examples of working with structured outputs using Groq API and Instructor, see our Groq API Cookbook\ntutorial [here](https://github.com/groq/groq-api-cookbook/blob/main/tutorials/structured-output-instructor/structured_output_instructor.ipynb).\n\n## Best Practices\n\n- Provide detailed tool descriptions for optimal performance.\n- We recommend tool use with the Instructor library for structured outputs.\n- Use the fine-tuned Llama 3 models by Groq or the Llama 3.1 models for your applications that require tool use.\n- Implement a routing system when using fine-tuned models in your workflow.\n- Handle tool execution errors by returning error messages with `\"is_error\": true`.",
    "metadata": {
      "url": "https://console.groq.com/docs/tool-use",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "e9695b27-4871-4611-aeac-9ca1fae8e8a2",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/tool-use",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "# Authentication required\n\nPlease log in to access this page.\n\n[Login](https://console.groq.com/login)",
    "metadata": {
      "url": "https://console.groq.com/settings",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "e33b8bbf-c054-439f-babe-94cde71d4e3a",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/settings",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Agno + Groq: Lightning Fast Agents\n\n[Agno](https://github.com/agno-agi/agno) is a lightweight framework for building multi-modal Agents. Its easy to use, extremely fast and supports multi-modal inputs and outputs.\n\nWith Groq & Agno, you can build:\n\n- **Agentic RAG**: Agents that can search different knowledge stores for RAG or dynamic few-shot learning.\n- **Image Agents**: Agents that can understand images and make tool calls accordingly.\n- **Reasoning Agents**: Agents that can reason using a reasoning model, then generate a result using another model.\n- **Structured Outputs**: Agents that can generate pydantic objects adhering to a schema.\n\n### [Python Quick Start (2 minutes to hello world)](https://console.groq.com/docs/agno\\#python-quick-start-2-minutes-to-hello-world)\n\nAgents are autonomous programs that use language models to achieve tasks. They solve problems by running tools, accessing knowledge and memory to improve responses.\n\nLet's build a simple web search agent, with a tool to search DuckDuckGo to get better results.\n\n#### 1\\. Create a file called `web_search_agent.py` and add the following code:\n\n```python\nfrom agno.agent import Agent\nfrom agno.models.groq import Groq\nfrom agno.tools.duckduckgo import DuckDuckGoTools\n\n# Initialize the agent with an LLM via Groq and DuckDuckGoTools\nagent = Agent(\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    description=\"You are an enthusiastic news reporter with a flair for storytelling!\",\n    tools=[DuckDuckGoTools()],      # Add DuckDuckGo tool to search the web\n    show_tool_calls=True,           # Shows tool calls in the response, set to False to hide\n    markdown=True                   # Format responses in markdown\n)\n\n# Prompt the agent to fetch a breaking news story from New York\nagent.print_response(\"Tell me about a breaking news story from New York.\", stream=True)\n```\n\n#### 3\\. Set up and activate your virtual environment:\n\n```shell\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\n#### 4\\. Install the Groq, Agno, and DuckDuckGo dependencies:\n\n```shell\npip install -U groq agno duckduckgo-search\n```\n\n#### 5\\. Configure your Groq API Key:\n\n```bash\nGROQ_API_KEY=\"your-api-key\"\n```\n\n#### 6\\. Run your Agno agent that now extends your LLM's context to include web search for up-to-date information and send results in seconds:\n\n```shell\npython web_search_agent.py\n```\n\n### [Multi-Agent Teams](https://console.groq.com/docs/agno\\#multiagent-teams)\n\nAgents work best when they have a singular purpose, a narrow scope, and a small number of tools. When the number of tools grows beyond what the language model can handle or the tools belong to different\ncategories, use a **team of agents** to spread the load.\n\nThe following code expands upon our quick start and creates a team of two agents to provide analysis on financial markets:\n\n```python\nfrom agno.agent import Agent\nfrom agno.models.groq import Groq\nfrom agno.tools.duckduckgo import DuckDuckGoTools\nfrom agno.tools.yfinance import YFinanceTools\n\nweb_agent = Agent(\n    name=\"Web Agent\",\n    role=\"Search the web for information\",\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    tools=[DuckDuckGoTools()],\n    instructions=\"Always include sources\",\n    markdown=True,\n)\n\nfinance_agent = Agent(\n    name=\"Finance Agent\",\n    role=\"Get financial data\",\n    model=Groq(id=\"llama-3.3-70b-versatile\"),\n    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],\n    instructions=\"Use tables to display data\",\n    markdown=True,\n)\n\nagent_team = Agent(\n    team=[web_agent, finance_agent],\n    model=Groq(id=\"llama-3.3-70b-versatile\"),  # You can use a different model for the team leader agent\n    instructions=[\"Always include sources\", \"Use tables to display data\"],\n    # show_tool_calls=True,  # Uncomment to see tool calls in the response\n    markdown=True,\n)\n\n# Give the team a task\nagent_team.print_response(\"What's the market outlook and financial performance of AI semiconductor companies?\", stream=True)\n```\n\n### [Additional Resources](https://console.groq.com/docs/agno\\#additional-resources)\n\nFor additional documentation and support, see the following:\n\n- [Agno Documentation](https://docs.agno.com/)\n- [Groq via Agno Documentation](https://docs.agno.com/models/groq)\n- [Groq via Agno examples](https://docs.agno.com/examples/models/groq/basic)\n- [Various industry-ready examples](https://docs.agno.com/examples/introduction)",
    "metadata": {
      "url": "https://console.groq.com/docs/agno",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "8b90f370-7dc1-415a-9acb-4689d966493e",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/agno",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## Supported Models\n\nGroqCloud currently supports the following models:\n\n### [Production Models](https://console.groq.com/docs/models\\#production-models)\n\n**Note:** Production models are intended for use in your production environments. They meet or exceed our high standards for speed and quality.\n\n| MODEL ID | DEVELOPER | CONTEXT WINDOW<br>(TOKENS) | MAX<br>COMPLETION TOKENS | MAX FILE<br>SIZE | MODEL CARD<br>LINK |\n| --- | --- | --- | --- | --- | --- |\n| distil-whisper-large-v3-en | HuggingFace | - | - | 25 MB | [Card](https://huggingface.co/distil-whisper/distil-large-v3) |\n| gemma2-9b-it | Google | 8,192 | - | - | [Card](https://huggingface.co/google/gemma-2-9b-it) |\n| llama-3.3-70b-versatile | Meta | 128K | 32,768 | - | [Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md) |\n| llama-3.1-8b-instant | Meta | 128K | 8,192 | - | [Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) |\n| llama-guard-3-8b | Meta | 8,192 | - | - | [Card](https://console.groq.com/docs/model/llama-guard-3-8b) |\n| llama3-70b-8192 | Meta | 8,192 | - | - | [Card](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) |\n| llama3-8b-8192 | Meta | 8,192 | - | - | [Card](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |\n| mixtral-8x7b-32768 | Mistral | 32,768 | - | - | [Card](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) |\n| whisper-large-v3 | OpenAI | - | - | 25 MB | [Card](https://huggingface.co/openai/whisper-large-v3) |\n| whisper-large-v3-turbo | OpenAI | - | - | 25 MB | [Card](https://huggingface.co/openai/whisper-large-v3-turbo) |\n\n### [Preview Models](https://console.groq.com/docs/models\\#preview-models)\n\n**Note:** Preview models are intended for evaluation purposes only and should not be used in production environments as they may be discontinued at short notice.\n\n| MODEL ID | DEVELOPER | CONTEXT WINDOW<br>(TOKENS) | MAX<br>COMPLETION TOKENS | MAX FILE<br>SIZE | MODEL CARD<br>LINK |\n| --- | --- | --- | --- | --- | --- |\n| qwen-2.5-coder-32b | Alibaba Cloud | 128K | - | - | [Card](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct) |\n| qwen-2.5-32b | Alibaba Cloud | 128K | - | - | [Card](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) |\n| deepseek-r1-distill-qwen-32b | DeepSeek | 128K | 16,384 | - | [Card](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) |\n| deepseek-r1-distill-llama-70b-specdec | DeepSeek | 128K | 16,384 | - | [Card](https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b) |\n| deepseek-r1-distill-llama-70b | DeepSeek | 128K | - | - | [Card](https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b) |\n| llama-3.3-70b-specdec | Meta | 8,192 | - | - | [Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md) |\n| llama-3.2-1b-preview | Meta | 128K | 8,192 | - | [Card](https://huggingface.co/meta-llama/Llama-3.2-1B) |\n| llama-3.2-3b-preview | Meta | 128K | 8,192 | - | [Card](https://huggingface.co/meta-llama/Llama-3.2-3B) |\n| llama-3.2-11b-vision-preview | Meta | 128K | 8,192 | - | [Card](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision) |\n| llama-3.2-90b-vision-preview | Meta | 128K | 8,192 | - | [Card](https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct) |\n\nDeprecated models are models that are no longer supported or will no longer be supported in the future. A suggested alternative model for you to use is listed for each deprecated model. [See our deprecated models here](https://console.groq.com/docs/deprecations)\n\nHosted models are directly accessible through the GroqCloud Models API endpoint using the model IDs mentioned above. You can use the `https://api.groq.com/openai/v1/models` endpoint to return a JSON list of all active models:\n\nPythonJavaScriptcurl\n\n```py\n1import requests\n2import os\n3\n4api_key = os.environ.get(\"GROQ_API_KEY\")\n5url = \"https://api.groq.com/openai/v1/models\"\n6\n7headers = {\n8    \"Authorization\": f\"Bearer {api_key}\",\n9    \"Content-Type\": \"application/json\"\n10}\n11\n12response = requests.get(url, headers=headers)\n13\n14print(response.json())\n```",
    "metadata": {
      "url": "https://console.groq.com/docs/models",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "8d7a3d24-98ca-46d7-8c63-0e39d80215cf",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/models",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  },
  {
    "markdown": "## Documentation\n\n## DeepSeek-R1-Distill-Llama-70B\n\nDeepSeek-R1-Distill-Llama-70B is a distilled version of DeepSeek's R1 model, fine-tuned from the Llama-3.3-70B-Instruct base model. This model leverages knowledge distillation to retain robust reasoning capabilities while enhancing efficiency.\n\n[**Try now on Groq**](https://chat.groq.com/?model=deepseek-r1-distill-llama-70b)\n\n### [Key Technical Specifications](https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b\\#key-technical-specifications)\n\n- **Model Architecture:** Built upon the Llama-3.3-70B-Instruct framework, the model comprises 70 billion parameters. The distillation process fine-tunes the base model using outputs from DeepSeek-R1, effectively transferring reasoning patterns.\n\n- **[Performance Metrics](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B):** The model demonstrates strong performance across various benchmarks:\n  - **AIME 2024:** Pass@1 score of 70.0.\n  - **MATH-500:** Pass@1 score of 94.5.\n  - **CodeForces Rating:** Achieved a rating of 1,633.\n\n### [Technical Details Table](https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b\\#technical-details-table)\n\n| Detail | Value |\n| --- | --- |\n| Context Window (Tokens) | 128k |\n| Max Output Tokens | - |\n| Max File Size | - |\n| Token Generation Speed (as of 2025-01-28) | 275 tps |\n| Pricing | [Pricing Details](https://groq.com/pricing/) |\n\n### [Capabilities and Features](https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b\\#capabilities-and-features)\n\nDeepSeek-R1-Distill-Llama-70B excels in the following areas:\n\n### [Supported Features](https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b\\#supported-features)\n\n| Feature | Supported |\n| --- | --- |\n| Tool Use | ✅ |\n| JSON Mode | ✅ |\n| Image Support | ❌ |\n\n### [Use Cases](https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b\\#use-cases)\n\n- **Mathematical Problem-Solving:** Effectively addresses complex mathematical queries, making it valuable for educational tools and research applications.\n- **Coding Assistance:** Supports code generation and debugging, beneficial for software development.\n- **Logical Reasoning:** Performs tasks requiring structured thinking and deduction, applicable in data analysis and strategic planning.\n\n### [Best Practices](https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b\\#best-practices)\n\n- **Prompt Engineering:** Set the temperature parameter between 0.5 and 0.7 (ideally 0.6) to prevent repetitive or incoherent outputs.\n- **System Prompt:** Avoid adding a system prompt and include all instructions within the user prompt.\n\n## Get Started with DeepSeek-R1-Distill-Llama-70B\n\nUnlock the full potential of logical reasoning with DeepSeek-R1-Distill-Llama-70B - engineered for the future of AI-driven problem-solving and optimized for exceptional performance on Groq hardware with near-instant reasoning now:\n\ncurlJavaScriptPythonJSON\n\n#### Install Groq and Perform Chat Completion Using Python:\n\n```shell\npip install groq\n```\n\n```py\n1import os\n2\n3from groq import Groq\n4\n5client = Groq(\n6    api_key=os.environ.get(\"GROQ_API_KEY\"),\n7)\n8\n9chat_completion = client.chat.completions.create(\n10    messages=[\\\n11        {\\\n12            \"role\": \"user\",\\\n13            \"content\": \"Explain why fast inference is critical for reasoning models\",\\\n14        }\\\n15    ],\n16    model=\"deepseek-r1-distill-llama-70b\",\n17)\n18\n19print(chat_completion.choices[0].message.content)\n```",
    "metadata": {
      "url": "https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b",
      "ogUrl": "https://console.groq.com",
      "title": "GroqCloud",
      "og:url": "https://console.groq.com",
      "favicon": {},
      "og:type": "website",
      "ogImage": "https://console.groq.com/og_cloudv3.jpg",
      "ogTitle": "GroqCloud",
      "language": "en",
      "og:image": "https://console.groq.com/og_cloudv3.jpg",
      "og:title": "GroqCloud",
      "scrapeId": "be026668-1a4e-41d4-8c00-c34ca4012cdb",
      "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
      "sourceURL": "https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b",
      "statusCode": 200,
      "twitter:card": "summary_large_image",
      "ogDescription": "Experience the fastest inference in the world",
      "twitter:image": "https://console.groq.com/og_cloudv3.jpg",
      "twitter:title": "GroqCloud",
      "og:description": "Experience the fastest inference in the world",
      "twitter:description": "Experience the fastest inference in the world"
    }
  }
]